{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "hungerGames.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiaqw/deep-learning/blob/main/hungerGames_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "KX8zKrS-hRJk"
      },
      "source": [
        "# Lab assignment: the hunger games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "vanYztAMhRJt"
      },
      "source": [
        "<table><tr>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "6E7BW750hRJu"
      },
      "source": [
        "In this assignment we will face a challenging image classification problem, building a deep learning model that is able to classify different kinds of foods. Let the hunger games begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "Mnn_FAm8hRJv"
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspUM6n3hRJw"
      },
      "source": [
        "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHQpQXrhRJw"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghhJf_HhRJx"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAbqrofhRJy"
      },
      "source": [
        "To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Deep Learning environment files](https://github.com/albarji/teaching-environments/tree/master/deeplearning), or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmf8R9DhRJy"
      },
      "source": [
        "The following code will embed any plots into the notebook instead of generating a new window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxmdgWqNhRJz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UgDu1WUhRJ0"
      },
      "source": [
        "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K733uf0ghRJ2"
      },
      "source": [
        "## Data acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYPquwFhRJ2"
      },
      "source": [
        "We will use a food images dataset available at [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). To download it you will need to create a user account in Kaggle, and obtain your API credential by following the instructions on [this section](https://www.kaggle.com/trolukovich/food11-image-dataset). Once you have your credentials JSON file, you can inform this notebook of them by setting the appropriate enviroment variables, as follows\n",
        "\n",
        "    import os\n",
        "\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n",
        "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n",
        "    \n",
        "Once this is done, you will be able to download the dataset to this computer using the command\n",
        "\n",
        "    !kaggle datasets download trolukovich/food11-image-dataset --unzip -p YOUR_LOCAL_FOLDER\n",
        "    \n",
        "where you should write a valid directory in your computer in \"YOUR_LOCAL_FOLDER\". If you are fine with downloading the data in the same folder as this notebook, just skip the `-p YOUR_LOCAL_FOLDER` part of the command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRX8BQsZhRJ2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Create a Kaggle account, obtain your credentials, and use the cell below to declare your Kaggle username and key variables, and to download the dataset to a local folder.\n",
        "    \n",
        "These credentials should remain secret to you. Remember to delete them before submitting this notebook!\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiFYSULfhRJ3",
        "outputId": "21392609-cf52-4ad4-a476-1c0a00991145"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "import os\n",
        "credentials = {\"username\":\"claudiaqw\",\n",
        "               \"key\":\"7a4a42e84feb6a7da837ebf98550b12b\"}\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']=credentials[\"username\"]\n",
        "os.environ['KAGGLE_KEY']=credentials[\"key\"]\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading food11-image-dataset.zip to /content\n",
            " 98% 1.06G/1.08G [00:09<00:00, 93.0MB/s]\n",
            "100% 1.08G/1.08G [00:09<00:00, 118MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh27gIEhRJ4"
      },
      "source": [
        "Take now a look into the folder where you downloaded the data. You will find it is made up of three subfolders:\n",
        "\n",
        "* **training**, containing the images to use to train the model.\n",
        "* **validation**, containing additional images you could use as more training data, or for some kind of validation strategy such as Early Stopping.\n",
        "* **evaluation**, containing the images you must use to test your model. Images in this folder can **only** be used to measure the model performance after the training procedure is completed.\n",
        "\n",
        "Furthermore, within each one of these folders you will find one folder for each one of the 11 classes in this problem:\n",
        "\n",
        "* Bread\n",
        "* Dairy product\n",
        "* Dessert\n",
        "* Egg\n",
        "* Fried food\n",
        "* Meat\n",
        "* Noodles-Pasta\n",
        "* Rice\n",
        "* Seafood\n",
        "* Soup\n",
        "* Vegetable-Fruit\n",
        "\n",
        "This is a standard structure for organizing image datasets: one folder per class. To easen the following data processing steps, let us define some variables telling us where the data is located."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrE8F8zhRJ4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Create variables <b>TRAINDIR</b>, <b>VALDIR</b> and <b>TESTDIR</b> with the paths to the folders with the training, validation and evaluation data, respectively.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzK2VI3ShRJ5"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "TRAINDIR, VALDIR, TESTDIR = '/content/training', '/content/validation', '/content/evaluation'\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtoTuArehRJ5"
      },
      "source": [
        "Let's plot a random sample of training images from each class, using the ipyplot package. If you are running this notebook in Google Colab, you will need to install this package first with\n",
        "\n",
        "    !pip install ipyplot\n",
        "\n",
        "You can inspect each class by clicking the different tabs in the interface that will appear when running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQams_NrqSEo"
      },
      "source": [
        "#!pip install ipyplot"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MWCsZR3ahRJ5"
      },
      "source": [
        "from glob import glob\n",
        "import ipyplot\n",
        "import numpy as np\n",
        "\n",
        "all_images = glob(f\"{TRAINDIR}/*/*.jpg\")  # Get all image paths\n",
        "np.random.shuffle(all_images)  # Randomize to show different images each run\n",
        "all_labels = [f.split(\"/\")[-2] for f in all_images]  # Extract class names from path\n",
        "\n",
        "ipyplot.plot_class_tabs(all_images, all_labels, max_imgs_per_tab=6, img_width=300, force_b64=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYtQyR-hRJ6"
      },
      "source": [
        "### Class reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsLot-7hRJ6"
      },
      "source": [
        "To make the problem more approachable for this exercise, we will focus on just six classes: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` and `Meat`. To do so, we will delete from the downloaded data the folders from other classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXoNw1-thRJ7",
        "outputId": "e011ee47-165d-4671-b84a-c8c589da0a8e"
      },
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting /content/validation/Soup...\n",
            "Deleting /content/validation/Seafood...\n",
            "Deleting /content/validation/Vegetable-Fruit...\n",
            "Deleting /content/validation/Noodles-Pasta...\n",
            "Deleting /content/validation/Rice...\n",
            "Deleting /content/evaluation/Soup...\n",
            "Deleting /content/evaluation/Seafood...\n",
            "Deleting /content/evaluation/Vegetable-Fruit...\n",
            "Deleting /content/evaluation/Noodles-Pasta...\n",
            "Deleting /content/evaluation/Rice...\n",
            "Deleting /content/training/Soup...\n",
            "Deleting /content/training/Seafood...\n",
            "Deleting /content/training/Vegetable-Fruit...\n",
            "Deleting /content/training/Noodles-Pasta...\n",
            "Deleting /content/training/Rice...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncCfOYihRJ7"
      },
      "source": [
        "## Image processing from files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTWrPr-hRJ7"
      },
      "source": [
        "This dataset of images is large, with images of larger resolution than in the tutorial MNIST problem, each one having different sizes and image ratios. Also, while for MNIST we had a keras function that prepared the dataset for us, this time we will need to do some loading and image processing work.\n",
        "\n",
        "A convenient way to do this work is through the use of Keras `image_dataset_from_directory` function. This function creates a TensorFlow `Dataset` with the images in the directory, loading images dynamically only when the neural network needs to use them, and also allowing us to specify some useful preprocessing options.\n",
        "\n",
        "For example, we can create such a `Dataset` with the data in the training folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edpP7-NhRJ8",
        "outputId": "204e354b-a706-4f2e-b12d-48a8c6d93071"
      },
      "source": [
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    TRAINDIR, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGEdEuF5hRJ8"
      },
      "source": [
        "Note the parameters used to configure the dataset:\n",
        "\n",
        "* The **directory** from which to load the images.\n",
        "* An **image size** that will be used to resize all the images to a common resolution, here 32x32.\n",
        "* The **size of the batches** of images to be generated. Note we define this parameter here instead in the network fit step, as the `Dataset` will make use of this information to keep in memory only a few batches of images at the same time in order to save memory.\n",
        "* The **label mode**, that is, the encoding used for the labels. `categorical` means we will use one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgeGjQDVhRJ9"
      },
      "source": [
        "A `Dataset` object works like a Python generator, which means we can iterate over it to obtain batches of processed images. For instance, to get the first batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAIrHu7hRJ9"
      },
      "source": [
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"Shape of input batch: {X_batch.shape}\")\n",
        "    print(f\"Shape of output batch: {y_batch.shape}\")\n",
        "    print(f\"Input batch:\\n{X_batch}\")\n",
        "    print(f\"Output batch:\\n{y_batch}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UaPCEthRJ-"
      },
      "source": [
        "We can see that indeed the generator produces a tensor of the appropriate dimensions with the inputs for the neural network, and that the outputs have also been properly codified in one-hot form. However, there is still an issue with the data: the pixel values are in the range [0, 255], which might produce training problems. We will solve this later in the network definition. For now, let's move on and prepare a funcion that builds the training, validation and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQyBbf-hRJ-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Create a function <b>create_datasets</b> that receives the following parameters:\n",
        "    <ul>\n",
        "      <li><b>traindir</b>: the directory where training images are located.</li>\n",
        "      <li><b>valdir</b>: the directory where validation images are located.</li>\n",
        "      <li><b>testdir</b>: the directory where test images are located.</li>\n",
        "      <li><b>image_size</b>: the size that will be used to resize all the images to a common resolution.</li>\n",
        "      <li><b>batch_size</b>: the size of the batches of images to be generated.</li>\n",
        "    </ul>\n",
        "    The function must create datasets for the training, validation and test directories, and return the three datasets created.\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-wm19eJhRJ-"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "def create_datasets(traindir=TRAINDIR, valdir=VALDIR, testdir=TESTDIR, image_size=32, batch_size=64):\n",
        "  label_mode = 'categorical'\n",
        "  \n",
        "  train_dataset = image_dataset_from_directory(\n",
        "    traindir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  val_dataset = image_dataset_from_directory(\n",
        "    valdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  test_dataset = image_dataset_from_directory(\n",
        "    testdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wGaULQsjwJz"
      },
      "source": [
        "Let's test if the function you just implemented works correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZlNbHhiOIR",
        "outputId": "56e62e67-8325-4f17-cd6c-6438b8d46c92"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=64)\n",
        "\n",
        "# Test whether all returned objects are valid Tensorflow datasets\n",
        "assert isinstance(train_dataset, tf.data.Dataset)\n",
        "assert isinstance(val_dataset, tf.data.Dataset)\n",
        "assert isinstance(test_dataset, tf.data.Dataset)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIo6LBfhRJ-"
      },
      "source": [
        "Now that we have our datasets we can train a deep learning model using them! For illustration purposes, let's build an extremely simple convolutional network. Note how we have added a special pre-processing layer `Rescaling` that takes care of normalizing the data to the range [0, 1].\n",
        "\n",
        "Be careful! This network will work, but has some flaws in its design you might want to fix in the network you will desing later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cumg6qzsZHCl"
      },
      "source": [
        "from numpy.random import seed\n",
        "import random\n",
        "\n",
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbwmYJiLhRJ_"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model.add(Convolution2D(4, 3, activation='linear'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9OxA-AhRJ_"
      },
      "source": [
        "The `fit` method of a Keras model can receive a `Dataset` with training data, instead of a pair of tensors with (inputs, outputs). Since when building the `Dataset` we already specified the batch size, we don't need to do it now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZXjs6CghRKA",
        "outputId": "b78a3179-9c2f-4556-c5f4-c96afc0875db"
      },
      "source": [
        "model.fit(train_dataset, epochs=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 43s 137ms/step - loss: 1.7418 - accuracy: 0.2497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ce09c4850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ux64pchRKA"
      },
      "source": [
        "Similarly, we can evaluate the performance of our model over our test `Dataset` as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_kQhkY-hRKA",
        "outputId": "40cfd3f4-504d-41b2-ef15-fef3774755df"
      },
      "source": [
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 130ms/step - loss: 2.0489 - accuracy: 0.2522\n",
            "Loss 2.05, accuracy 25.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXf9OBehRKB"
      },
      "source": [
        "The accuracy might seem poor, but take into account we have used a very simple model and this problem has 6 classes. Will you be able to do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZnaNQIhRKB"
      },
      "source": [
        "## Building your network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNd8dC2whRKB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Design a neural network that maximizes the accuracy over the test data. You can use the training and validation datasets for anything you like, but you can <b>only</b> use the test data to evaluate the model performance. You should obtain a network able to attain at least 40% accuracy over the test set.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C9GNYihRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Some tips and strategies that can help you optimize your network design:\n",
        "\n",
        "    \n",
        "- Make use of all the tricks you learned from previous notebooks: convolutional + pooling layers, ReLU activations, dropout... also make sure to use a good optimizer with an adequate loss function, as well as the correct activation for the output layer.\n",
        "- Configuring the datasets to load the images with a larger size can significantly improve your performance. But be careful, you can also run out of memory (CUDA memory error) if they become too large! Note that for this problem a size larger than 256 might be too large.\n",
        "- Start with networks with a small number of parameters, so you are able to check fast how well they work. Then you can make your network larger in three directions: larger input images, more layers and more kernels per convolutional layer or units per dense layer. If you use larger images make sure to add more Convolution+Pooling layers, so that only very small images (less than 10x10 pixels) arrive at the Flatten layer.\n",
        "- If you see large differences in loss between your training data and your validation or test data, try increasing the Dropout probabilities, especially for the Dense layers.\n",
        "- Make use of that validation data! For instance, use an <a href=\"https://keras.io/api/callbacks/early_stopping/\">**EarlyStopping strategy**</a> to monitor the loss of these validation data, and stop when training when after a number of iterations such loss has not decreased. Configuring the EarlyStopping to restore the best weights found in the optimization is also useful.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDs0SGghRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "Other advanced strategies you can try are:\n",
        "\n",
        "- Use **image augmentation techniques** to artifically create new training images. To do so, explore the rest of layers available in the <a href=\"https://keras.io/api/layers/preprocessing_layers/image_preprocessing/\">Keras Image Preprocessing module</a>.\n",
        "- Use <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> layers to improve the optimization procedure.\n",
        "    \n",
        "It you use all the tricks, it is possible to obtain more than 60% accuracy in the test set.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvLgIBwvPPo"
      },
      "source": [
        "### First model: Changing optimizer\n",
        "\n",
        "The first change in the original model is to replace the *sgd* optimizer by the *Adam*. As we can see, the model performance on test has improved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kzlLHkFSIxd"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgz7w3VqhRKC"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "pool_size = 2\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_1.add(Convolution2D(4, 3, activation='linear'))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVO-hqeLyNBw",
        "outputId": "38ec42bc-4fb5-42a5-980b-508a7e649f49"
      },
      "source": [
        "model_1.fit(train_dataset, epochs=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 17s 160ms/step - loss: 2.4922 - accuracy: 0.2385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7cd0206bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLFP2VEFySIb",
        "outputId": "32877c1d-2c86-4b38-cc9c-a6da2a438b83"
      },
      "source": [
        "loss, acc = model_1.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 137ms/step - loss: 1.7244 - accuracy: 0.3256\n",
            "Loss 1.72, accuracy 32.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmpC3tQgqQys"
      },
      "source": [
        "### Second model: adding max pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5oWxpIIrXnm"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5NcS9d1qT_7"
      },
      "source": [
        "pool_size = 2\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_2.add(Convolution2D(4, 3, activation='linear'))\n",
        "model_2.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s03O1V5yrrlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fe6305-2d9a-4e55-d21a-57c449f23fa5"
      },
      "source": [
        "model_2.fit(train_dataset, epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 17s 158ms/step - loss: 1.8477 - accuracy: 0.2595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7cd00c6250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEN_VuEktyfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bee40b-57d0-4669-edfc-918fa2c52ff0"
      },
      "source": [
        "loss, acc = model_2.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 141ms/step - loss: 1.6033 - accuracy: 0.3493\n",
            "Loss 1.6, accuracy 34.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdtuqZOHtgS"
      },
      "source": [
        "### Third model: adding another convolution but with relu activation, a rectified linear activation and dropout with probability 0.5\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXsLxGEeIScc"
      },
      "source": [
        "Now, we try to improve the network adding another convolution with 32 channels to generate and an relu activation, a dense layer with 50 units and rectified linear activation and a dropout with 50% of probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi9W_R3sSO8v"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxr4uPIIOzC"
      },
      "source": [
        "kernel_size = 3\n",
        "model_3 = Sequential()\n",
        "model_3.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_3.add(Convolution2D(4, (kernel_size, kernel_size), input_shape=(image_size, image_size, 3), activation='linear'))\n",
        "model_3.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "model_3.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_3.add(Dense(50, activation=\"relu\"))\n",
        "model_3.add(Dropout(0.5))\n",
        "model_3.add(Flatten())\n",
        "model_3.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoNzcrB0JKcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8c1999-8f4f-42d3-a63f-8dfc704f3acd"
      },
      "source": [
        "model_3.fit(train_dataset, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 14s 136ms/step - loss: 1.8523 - accuracy: 0.2556\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.5526 - accuracy: 0.3514\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.5708 - accuracy: 0.3502\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 1.5821 - accuracy: 0.3539\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 13s 125ms/step - loss: 1.5140 - accuracy: 0.3748\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4610 - accuracy: 0.4019\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4637 - accuracy: 0.4084\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4913 - accuracy: 0.3979\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 1.4761 - accuracy: 0.3991\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4976 - accuracy: 0.3978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fef101b2dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvw5uKOB8XN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cb18d5-8a4c-428d-a7e2-f75e4efbcf6f"
      },
      "source": [
        "loss, acc = model_3.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 89ms/step - loss: 1.5514 - accuracy: 0.3478\n",
            "Loss 1.55, accuracy 34.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO95UHFHAC2Q"
      },
      "source": [
        "As we can see, although the accuracy in train has decreased slightly, we have achieved a better one in test. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6s5XhID-Wk"
      },
      "source": [
        "### Fourth model: we decided to try with larger input images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Waym0ZKBWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721ee984-ce47-4163-f6f2-95a928a62390"
      },
      "source": [
        "train_dataset_64, val_dataset_64, test_dataset_64 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=64, batch_size=64)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CW3g1TjST-P"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbzAqV1IEKe0"
      },
      "source": [
        "kernel_size = 3\n",
        "image_size = 64\n",
        "model_4 = Sequential()\n",
        "model_4.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_4.add(Convolution2D(4, (kernel_size, kernel_size), input_shape=(image_size, image_size, 3), activation='linear'))\n",
        "model_4.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "model_4.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_4.add(Dense(50, activation=\"relu\"))\n",
        "model_4.add(Dropout(0.5))\n",
        "model_4.add(Flatten())\n",
        "model_4.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_4.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpDw4mI5Egle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430e84f5-c1cd-44e6-cdc1-0fce7a570de7"
      },
      "source": [
        "model_4.fit(train_dataset_64, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 14s 127ms/step - loss: 2.6222 - accuracy: 0.1988\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 14s 133ms/step - loss: 1.7344 - accuracy: 0.2411\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 14s 135ms/step - loss: 1.7311 - accuracy: 0.2402\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.7304 - accuracy: 0.2443\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.7313 - accuracy: 0.2454\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 14s 133ms/step - loss: 1.7306 - accuracy: 0.2416\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.7332 - accuracy: 0.2403\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 14s 135ms/step - loss: 1.7299 - accuracy: 0.2454\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.7292 - accuracy: 0.2447\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.7178 - accuracy: 0.2508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feed14de250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVe7QIVwEsQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24240b0-84c5-45f8-94b0-0c6f2afc63d1"
      },
      "source": [
        "loss, acc = model_4.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 122ms/step - loss: 1.7360 - accuracy: 0.2415\n",
            "Loss 1.74, accuracy 24.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXsfvkKZSZTV"
      },
      "source": [
        "### Fifth Model:\n",
        "\n",
        "This we are going to ...\n",
        "\n",
        "This model reaches a high performance over training set, but not on validation wich means is overfitted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwZBt3jxSjze"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfafYODYShL-"
      },
      "source": [
        "model_5 = Sequential()\n",
        "model_5.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_5.add(Convolution2D(4, 2, input_shape=(image_size, image_size, 3), activation='linear'))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "model_5.add(Dropout(0.5))\n",
        "model_5.add(Convolution2D(16, 2, activation='relu'))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "model_5.add(Dropout(0.5))\n",
        "model_5.add(Convolution2D(32, 2, activation='linear'))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "# model_4.add(Dropout(0.5))\n",
        "model_5.add(Flatten())\n",
        "model_5.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_5.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW6eJGqvX0JW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a90428-921b-42be-b15e-3a240425c73d"
      },
      "source": [
        "model_5.fit(train_dataset_64, epochs=10)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 17s 159ms/step - loss: 37.8502 - accuracy: 0.2205\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 17s 163ms/step - loss: 1.9058 - accuracy: 0.3224\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 17s 162ms/step - loss: 1.6885 - accuracy: 0.3800\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 17s 163ms/step - loss: 1.8685 - accuracy: 0.3746\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 17s 168ms/step - loss: 1.5763 - accuracy: 0.3998\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 17s 169ms/step - loss: 1.4071 - accuracy: 0.4487\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 17s 169ms/step - loss: 1.3393 - accuracy: 0.4766\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 17s 165ms/step - loss: 1.3603 - accuracy: 0.4710\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 17s 165ms/step - loss: 1.2411 - accuracy: 0.5100\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 17s 167ms/step - loss: 1.1827 - accuracy: 0.5487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c882b8390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_hDIVlEZRw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab026a8-227e-4e2b-b660-53b1e65673ec"
      },
      "source": [
        "loss, acc = model_5.evaluate(val_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 129ms/step - loss: 1.9813 - accuracy: 0.2386\n",
            "Loss 1.98, accuracy 23.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2CbnErkX9oF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f8b2d2-c24b-49d1-b765-195005618ede"
      },
      "source": [
        "loss, acc = model_5.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 141ms/step - loss: 1.9753 - accuracy: 0.2493\n",
            "Loss 1.98, accuracy 24.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Uo-Uoth2xW"
      },
      "source": [
        "### Model 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrpR8yDlduw4"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MCABcWxYe3H"
      },
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_6.add(Convolution2D(4, 2, input_shape=(image_size, image_size, 3), activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "model_6.add(Dropout(0.5))\n",
        "model_6.add(Convolution2D(16, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "model_6.add(Dropout(0.5))\n",
        "model_6.add(Convolution2D(32, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "# model_4.add(Dropout(0.5))\n",
        "model_6.add(Flatten())\n",
        "model_6.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_6.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqD8fl7CYrTn",
        "outputId": "f1a030d4-a765-48ea-9888-1f5572adc745"
      },
      "source": [
        "model_6.fit(train_dataset_64, epochs=10)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 20s 189ms/step - loss: 23.0424 - accuracy: 0.2116\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.7336 - accuracy: 0.2409\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.7290 - accuracy: 0.2443\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.7343 - accuracy: 0.2409\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 19s 189ms/step - loss: 1.7346 - accuracy: 0.2394\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7325 - accuracy: 0.2394\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7292 - accuracy: 0.2411\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.7302 - accuracy: 0.2412\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 19s 180ms/step - loss: 1.7302 - accuracy: 0.2446\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 19s 181ms/step - loss: 1.7319 - accuracy: 0.2414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c88163950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVLDWrGHZLOB",
        "outputId": "dc01ef70-07d4-40d4-895b-a63b4a5103bf"
      },
      "source": [
        "loss, acc = model_6.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 7s 153ms/step - loss: 1.7360 - accuracy: 0.2415\n",
            "Loss 1.74, accuracy 24.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0h4XmHdvYI"
      },
      "source": [
        "### MODEL 7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiYfmNMlc5Zq"
      },
      "source": [
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKy7FQYKdM1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591dee9e-5ae8-4295-bef5-1da686816d82"
      },
      "source": [
        "train_dataset_64, val_dataset_64, test_dataset_64 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=64, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waCdcJGJaHYR"
      },
      "source": [
        "kernel_size = 3\n",
        "image_size = 64\n",
        "model_7 = Sequential()\n",
        "model_7.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_7.add(Convolution2D(4, (kernel_size, kernel_size), input_shape=(image_size, image_size, 3)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(Convolution2D(4, (kernel_size, kernel_size)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
        "model_7.add(Convolution2D(8, (kernel_size, kernel_size)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(Convolution2D(8, (kernel_size, kernel_size)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_7.add(Convolution2D(16, (kernel_size, kernel_size)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(Convolution2D(16, (kernel_size, kernel_size)))\n",
        "model_7.add(Dense(50, activation=\"relu\"))\n",
        "model_7.add(MaxPooling2D(pool_size=(5, 5), strides=3))\n",
        "model_7.add(Flatten())\n",
        "model_7.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_7.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKvzPdiAds2n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7X6MAbBdFDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc89e21e-875a-4cc1-f7c2-b7dc3600f403"
      },
      "source": [
        "model_7.fit(train_dataset_64, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 20s 189ms/step - loss: 1.7924 - accuracy: 0.2142\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7464 - accuracy: 0.2388\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7362 - accuracy: 0.2370\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7299 - accuracy: 0.2444\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.7427 - accuracy: 0.2368\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.7321 - accuracy: 0.2402\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 19s 184ms/step - loss: 1.7305 - accuracy: 0.2443\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.7315 - accuracy: 0.2454\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.7307 - accuracy: 0.2416\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.7333 - accuracy: 0.2403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f452df0fb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0mQuO-d34y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf64615-b4de-4b04-b7bd-7d8bae0bd50d"
      },
      "source": [
        "loss, acc = model_7.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 140ms/step - loss: 1.7358 - accuracy: 0.2415\n",
            "Loss 1.74, accuracy 24.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc10VqihZnTD"
      },
      "source": [
        "### Model 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQgaqg7II4Km"
      },
      "source": [
        "image_size = 64\n",
        "model_8 = Sequential()\n",
        "model_8.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_8.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=2, activation= 'relu'))\n",
        "model_8.add(Convolution2D(8, 3,strides=1, activation= 'relu'))\n",
        "model_8.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_8.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_8.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_8.add(Flatten())\n",
        "model_8.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model_8.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbPnPwkKjsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daefac8e-bfb8-462f-cc6b-20dd5e156473"
      },
      "source": [
        "model_8.fit(train_dataset_64, epochs=20)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.7289 - accuracy: 0.2546\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.5786 - accuracy: 0.3330\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.5501 - accuracy: 0.3519\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.4830 - accuracy: 0.3987\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.4454 - accuracy: 0.4164\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.4167 - accuracy: 0.4370\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 11s 104ms/step - loss: 1.4064 - accuracy: 0.4383\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3930 - accuracy: 0.4433\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3772 - accuracy: 0.4496\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3482 - accuracy: 0.4615\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3813 - accuracy: 0.4513\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3531 - accuracy: 0.4577\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3095 - accuracy: 0.4850\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 11s 104ms/step - loss: 1.3265 - accuracy: 0.4812\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3409 - accuracy: 0.4733\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3099 - accuracy: 0.4651\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 12s 114ms/step - loss: 1.2809 - accuracy: 0.4948\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 11s 110ms/step - loss: 1.2755 - accuracy: 0.5014\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 12s 114ms/step - loss: 1.2690 - accuracy: 0.5075\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 12s 112ms/step - loss: 1.2503 - accuracy: 0.5043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c1a6df0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqgEkVjGLFio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b78ae-0d38-4b30-ccf3-f9bc298295d4"
      },
      "source": [
        "loss, acc = model_8.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 95ms/step - loss: 1.3564 - accuracy: 0.4700\n",
            "Loss 1.36, accuracy 47.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMp8ZdtPLgvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7168390e-9f21-45a7-92a2-05ae134f4116"
      },
      "source": [
        "model_8.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_11 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 31, 31, 4)         112       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 29, 29, 8)         296       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 6)                 3462      \n",
            "=================================================================\n",
            "Total params: 5,038\n",
            "Trainable params: 5,038\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpjC7Zz2i1AU"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4lM7k12tbpm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YIb8WuYi4rX"
      },
      "source": [
        "image_size = 64\n",
        "model_10 = Sequential()\n",
        "model_10.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_10.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=2, activation= 'relu'))\n",
        "model_10.add(Convolution2D(8, 3,strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_10.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_10.add(Flatten())\n",
        "model_10.add(Dense(128, activation='relu'))\n",
        "model_10.add(Dropout(0.5))\n",
        "model_10.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model_10.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYJXy2hMj0BW",
        "outputId": "ee6db1e4-c551-46e9-caec-29cd7de8473b"
      },
      "source": [
        "model_10.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_25 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_83 (Conv2D)           (None, 31, 31, 4)         112       \n",
            "_________________________________________________________________\n",
            "conv2d_84 (Conv2D)           (None, 29, 29, 8)         296       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_85 (Conv2D)           (None, 12, 12, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 76,206\n",
            "Trainable params: 76,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmrrqdggjAhy",
        "outputId": "e7fb3244-443f-45ff-f522-3328e51d83a7"
      },
      "source": [
        "model_10.fit(train_dataset_64, epochs=50)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.7109 - accuracy: 0.2640\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.5476 - accuracy: 0.3603\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 11s 109ms/step - loss: 1.4872 - accuracy: 0.3764\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.4507 - accuracy: 0.4143\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.4093 - accuracy: 0.4332\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3941 - accuracy: 0.4407\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3824 - accuracy: 0.4376\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.3368 - accuracy: 0.4566\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3456 - accuracy: 0.4583\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3537 - accuracy: 0.4631\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.3045 - accuracy: 0.4732\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.2928 - accuracy: 0.4740\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.2560 - accuracy: 0.4977\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 11s 109ms/step - loss: 1.2349 - accuracy: 0.5145\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.2326 - accuracy: 0.5033\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.1944 - accuracy: 0.5291\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.2157 - accuracy: 0.5169\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.2115 - accuracy: 0.5190\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.1372 - accuracy: 0.5513\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.1473 - accuracy: 0.5444\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.1639 - accuracy: 0.5364\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.1226 - accuracy: 0.5663\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 11s 109ms/step - loss: 1.0934 - accuracy: 0.5692\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.0619 - accuracy: 0.5793\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 12s 111ms/step - loss: 1.1022 - accuracy: 0.5711\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 11s 109ms/step - loss: 1.0329 - accuracy: 0.5915\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 11s 109ms/step - loss: 0.9870 - accuracy: 0.6086\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.0076 - accuracy: 0.6060\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 1.0683 - accuracy: 0.5737\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 0.9388 - accuracy: 0.6458\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 0.9662 - accuracy: 0.6309\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 0.9278 - accuracy: 0.6308\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.9125 - accuracy: 0.6508\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 0.9207 - accuracy: 0.6386\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.8675 - accuracy: 0.6663\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 0.8629 - accuracy: 0.6524\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 0.8350 - accuracy: 0.6820\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.8193 - accuracy: 0.6834\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 0.8166 - accuracy: 0.6743\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.8181 - accuracy: 0.6787\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 0.7744 - accuracy: 0.6950\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.7744 - accuracy: 0.6886\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 0.7266 - accuracy: 0.7178\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.7654 - accuracy: 0.7091\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 0.6925 - accuracy: 0.7257\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 0.6796 - accuracy: 0.7367\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 11s 110ms/step - loss: 0.6612 - accuracy: 0.7409\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 0.6797 - accuracy: 0.7268\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 0.6464 - accuracy: 0.7402\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 11s 108ms/step - loss: 0.6515 - accuracy: 0.7403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c940f2ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM89AiMhluS7",
        "outputId": "20c98c37-8b99-4b68-e565-ef6030c39277"
      },
      "source": [
        "loss, acc = model_10.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 91ms/step - loss: 1.7002 - accuracy: 0.5019\n",
            "Loss 1.7, accuracy 50.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RidbDkp5Z9qd"
      },
      "source": [
        "### Model 9: AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6PZyzD5agSW",
        "outputId": "1f04f89d-e7f5-4b26-f269-a54c63ddd137"
      },
      "source": [
        "train_dataset_128, val_dataset_256, test_dataset_256 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=256, batch_size=32)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cNg64tCMZNB"
      },
      "source": [
        "image_size = 256\n",
        "model_9 = Sequential()\n",
        "model_9.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_9.add(Convolution2D(96, 7, input_shape=(image_size, image_size, 3), strides=1, activation= 'relu'))\n",
        "model_9.add(Convolution2D(128, 5, strides=2, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_9.add(Convolution2D(256, 5, strides=1, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_9.add(Convolution2D(256, 3, strides=1, activation= 'relu'))\n",
        "model_9.add(Convolution2D(512, 3, strides=1, activation= 'relu'))\n",
        "# model_9.add(Convolution2D(512, 3, strides=1, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_9.add(Flatten())\n",
        "model_9.add(Dense(512, activation='relu'))\n",
        "model_9.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model_9.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk3EwCrddFo6",
        "outputId": "81b03e43-7045-4fee-f4b4-68350d8acd16"
      },
      "source": [
        "model_9.summary()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_22 (Rescaling)     (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 250, 250, 96)      14208     \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 123, 123, 128)     307328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 61, 61, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 57, 57, 256)       819456    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 26, 26, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 24, 24, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 73728)             0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 512)               37749248  \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 40,663,558\n",
            "Trainable params: 40,663,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqkgj_hycu0W"
      },
      "source": [
        "model_9.fit(train_dataset_256, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJzOLNTpezJL"
      },
      "source": [
        "loss, acc = model_9.evaluate(test_dataset_256)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU2gG22CtdFs"
      },
      "source": [
        "### Model 11:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGmJMw3vFyA",
        "outputId": "a339061e-4e72-414b-ef96-7be00eab4a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset_128, val_dataset_128, test_dataset_128 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=64)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsqLDfE6tmGE"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEZoGayAvN-j"
      },
      "source": [
        "image_size = 128\n",
        "model_11 = Sequential()\n",
        "model_11.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_11.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
        "model_11.add(Convolution2D(8, 3,strides=2, activation= 'relu'))\n",
        "model_11.add(Dropout(0.5))\n",
        "model_11.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=3, strides=2))\n",
        "model_11.add(Convolution2D(16, 3,strides=2, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=2, strides=1))\n",
        "model_11.add(Flatten())\n",
        "model_11.add(Dense(1024, activation='relu'))\n",
        "model_11.add(Dropout(0.7))\n",
        "model_11.add(Dense(256, activation='relu'))\n",
        "model_11.add(Dropout(0.7))\n",
        "model_11.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_11.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i1YVr7ww0q_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHSVzCJPwrgm",
        "outputId": "73a53a0c-60ae-45ec-fdab-77c45c33189d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_11.summary()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_40 (Rescaling)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_141 (Conv2D)          (None, 126, 126, 4)       112       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_100 (MaxPoolin (None, 125, 125, 4)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_142 (Conv2D)          (None, 62, 62, 8)         296       \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 62, 62, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_143 (Conv2D)          (None, 60, 60, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_101 (MaxPoolin (None, 29, 29, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_144 (Conv2D)          (None, 14, 14, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_102 (MaxPoolin (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_39 (Flatten)         (None, 2704)              0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1024)              2769920   \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 3,037,758\n",
            "Trainable params: 3,037,758\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC2HEiGBxDZ-",
        "outputId": "3dba975a-735f-4941-cda5-6b966faf518e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_11.fit(train_dataset_128, epochs=20)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 44s 448ms/step - loss: 2.0111 - accuracy: 0.2342\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 43s 444ms/step - loss: 1.7251 - accuracy: 0.2481\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 42s 442ms/step - loss: 1.6612 - accuracy: 0.2585\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 43s 449ms/step - loss: 1.6241 - accuracy: 0.2924\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 43s 448ms/step - loss: 1.5824 - accuracy: 0.3137\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 44s 455ms/step - loss: 1.5341 - accuracy: 0.3515\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 44s 455ms/step - loss: 1.5409 - accuracy: 0.3477\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 44s 459ms/step - loss: 1.5757 - accuracy: 0.3329\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 44s 456ms/step - loss: 1.4987 - accuracy: 0.3703\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 44s 460ms/step - loss: 1.4530 - accuracy: 0.4122\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 44s 462ms/step - loss: 1.4643 - accuracy: 0.3885\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 44s 463ms/step - loss: 1.4447 - accuracy: 0.4005\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 44s 463ms/step - loss: 1.4503 - accuracy: 0.4044\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 44s 460ms/step - loss: 1.4258 - accuracy: 0.4276\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 44s 458ms/step - loss: 1.4297 - accuracy: 0.4161\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 44s 456ms/step - loss: 1.4174 - accuracy: 0.4123\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 44s 453ms/step - loss: 1.4037 - accuracy: 0.4268\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 43s 453ms/step - loss: 1.3868 - accuracy: 0.4367\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 43s 450ms/step - loss: 1.3976 - accuracy: 0.4361\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 43s 447ms/step - loss: 1.3923 - accuracy: 0.4328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c86f43d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it794Ja9xPdU",
        "outputId": "66e12d08-46a7-42fa-c6c2-f2884844a701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss, acc = model_11.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 107ms/step - loss: 1.4830 - accuracy: 0.3838\n",
            "Loss 1.46, accuracy 40.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlSt0kaq5ctf"
      },
      "source": [
        "### Model: ImageGenerator and Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21C4u-xw6R08"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "def create_dataset_image_generator(traindir=TRAINDIR, valdir=VALDIR, testdir=TESTDIR, image_size=32, batch_size=64):\n",
        "  label_mode = 'categorical'\n",
        "  train_datagen = ImageDataGenerator(\n",
        "    zoom_range=0.2,\n",
        "    rotation_range = 5,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "\n",
        "  train_dataset = train_datagen.flow_from_directory(\n",
        "    traindir,\n",
        "    target_size = (image_size,image_size),\n",
        "    class_mode='categorical',\n",
        "    batch_size = batch_size,\n",
        "\n",
        "  )\n",
        "\n",
        "  val_dataset = image_dataset_from_directory(\n",
        "    valdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "\n",
        "  test_dataset = image_dataset_from_directory(\n",
        "    testdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orhMGmYL5kuH",
        "outputId": "c6c7d678-645d-4fed-a4e5-7978d3e07d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset_128, val_dataset_128, test_dataset_128 = create_dataset_image_generator(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=64)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 images belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLkea_yS8yHE"
      },
      "source": [
        "image_size = 128\n",
        "model_12 = Sequential()\n",
        "model_12.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_12.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
        "model_12.add(Convolution2D(8, 3,strides=2, activation= 'relu'))\n",
        "# model_12.add(Dropout(0.5))\n",
        "model_12.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=3, strides=2))\n",
        "model_12.add(Convolution2D(16, 3,strides=2, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=2, strides=1))\n",
        "model_12.add(Flatten())\n",
        "model_12.add(Dense(1024, activation='relu'))\n",
        "model_12.add(Dropout(0.5))\n",
        "model_12.add(Dense(256, activation='relu'))\n",
        "model_12.add(Dropout(0.5))\n",
        "model_12.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_12.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzJdNUrP9L0H",
        "outputId": "307edb47-fb98-4fd5-91fe-5ba9c7a07cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model_12.fit(train_dataset_128, epochs=20)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 45s 464ms/step - loss: 1.8244 - accuracy: 0.2221\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 45s 472ms/step - loss: 1.7450 - accuracy: 0.2190\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 44s 459ms/step - loss: 1.7304 - accuracy: 0.2447\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 44s 460ms/step - loss: 1.7329 - accuracy: 0.2422\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 44s 461ms/step - loss: 1.7295 - accuracy: 0.2457\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 44s 461ms/step - loss: 1.7351 - accuracy: 0.2377\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 44s 458ms/step - loss: 1.7321 - accuracy: 0.2499\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 44s 464ms/step - loss: 1.7279 - accuracy: 0.2496\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 46s 477ms/step - loss: 1.7309 - accuracy: 0.2436\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 45s 469ms/step - loss: 1.7343 - accuracy: 0.2342\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 44s 461ms/step - loss: 1.7302 - accuracy: 0.2543\n",
            "Epoch 12/20\n",
            "79/96 [=======================>......] - ETA: 7s - loss: 1.7293 - accuracy: 0.2443"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-d8d5b76ec65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8PzKoIW9SG6"
      },
      "source": [
        "loss, acc = model_12.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMeaVHst9x2r"
      },
      "source": [
        "from tf.keras.callbacks import EarlyStopping\n",
        "\n",
        "callback = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# history = model_12.fit(train_dataset_128, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLe9mFwhRKC"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5xs6_RhRKC"
      },
      "source": [
        "While designing your own network might produce some nice results, it is generally better to transfer the knowledge available in a pre-trained network. This not only can produce better results, but also saves a lot of design time. The [Keras Applications](https://keras.io/api/applications/) module contains several network designs ready to use. For instance, to exploit the famous VGG16 network we do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTLIqMTAhRKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dad0ca1-a3f4-4fa3-9a3c-597a8c1ac2e4"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUvx4zdhRKD"
      },
      "source": [
        "By default all Keras Applications networks are preloaded with the weights that were obtained from training the network over the [ImageNet dataset](http://www.image-net.org/). To adapt the network to our problem we need to specify the shape of our input images, and also remove the output layers (top) of the original network, since we have a different number of classes.\n",
        "\n",
        "Now, how do we do transfer learning over this network? We will show here how to implement the bottleneck features strategy. First, we will mark the VGG16 model as **non-trainable**, so that the weights remain frozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWsuQg0phRKD"
      },
      "source": [
        "vgg16_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljtS1PhRhRKE"
      },
      "source": [
        "Now we will build a neural network that includes the VGG16 model as one of its \"layers\". Since the VGG16 was trained with an specific way of normalizing the images, we will need to normalize our images in the same way. Conveniently, Keras also provides a function for doing VGG16-style normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjFpoNRIhRKE"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBk5uRChRKF"
      },
      "source": [
        "We can try this with some image ir our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ukW26shRKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebc344b-a988-4e99-a686-a1c89486335f"
      },
      "source": [
        "for X_batch, _ in train_dataset:\n",
        "    break\n",
        "    \n",
        "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
        "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before normalizing: [[[13.75  9.75  6.75]\n",
            "  [13.25 12.25 10.25]\n",
            "  [13.25  9.25  8.25]]\n",
            "\n",
            " [[16.5  12.5   9.5 ]\n",
            "  [15.75 10.75  7.75]\n",
            "  [19.25 14.25 11.25]]\n",
            "\n",
            " [[16.25 12.25  9.25]\n",
            "  [12.    8.    5.  ]\n",
            "  [21.75 17.75 13.25]]]\n",
            "After normalizing: [[[ -97.189 -107.029 -109.93 ]\n",
            "  [ -93.689 -104.529 -110.43 ]\n",
            "  [ -95.689 -107.529 -110.43 ]]\n",
            "\n",
            " [[ -94.439 -104.279 -107.18 ]\n",
            "  [ -96.189 -106.029 -107.93 ]\n",
            "  [ -92.689 -102.529 -104.43 ]]\n",
            "\n",
            " [[ -94.689 -104.529 -107.43 ]\n",
            "  [ -98.939 -108.779 -111.68 ]\n",
            "  [ -90.689  -99.029 -101.93 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx3EQeEThRKF"
      },
      "source": [
        "The normalization required by VGG16 involves swapping the order of color channels (RGB -> BGR) and substracting the mean values over the ImageNet dataset for each color channel separately. Fortunately the `preprocess_input` function does all the work for us. Furthermore, we can plug this function as the first layer of our network, similarly to the `Rescaling` we used before. We can do this with a `Lambda` layer, which allows building a layer out of any (differentiable!) function. Let's start our model with this layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnSoUUrQhRKG"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83-_MmdhRKG"
      },
      "source": [
        "After this, we can add the whole VGG16 network as a layer, and our custom trainable layers after it. Note this is an overly simple model with some mistakes introduced; a real transfer learning network would have a better design."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f0RdEumhRKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69694b14-3545-4b66-fc35-754a6712d2ab"
      },
      "source": [
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 6)                 12294     \n",
            "=================================================================\n",
            "Total params: 14,726,982\n",
            "Trainable params: 12,294\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NueAGF7DhRKG"
      },
      "source": [
        "Notice how in the model summary we can see the whole network has millions of parameters, but since we have frozen the VGG16 part, only a few thousand parameters will be trained: those in the Dense layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7iMKlQhRKH"
      },
      "source": [
        "We can now compile and train this model in the usual way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBG6zKwhRKH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "585d18d4-a947-47f7-98fd-b2843e46ec66"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
        "model.fit(train_dataset, epochs=1)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='lambda_input'), name='lambda_input', description=\"created by layer 'lambda_input'\"), but it was called on an input with incompatible shape (None, 32, 32, 3).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-de843f2458e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss {loss:.3}, accuracy {acc:.1%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer vgg16: expected shape=(None, 64, 64, 3), found shape=(None, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sb7SVSkhRKH"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Using the bottleneck strategy, implement a network doing transfer learning from VGG16. If you do it properly, you should be able to obtain better results than with your previously designed network, at least a 80% of accuracy over the test set.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2LQa-mhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Some tips and strategies that can help you optimize your network design:\n",
        "\n",
        "    \n",
        "- Include one or more Dense layers with the appropriate activation functions before the output layer.\n",
        "- Try using a [GlobalAveragePooling layer](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) instead of a Flatten layer. This layer computes an average of all pixel values for each channel, and sometimes performs better than a regular Flatten.\n",
        "- And remember all the tricks from the previous exercise!\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6bbHEWhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "You can further improve the network accuracy with the following ideas\n",
        "\n",
        "- Use the PRO strategies from the previous exercise.\n",
        "- Try other pre-trained networks from <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, such as ResNet, Xception or EfficientNet.\n",
        "- Use more advanced transfer learning strategies, like fine-tuning or a combination of bottleneck features and fine-tuning.\n",
        "   \n",
        "If you use all the tricks, it is possible to obtain more than 90% accuracy in the test set.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLJ8IgANhRKI"
      },
      "source": [
        "####### INSERT YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAqZScMGEo7B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc7DMJhRKK"
      },
      "source": [
        "## Summary of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2c2JCKhRKK"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Write in the following cell a short report explaining what network designs you tried, and what test accuracies you obtained. What worked and what didn't? What have you learned from this exercise?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofHCraehRKL"
      },
      "source": [
        "Example results table\n",
        "\n",
        "|Image processing|Neural network model|Training strategy|Test accuracy|\n",
        "|----------------|--------------------|-----------------|-------------|\n",
        "|Size 32x32, batch size 16|Convolutional(32) + Flatten + Dense(64)|Train from scratch|xx%|\n",
        "|Size 64x64, batch size 32|VGG16 + Flatten + Dense(32)|Bottleneck features|yy%|\n",
        "|...|...|...|...|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8mAq5ohrWyt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}