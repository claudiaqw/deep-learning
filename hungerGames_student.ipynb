{"nbformat": 4, "nbformat_minor": 0, "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "colab": {"name": "hungerGames.ipynb", "provenance": [], "toc_visible": true}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}, "id": "KX8zKrS-hRJk"}, "source": ["# Lab assignment: the hunger games"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}, "id": "vanYztAMhRJt"}, "source": ["<table><tr>\n", "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n", "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n", "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n", "</tr></table>"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}, "id": "6E7BW750hRJu"}, "source": ["In this assignment we will face a challenging image classification problem, building a deep learning model that is able to classify different kinds of foods. Let the hunger games begin!"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}, "id": "Mnn_FAm8hRJv"}, "source": ["## Guidelines"]}, {"cell_type": "markdown", "metadata": {"id": "gspUM6n3hRJw"}, "source": ["Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n", "\n", "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "sZHQpQXrhRJw"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "xghhJf_HhRJx"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "NWAbqrofhRJy"}, "source": ["To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Deep Learning environment files](https://github.com/albarji/teaching-environments/tree/master/deeplearning), or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."]}, {"cell_type": "markdown", "metadata": {"id": "mZmf8R9DhRJy"}, "source": ["The following code will embed any plots into the notebook instead of generating a new window:"]}, {"cell_type": "code", "metadata": {"id": "DxmdgWqNhRJz"}, "source": ["import matplotlib.pyplot as plt\n", "%matplotlib inline"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9UgDu1WUhRJ0"}, "source": ["Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n", "\n", "Let's go!"]}, {"cell_type": "markdown", "metadata": {"id": "K733uf0ghRJ2"}, "source": ["## Data acquisition"]}, {"cell_type": "markdown", "metadata": {"id": "5ZYPquwFhRJ2"}, "source": ["We will use a food images dataset available at [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). To download it you will need to create a user account in Kaggle, and obtain your API credential by following the instructions on [this section](https://www.kaggle.com/trolukovich/food11-image-dataset). Once you have your credentials JSON file, you can inform this notebook of them by setting the appropriate enviroment variables, as follows\n", "\n", "    import os\n", "\n", "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n", "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n", "    \n", "Once this is done, you will be able to download the dataset to this computer using the command\n", "\n", "    !kaggle datasets download trolukovich/food11-image-dataset --unzip -p YOUR_LOCAL_FOLDER\n", "    \n", "where you should write a valid directory in your computer in \"YOUR_LOCAL_FOLDER\". If you are fine with downloading the data in the same folder as this notebook, just skip the `-p YOUR_LOCAL_FOLDER` part of the command."]}, {"cell_type": "markdown", "metadata": {"id": "VRX8BQsZhRJ2"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "Create a Kaggle account, obtain your credentials, and use the cell below to declare your Kaggle username and key variables, and to download the dataset to a local folder.\n", "    \n", "These credentials should remain secret to you. Remember to delete them before submitting this notebook!\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wiFYSULfhRJ3", "outputId": "a3347815-af19-4c42-ceda-0eaea799676e"}, "source": ["####### INSERT YOUR CODE HERE"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Xmh27gIEhRJ4"}, "source": ["Take now a look into the folder where you downloaded the data. You will find it is made up of three subfolders:\n", "\n", "* **training**, containing the images to use to train the model.\n", "* **validation**, containing additional images you could use as more training data, or for some kind of validation strategy such as Early Stopping.\n", "* **evaluation**, containing the images you must use to test your model. Images in this folder can **only** be used to measure the model performance after the training procedure is completed.\n", "\n", "Furthermore, within each one of these folders you will find one folder for each one of the 11 classes in this problem:\n", "\n", "* Bread\n", "* Dairy product\n", "* Dessert\n", "* Egg\n", "* Fried food\n", "* Meat\n", "* Noodles-Pasta\n", "* Rice\n", "* Seafood\n", "* Soup\n", "* Vegetable-Fruit\n", "\n", "This is a standard structure for organizing image datasets: one folder per class. To easen the following data processing steps, let us define some variables telling us where the data is located."]}, {"cell_type": "markdown", "metadata": {"id": "MJrE8F8zhRJ4"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "    Create variables <b>TRAINDIR</b>, <b>VALDIR</b> and <b>TESTDIR</b> with the paths to the folders with the training, validation and evaluation data, respectively.\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "metadata": {"id": "TzK2VI3ShRJ5"}, "source": ["####### INSERT YOUR CODE HERE"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WtoTuArehRJ5"}, "source": ["Let's plot a random sample of training images from each class, using the ipyplot package. If you are running this notebook in Google Colab, you will need to install this package first with\n", "\n", "    !pip install ipyplot\n", "\n", "You can inspect each class by clicking the different tabs in the interface that will appear when running the following cell."]}, {"cell_type": "code", "metadata": {"scrolled": false, "colab": {"base_uri": "https://localhost:8080/", "height": 390}, "id": "MWCsZR3ahRJ5", "outputId": "f63cb22c-6bca-4354-96bd-cc8180cc98b9"}, "source": ["from glob import glob\n", "import ipyplot\n", "import numpy as np\n", "\n", "all_images = glob(f\"{TRAINDIR}/*/*.jpg\")  # Get all image paths\n", "np.random.shuffle(all_images)  # Randomize to show different images each run\n", "all_labels = [f.split(\"/\")[-2] for f in all_images]  # Extract class names from path\n", "\n", "ipyplot.plot_class_tabs(all_images, all_labels, max_imgs_per_tab=6, img_width=300, force_b64=True)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ikYtQyR-hRJ6"}, "source": ["### Class reduction"]}, {"cell_type": "markdown", "metadata": {"id": "AWsLot-7hRJ6"}, "source": ["To make the problem more approachable for this exercise, we will focus on just six classes: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` and `Meat`. To do so, we will delete from the downloaded data the folders from other classes."]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "TXoNw1-thRJ7", "outputId": "b7c0d715-1a4c-4ecc-b3ed-c89fcab1261b"}, "source": ["from glob import glob\n", "import os\n", "\n", "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n", "datasets = {TRAINDIR, VALDIR, TESTDIR}\n", "\n", "for dataset in datasets:\n", "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n", "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n", "            print(f\"Deleting {classdir}...\")\n", "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n", "                os.remove(fname)\n", "            os.rmdir(classdir)  # Remove folder"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fncCfOYihRJ7"}, "source": ["## Image processing from files"]}, {"cell_type": "markdown", "metadata": {"id": "6wTWrPr-hRJ7"}, "source": ["This dataset of images is large, with images of larger resolution than in the tutorial MNIST problem, each one having different sizes and image ratios. Also, while for MNIST we had a keras function that prepared the dataset for us, this time we will need to do some loading and image processing work.\n", "\n", "A convenient way to do this work is through the use of Keras `image_dataset_from_directory` function. This function creates a TensorFlow `Dataset` with the images in the directory, loading images dynamically only when the neural network needs to use them, and also allowing us to specify some useful preprocessing options.\n", "\n", "For example, we can create such a `Dataset` with the data in the training folder:"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-edpP7-NhRJ8", "outputId": "5e90e1d6-c46a-4cc6-d8cb-775fa28b5050"}, "source": ["from keras.preprocessing import image_dataset_from_directory\n", "\n", "image_size = 32\n", "batch_size = 64\n", "\n", "train_dataset = image_dataset_from_directory(\n", "    TRAINDIR, \n", "    image_size = (image_size, image_size),\n", "    batch_size = batch_size, \n", "    label_mode = 'categorical'\n", ")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dGEdEuF5hRJ8"}, "source": ["Note the parameters used to configure the dataset:\n", "\n", "* The **directory** from which to load the images.\n", "* An **image size** that will be used to resize all the images to a common resolution, here 32x32.\n", "* The **size of the batches** of images to be generated. Note we define this parameter here instead in the network fit step, as the `Dataset` will make use of this information to keep in memory only a few batches of images at the same time in order to save memory.\n", "* The **label mode**, that is, the encoding used for the labels. `categorical` means we will use one-hot encoding."]}, {"cell_type": "markdown", "metadata": {"id": "jgeGjQDVhRJ9"}, "source": ["A `Dataset` object works like a Python generator, which means we can iterate over it to obtain batches of processed images. For instance, to get the first batch"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "bPAIrHu7hRJ9", "outputId": "ac888d83-1dfa-4f98-93ac-2aa639c6a801"}, "source": ["for X_batch, y_batch in train_dataset:\n", "    print(f\"Shape of input batch: {X_batch.shape}\")\n", "    print(f\"Shape of output batch: {y_batch.shape}\")\n", "    print(f\"Input batch:\\n{X_batch}\")\n", "    print(f\"Output batch:\\n{y_batch}\")\n", "    break"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "K2UaPCEthRJ-"}, "source": ["We can see that indeed the generator produces a tensor of the appropriate dimensions with the inputs for the neural network, and that the outputs have also been properly codified in one-hot form. However, there is still an issue with the data: the pixel values are in the range [0, 255], which might produce training problems. We will solve this later in the network definition. For now, let's move on and prepare a funcion that builds the training, validation and test datasets."]}, {"cell_type": "markdown", "metadata": {"id": "jTQyBbf-hRJ-"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "    Create a function <b>create_datasets</b> that receives the following parameters:\n", "    <ul>\n", "      <li><b>traindir</b>: the directory where training images are located.</li>\n", "      <li><b>valdir</b>: the directory where validation images are located.</li>\n", "      <li><b>testdir</b>: the directory where test images are located.</li>\n", "      <li><b>image_size</b>: the size that will be used to resize all the images to a common resolution.</li>\n", "      <li><b>batch_size</b>: the size of the batches of images to be generated.</li>\n", "    </ul>\n", "    The function must create datasets for the training, validation and test directories, and return the three datasets created.\n", "\n", "    return train_dataset, val_dataset, test_dataset\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "metadata": {"id": "D-wm19eJhRJ-"}, "source": ["####### INSERT YOUR CODE HERE"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5wGaULQsjwJz"}, "source": ["Let's test if the function you just implemented works correctly"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JtZlNbHhiOIR", "outputId": "82b2d44f-2b25-4956-c7f5-a45fe346a352"}, "source": ["import tensorflow as tf\n", "\n", "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=64)\n", "\n", "# Test whether all returned objects are valid Tensorflow datasets\n", "assert isinstance(train_dataset, tf.data.Dataset)\n", "assert isinstance(val_dataset, tf.data.Dataset)\n", "assert isinstance(test_dataset, tf.data.Dataset)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LLIo6LBfhRJ-"}, "source": ["Now that we have our datasets we can train a deep learning model using them! For illustration purposes, let's build an extremely simple convolutional network. Note how we have added a special pre-processing layer `Rescaling` that takes care of normalizing the data to the range [0, 1].\n", "\n", "Be careful! This network will work, but has some flaws in its design you might want to fix in the network you will desing later in this notebook."]}, {"cell_type": "code", "metadata": {"id": "gbwmYJiLhRJ_"}, "source": ["from keras.models import Sequential\n", "from keras.layers.core import Dense, Activation, Flatten, Dropout\n", "from keras.layers.convolutional import Convolution2D\n", "from keras.layers.experimental.preprocessing import Rescaling\n", "\n", "model = Sequential()\n", "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n", "model.add(Convolution2D(4, 3, activation='linear'))\n", "model.add(Flatten())\n", "model.add(Dense(6, activation='sigmoid'))\n", "\n", "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4v9OxA-AhRJ_"}, "source": ["The `fit` method of a Keras model can receive a `Dataset` with training data, instead of a pair of tensors with (inputs, outputs). Since when building the `Dataset` we already specified the batch size, we don't need to do it now."]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_ZXjs6CghRKA", "outputId": "e5736dc1-1676-49ca-fc93-f850dcacd055"}, "source": ["model.fit(train_dataset, epochs=1)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "T_ux64pchRKA"}, "source": ["Similarly, we can evaluate the performance of our model over our test `Dataset` as follows"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Q_kQhkY-hRKA", "outputId": "ae5e20be-6ce9-48b3-f99d-a905bd367d2c"}, "source": ["loss, acc = model.evaluate(test_dataset)\n", "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "AlXf9OBehRKB"}, "source": ["The accuracy might seem poor, but take into account we have used a very simple model and this problem has 6 classes. Will you be able to do better?"]}, {"cell_type": "markdown", "metadata": {"id": "1kZnaNQIhRKB"}, "source": ["## Building your network"]}, {"cell_type": "markdown", "metadata": {"id": "uNd8dC2whRKB"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "    Design a neural network that maximizes the accuracy over the test data. You can use the training and validation datasets for anything you like, but you can <b>only</b> use the test data to evaluate the model performance. You should obtain a network able to attain at least 40% accuracy over the test set.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "s9C9GNYihRKC"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "    \n", "Some tips and strategies that can help you optimize your network design:\n", "\n", "    \n", "- Make use of all the tricks you learned from previous notebooks: convolutional + pooling layers, ReLU activations, dropout... also make sure to use a good optimizer with an adequate loss function, as well as the correct activation for the output layer.\n", "- Configuring the datasets to load the images with a larger size can significantly improve your performance. But be careful, you can also run out of memory (CUDA memory error) if they become too large! Note that for this problem a size larger than 256 might be too large.\n", "- Start with networks with a small number of parameters, so you are able to check fast how well they work. Then you can make your network larger in three directions: larger input images, more layers and more kernels per convolutional layer or units per dense layer. If you use larger images make sure to add more Convolution+Pooling layers, so that only very small images (less than 10x10 pixels) arrive at the Flatten layer.\n", "- If you see large differences in loss between your training data and your validation or test data, try increasing the Dropout probabilities, especially for the Dense layers.\n", "- Make use of that validation data! For instance, use an <a href=\"https://keras.io/api/callbacks/early_stopping/\">**EarlyStopping strategy**</a> to monitor the loss of these validation data, and stop when training when after a number of iterations such loss has not decreased. Configuring the EarlyStopping to restore the best weights found in the optimization is also useful.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "ssDs0SGghRKC"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "    \n", "Other advanced strategies you can try are:\n", "\n", "- Use **image augmentation techniques** to artifically create new training images. To do so, explore the rest of layers available in the <a href=\"https://keras.io/api/layers/preprocessing_layers/image_preprocessing/\">Keras Image Preprocessing module</a>.\n", "- Use <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> layers to improve the optimization procedure.\n", "    \n", "It you use all the tricks, it is possible to obtain more than 60% accuracy in the test set.\n", "\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "metadata": {"id": "Jgz7w3VqhRKC"}, "source": ["####### INSERT YOUR CODE HERE"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "HrLe9mFwhRKC"}, "source": ["## Transfer learning"]}, {"cell_type": "markdown", "metadata": {"id": "7a5xs6_RhRKC"}, "source": ["While designing your own network might produce some nice results, it is generally better to transfer the knowledge available in a pre-trained network. This not only can produce better results, but also saves a lot of design time. The [Keras Applications](https://keras.io/api/applications/) module contains several network designs ready to use. For instance, to exploit the famous VGG16 network we do"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fTLIqMTAhRKD", "outputId": "500a35e0-6173-45d2-f4e9-8fa1e5d7bebe"}, "source": ["from keras.applications import VGG16\n", "\n", "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "_ZUvx4zdhRKD"}, "source": ["By default all Keras Applications networks are preloaded with the weights that were obtained from training the network over the [ImageNet dataset](http://www.image-net.org/). To adapt the network to our problem we need to specify the shape of our input images, and also remove the output layers (top) of the original network, since we have a different number of classes.\n", "\n", "Now, how do we do transfer learning over this network? We will show here how to implement the bottleneck features strategy. First, we will mark the VGG16 model as **non-trainable**, so that the weights remain frozen"]}, {"cell_type": "code", "metadata": {"id": "iWsuQg0phRKD"}, "source": ["vgg16_model.trainable = False"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ljtS1PhRhRKE"}, "source": ["Now we will build a neural network that includes the VGG16 model as one of its \"layers\". Since the VGG16 was trained with an specific way of normalizing the images, we will need to normalize our images in the same way. Conveniently, Keras also provides a function for doing VGG16-style normalization."]}, {"cell_type": "code", "metadata": {"id": "ZjFpoNRIhRKE"}, "source": ["from keras.applications.vgg16 import preprocess_input"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dTBk5uRChRKF"}, "source": ["We can try this with some image ir our dataset"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "V6ukW26shRKF", "outputId": "80a68b4c-729c-4165-b932-fe5aced6f4d3"}, "source": ["for X_batch, _ in train_dataset:\n", "    break\n", "    \n", "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n", "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sx3EQeEThRKF"}, "source": ["The normalization required by VGG16 involves swapping the order of color channels (RGB -> BGR) and substracting the mean values over the ImageNet dataset for each color channel separately. Fortunately the `preprocess_input` function does all the work for us. Furthermore, we can plug this function as the first layer of our network, similarly to the `Rescaling` we used before. We can do this with a `Lambda` layer, which allows building a layer out of any (differentiable!) function. Let's start our model with this layer."]}, {"cell_type": "code", "metadata": {"id": "DnSoUUrQhRKG"}, "source": ["from keras.layers import Lambda\n", "\n", "model = Sequential()\n", "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "k83-_MmdhRKG"}, "source": ["After this, we can add the whole VGG16 network as a layer, and our custom trainable layers after it. Note this is an overly simple model with some mistakes introduced; a real transfer learning network would have a better design."]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9f0RdEumhRKG", "outputId": "5e61046a-1c5f-46db-8a7a-45ccd2dd3cbf"}, "source": ["model.add(vgg16_model)\n", "model.add(Flatten())\n", "model.add(Dense(6, activation='sigmoid'))\n", "model.summary()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "NueAGF7DhRKG"}, "source": ["Notice how in the model summary we can see the whole network has millions of parameters, but since we have frozen the VGG16 part, only a few thousand parameters will be trained: those in the Dense layer."]}, {"cell_type": "markdown", "metadata": {"id": "qn7iMKlQhRKH"}, "source": ["We can now compile and train this model in the usual way"]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "5eBG6zKwhRKH", "outputId": "0ab377f4-e29f-4921-b2ea-879c9e6d3880"}, "source": ["model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n", "model.fit(train_dataset, epochs=1)\n", "\n", "loss, acc = model.evaluate(test_dataset)\n", "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9Sb7SVSkhRKH"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "    Using the bottleneck strategy, implement a network doing transfer learning from VGG16. If you do it properly, you should be able to obtain better results than with your previously designed network, at least a 80% of accuracy over the test set.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "rm2LQa-mhRKI"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "    \n", "Some tips and strategies that can help you optimize your network design:\n", "\n", "    \n", "- Include one or more Dense layers with the appropriate activation functions before the output layer.\n", "- Try using a [GlobalAveragePooling layer](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) instead of a Flatten layer. This layer computes an average of all pixel values for each channel, and sometimes performs better than a regular Flatten.\n", "- And remember all the tricks from the previous exercise!\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "sl6bbHEWhRKI"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "    \n", "You can further improve the network accuracy with the following ideas\n", "\n", "- Use the PRO strategies from the previous exercise.\n", "- Try other pre-trained networks from <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, such as ResNet, Xception or EfficientNet.\n", "- Use more advanced transfer learning strategies, like fine-tuning or a combination of bottleneck features and fine-tuning.\n", "   \n", "If you use all the tricks, it is possible to obtain more than 90% accuracy in the test set.\n", "\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "metadata": {"id": "qLJ8IgANhRKI"}, "source": ["####### INSERT YOUR CODE HERE"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fVJc7DMJhRKK"}, "source": ["## Summary of results"]}, {"cell_type": "markdown", "metadata": {"id": "PV2c2JCKhRKK"}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "Write in the following cell a short report explaining what network designs you tried, and what test accuracies you obtained. What worked and what didn't? What have you learned from this exercise?\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {"id": "EofHCraehRKL"}, "source": ["Example results table\n", "\n", "|Image processing|Neural network model|Training strategy|Test accuracy|\n", "|----------------|--------------------|-----------------|-------------|\n", "|Size 32x32, batch size 16|Convolutional(32) + Flatten + Dense(64)|Train from scratch|xx%|\n", "|Size 64x64, batch size 32|VGG16 + Flatten + Dense(32)|Bottleneck features|yy%|\n", "|...|...|...|...|"]}]}