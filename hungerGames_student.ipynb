{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "hungerGames.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiaqw/deep-learning/blob/main/hungerGames_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "KX8zKrS-hRJk"
      },
      "source": [
        "# Lab assignment: the hunger games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "vanYztAMhRJt"
      },
      "source": [
        "<table><tr>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "6E7BW750hRJu"
      },
      "source": [
        "In this assignment we will face a challenging image classification problem, building a deep learning model that is able to classify different kinds of foods. Let the hunger games begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "Mnn_FAm8hRJv"
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspUM6n3hRJw"
      },
      "source": [
        "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHQpQXrhRJw"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghhJf_HhRJx"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAbqrofhRJy"
      },
      "source": [
        "To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Deep Learning environment files](https://github.com/albarji/teaching-environments/tree/master/deeplearning), or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmf8R9DhRJy"
      },
      "source": [
        "The following code will embed any plots into the notebook instead of generating a new window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxmdgWqNhRJz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UgDu1WUhRJ0"
      },
      "source": [
        "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K733uf0ghRJ2"
      },
      "source": [
        "## Data acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYPquwFhRJ2"
      },
      "source": [
        "We will use a food images dataset available at [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). To download it you will need to create a user account in Kaggle, and obtain your API credential by following the instructions on [this section](https://www.kaggle.com/trolukovich/food11-image-dataset). Once you have your credentials JSON file, you can inform this notebook of them by setting the appropriate enviroment variables, as follows\n",
        "\n",
        "    import os\n",
        "\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n",
        "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n",
        "    \n",
        "Once this is done, you will be able to download the dataset to this computer using the command\n",
        "\n",
        "    !kaggle datasets download trolukovich/food11-image-dataset --unzip -p YOUR_LOCAL_FOLDER\n",
        "    \n",
        "where you should write a valid directory in your computer in \"YOUR_LOCAL_FOLDER\". If you are fine with downloading the data in the same folder as this notebook, just skip the `-p YOUR_LOCAL_FOLDER` part of the command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRX8BQsZhRJ2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Create a Kaggle account, obtain your credentials, and use the cell below to declare your Kaggle username and key variables, and to download the dataset to a local folder.\n",
        "    \n",
        "These credentials should remain secret to you. Remember to delete them before submitting this notebook!\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiFYSULfhRJ3",
        "outputId": "c8da0e19-f531-465a-85f5-0af4ad1c24b2"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "import os\n",
        "credentials = {\"username\":\"claudiaqw\",\n",
        "               \"key\":\"7a4a42e84feb6a7da837ebf98550b12b\"}\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']=credentials[\"username\"]\n",
        "os.environ['KAGGLE_KEY']=credentials[\"key\"]\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading food11-image-dataset.zip to /content\n",
            " 99% 1.07G/1.08G [00:06<00:00, 170MB/s]\n",
            "100% 1.08G/1.08G [00:06<00:00, 167MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh27gIEhRJ4"
      },
      "source": [
        "Take now a look into the folder where you downloaded the data. You will find it is made up of three subfolders:\n",
        "\n",
        "* **training**, containing the images to use to train the model.\n",
        "* **validation**, containing additional images you could use as more training data, or for some kind of validation strategy such as Early Stopping.\n",
        "* **evaluation**, containing the images you must use to test your model. Images in this folder can **only** be used to measure the model performance after the training procedure is completed.\n",
        "\n",
        "Furthermore, within each one of these folders you will find one folder for each one of the 11 classes in this problem:\n",
        "\n",
        "* Bread\n",
        "* Dairy product\n",
        "* Dessert\n",
        "* Egg\n",
        "* Fried food\n",
        "* Meat\n",
        "* Noodles-Pasta\n",
        "* Rice\n",
        "* Seafood\n",
        "* Soup\n",
        "* Vegetable-Fruit\n",
        "\n",
        "This is a standard structure for organizing image datasets: one folder per class. To easen the following data processing steps, let us define some variables telling us where the data is located."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrE8F8zhRJ4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Create variables <b>TRAINDIR</b>, <b>VALDIR</b> and <b>TESTDIR</b> with the paths to the folders with the training, validation and evaluation data, respectively.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzK2VI3ShRJ5"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "TRAINDIR, VALDIR, TESTDIR = '/content/training', '/content/validation', '/content/evaluation'\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtoTuArehRJ5"
      },
      "source": [
        "Let's plot a random sample of training images from each class, using the ipyplot package. If you are running this notebook in Google Colab, you will need to install this package first with\n",
        "\n",
        "    !pip install ipyplot\n",
        "\n",
        "You can inspect each class by clicking the different tabs in the interface that will appear when running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQams_NrqSEo"
      },
      "source": [
        "# !pip install ipyplot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MWCsZR3ahRJ5"
      },
      "source": [
        "# from glob import glob\n",
        "# import ipyplot\n",
        "# import numpy as np\n",
        "\n",
        "# all_images = glob(f\"{TRAINDIR}/*/*.jpg\")  # Get all image paths\n",
        "# np.random.shuffle(all_images)  # Randomize to show different images each run\n",
        "# all_labels = [f.split(\"/\")[-2] for f in all_images]  # Extract class names from path\n",
        "\n",
        "# ipyplot.plot_class_tabs(all_images, all_labels, max_imgs_per_tab=6, img_width=300, force_b64=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYtQyR-hRJ6"
      },
      "source": [
        "### Class reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsLot-7hRJ6"
      },
      "source": [
        "To make the problem more approachable for this exercise, we will focus on just six classes: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` and `Meat`. To do so, we will delete from the downloaded data the folders from other classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXoNw1-thRJ7",
        "outputId": "c6640d92-a967-4915-857b-7dc03134534b"
      },
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting /content/training/Rice...\n",
            "Deleting /content/training/Soup...\n",
            "Deleting /content/training/Noodles-Pasta...\n",
            "Deleting /content/training/Seafood...\n",
            "Deleting /content/training/Vegetable-Fruit...\n",
            "Deleting /content/evaluation/Rice...\n",
            "Deleting /content/evaluation/Soup...\n",
            "Deleting /content/evaluation/Noodles-Pasta...\n",
            "Deleting /content/evaluation/Seafood...\n",
            "Deleting /content/evaluation/Vegetable-Fruit...\n",
            "Deleting /content/validation/Rice...\n",
            "Deleting /content/validation/Soup...\n",
            "Deleting /content/validation/Noodles-Pasta...\n",
            "Deleting /content/validation/Seafood...\n",
            "Deleting /content/validation/Vegetable-Fruit...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncCfOYihRJ7"
      },
      "source": [
        "## Image processing from files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTWrPr-hRJ7"
      },
      "source": [
        "This dataset of images is large, with images of larger resolution than in the tutorial MNIST problem, each one having different sizes and image ratios. Also, while for MNIST we had a keras function that prepared the dataset for us, this time we will need to do some loading and image processing work.\n",
        "\n",
        "A convenient way to do this work is through the use of Keras `image_dataset_from_directory` function. This function creates a TensorFlow `Dataset` with the images in the directory, loading images dynamically only when the neural network needs to use them, and also allowing us to specify some useful preprocessing options.\n",
        "\n",
        "For example, we can create such a `Dataset` with the data in the training folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edpP7-NhRJ8",
        "outputId": "5634ef53-ad3e-4416-b3e6-7d161ff5e42f"
      },
      "source": [
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    TRAINDIR, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGEdEuF5hRJ8"
      },
      "source": [
        "Note the parameters used to configure the dataset:\n",
        "\n",
        "* The **directory** from which to load the images.\n",
        "* An **image size** that will be used to resize all the images to a common resolution, here 32x32.\n",
        "* The **size of the batches** of images to be generated. Note we define this parameter here instead in the network fit step, as the `Dataset` will make use of this information to keep in memory only a few batches of images at the same time in order to save memory.\n",
        "* The **label mode**, that is, the encoding used for the labels. `categorical` means we will use one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgeGjQDVhRJ9"
      },
      "source": [
        "A `Dataset` object works like a Python generator, which means we can iterate over it to obtain batches of processed images. For instance, to get the first batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAIrHu7hRJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a2ff20-79b9-48d7-dbb7-7ae6b1c9cfb4"
      },
      "source": [
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"Shape of input batch: {X_batch.shape}\")\n",
        "    print(f\"Shape of output batch: {y_batch.shape}\")\n",
        "    print(f\"Input batch:\\n{X_batch}\")\n",
        "    print(f\"Output batch:\\n{y_batch}\")\n",
        "    break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of input batch: (64, 32, 32, 3)\n",
            "Shape of output batch: (64, 6)\n",
            "Input batch:\n",
            "[[[[7.92500000e+01 5.32500000e+01 4.72500000e+01]\n",
            "   [8.12500000e+01 4.92500000e+01 4.07500000e+01]\n",
            "   [1.36250000e+02 1.39750000e+02 1.49750000e+02]\n",
            "   ...\n",
            "   [1.49000000e+02 1.57500000e+02 1.67500000e+02]\n",
            "   [1.46250000e+02 1.51250000e+02 1.55750000e+02]\n",
            "   [1.53000000e+02 1.56250000e+02 1.61750000e+02]]\n",
            "\n",
            "  [[7.60000000e+01 5.95000000e+01 5.55000000e+01]\n",
            "   [8.22500000e+01 4.70000000e+01 5.20000000e+01]\n",
            "   [1.48000000e+02 1.54000000e+02 1.66750000e+02]\n",
            "   ...\n",
            "   [1.33250000e+02 1.38750000e+02 1.46250000e+02]\n",
            "   [1.29250000e+02 1.36750000e+02 1.51250000e+02]\n",
            "   [1.07250000e+02 1.11750000e+02 1.19250000e+02]]\n",
            "\n",
            "  [[7.17500000e+01 6.27500000e+01 6.10000000e+01]\n",
            "   [7.95000000e+01 4.30000000e+01 4.72500000e+01]\n",
            "   [1.11750000e+02 1.14750000e+02 1.34000000e+02]\n",
            "   ...\n",
            "   [1.30000000e+02 1.38500000e+02 1.44750000e+02]\n",
            "   [1.19000000e+02 1.21500000e+02 1.29750000e+02]\n",
            "   [1.65500000e+02 1.67000000e+02 1.70750000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.78250000e+02 1.72750000e+02 1.71250000e+02]\n",
            "   [2.00250000e+02 1.98750000e+02 1.89250000e+02]\n",
            "   [1.92750000e+02 1.85500000e+02 1.79750000e+02]\n",
            "   ...\n",
            "   [1.26250000e+02 1.03500000e+02 1.03000000e+02]\n",
            "   [1.88500000e+02 2.02250000e+02 2.02750000e+02]\n",
            "   [1.32000000e+02 1.31500000e+02 1.27000000e+02]]\n",
            "\n",
            "  [[1.65750000e+02 1.56000000e+02 1.54250000e+02]\n",
            "   [1.46500000e+02 1.47250000e+02 1.44750000e+02]\n",
            "   [1.85000000e+02 1.85500000e+02 1.80250000e+02]\n",
            "   ...\n",
            "   [1.95000000e+02 2.04000000e+02 2.13000000e+02]\n",
            "   [1.89500000e+02 2.00000000e+02 2.03000000e+02]\n",
            "   [1.54750000e+02 1.53250000e+02 1.58750000e+02]]\n",
            "\n",
            "  [[1.94750000e+02 2.04750000e+02 2.14750000e+02]\n",
            "   [1.92250000e+02 1.95750000e+02 1.99750000e+02]\n",
            "   [1.60750000e+02 1.62750000e+02 1.61000000e+02]\n",
            "   ...\n",
            "   [1.93750000e+02 2.04750000e+02 2.09750000e+02]\n",
            "   [1.89500000e+02 2.00500000e+02 2.05500000e+02]\n",
            "   [1.88250000e+02 2.01750000e+02 2.07500000e+02]]]\n",
            "\n",
            "\n",
            " [[[3.21718750e+01 1.62343750e+01 8.76562500e+00]\n",
            "   [3.49375000e+01 1.54687500e+01 8.46875000e+00]\n",
            "   [3.15781250e+01 1.61718750e+01 7.64062500e+00]\n",
            "   ...\n",
            "   [2.27031250e+01 1.17031250e+01 5.70312500e+00]\n",
            "   [3.96875000e+00 6.50000000e+00 7.34375000e-01]\n",
            "   [2.43750000e+00 5.23437500e+00 2.34375000e-01]]\n",
            "\n",
            "  [[3.56093750e+01 1.66093750e+01 9.60937500e+00]\n",
            "   [3.30000000e+01 1.60000000e+01 9.00000000e+00]\n",
            "   [3.55000000e+01 1.65000000e+01 9.50000000e+00]\n",
            "   ...\n",
            "   [3.18125000e+01 1.68125000e+01 9.81250000e+00]\n",
            "   [1.95156250e+01 1.25156250e+01 5.51562500e+00]\n",
            "   [8.39062500e+00 8.20312500e+00 3.79687500e+00]]\n",
            "\n",
            "  [[3.93281250e+01 2.03281250e+01 1.33281250e+01]\n",
            "   [3.38437500e+01 1.68437500e+01 8.15625000e+00]\n",
            "   [3.70000000e+01 1.80000000e+01 1.10000000e+01]\n",
            "   ...\n",
            "   [3.40156250e+01 1.90156250e+01 1.20156250e+01]\n",
            "   [3.73437500e+01 2.03437500e+01 1.33437500e+01]\n",
            "   [1.51718750e+01 7.67187500e+00 3.17187500e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.21406250e+01 1.96250000e+01 8.65625000e+00]\n",
            "   [1.12812500e+01 6.96875000e+00 1.96875000e+00]\n",
            "   [3.18750000e+00 4.51562500e+00 5.15625000e-01]\n",
            "   ...\n",
            "   [1.48281250e+01 8.85937500e+00 1.68750000e+00]\n",
            "   [1.26093750e+01 7.79687500e+00 6.56250000e-01]\n",
            "   [1.84375000e+01 1.07500000e+01 3.60937500e+00]]\n",
            "\n",
            "  [[3.23437500e+01 2.03437500e+01 6.34375000e+00]\n",
            "   [3.07187500e+01 1.73125000e+01 6.01562500e+00]\n",
            "   [2.16718750e+01 1.34843750e+01 5.09375000e+00]\n",
            "   ...\n",
            "   [1.96875000e+01 1.56875000e+01 6.68750000e+00]\n",
            "   [1.57968750e+01 8.79687500e+00 1.00000000e+00]\n",
            "   [1.63906250e+01 1.26875000e+01 3.68750000e+00]]\n",
            "\n",
            "  [[3.46875000e+01 2.06875000e+01 9.68750000e+00]\n",
            "   [3.07031250e+01 1.77031250e+01 6.20312500e+00]\n",
            "   [2.18906250e+01 9.39062500e+00 5.31250000e-01]\n",
            "   ...\n",
            "   [1.32031250e+01 9.20312500e+00 4.68750000e-01]\n",
            "   [1.62500000e+01 1.22500000e+01 3.51562500e+00]\n",
            "   [1.45000000e+01 7.50000000e+00 0.00000000e+00]]]\n",
            "\n",
            "\n",
            " [[[3.25000000e+00 3.25000000e+00 1.25000000e+00]\n",
            "   [7.00000000e+00 7.00000000e+00 5.00000000e+00]\n",
            "   [7.50000000e+00 7.50000000e+00 5.50000000e+00]\n",
            "   ...\n",
            "   [1.50750000e+02 1.33500000e+02 1.29250000e+02]\n",
            "   [1.69000000e+02 1.68000000e+02 1.79000000e+02]\n",
            "   [1.38250000e+02 1.26250000e+02 1.36000000e+02]]\n",
            "\n",
            "  [[9.00000000e+00 9.00000000e+00 7.00000000e+00]\n",
            "   [1.52500000e+01 1.12500000e+01 1.02500000e+01]\n",
            "   [7.50000000e-01 7.50000000e-01 7.50000000e-01]\n",
            "   ...\n",
            "   [1.70250000e+02 1.67250000e+02 1.86250000e+02]\n",
            "   [1.69750000e+02 1.67500000e+02 1.80500000e+02]\n",
            "   [1.39000000e+02 1.10250000e+02 9.72500000e+01]]\n",
            "\n",
            "  [[6.50000000e+00 7.50000000e+00 2.50000000e+00]\n",
            "   [7.25000000e+00 7.25000000e+00 5.25000000e+00]\n",
            "   [5.00000000e-01 5.00000000e-01 5.00000000e-01]\n",
            "   ...\n",
            "   [1.83000000e+02 1.81250000e+02 1.97000000e+02]\n",
            "   [1.72500000e+02 1.69500000e+02 1.88000000e+02]\n",
            "   [1.71500000e+02 1.70000000e+02 1.89500000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.61000000e+02 1.75500000e+02 1.12000000e+02]\n",
            "   [1.19000000e+02 1.29500000e+02 7.50000000e+01]\n",
            "   [1.30500000e+02 1.41250000e+02 8.47500000e+01]\n",
            "   ...\n",
            "   [9.00000000e+00 9.75000000e+00 5.50000000e+00]\n",
            "   [2.00000000e+01 2.00000000e+01 1.80000000e+01]\n",
            "   [9.00000000e+00 9.00000000e+00 8.00000000e+00]]\n",
            "\n",
            "  [[1.57500000e+02 1.66500000e+02 1.16750000e+02]\n",
            "   [1.15750000e+02 1.29000000e+02 6.97500000e+01]\n",
            "   [1.01000000e+02 1.11500000e+02 5.15000000e+01]\n",
            "   ...\n",
            "   [1.15000000e+01 1.15000000e+01 9.50000000e+00]\n",
            "   [9.75000000e+00 9.75000000e+00 8.75000000e+00]\n",
            "   [1.32500000e+01 1.32500000e+01 1.12500000e+01]]\n",
            "\n",
            "  [[4.90000000e+01 5.45000000e+01 5.60000000e+01]\n",
            "   [4.47500000e+01 5.00000000e+01 3.77500000e+01]\n",
            "   [2.05000000e+01 2.05000000e+01 2.05000000e+01]\n",
            "   ...\n",
            "   [2.02500000e+01 1.97500000e+01 1.55000000e+01]\n",
            "   [6.00000000e+00 7.00000000e+00 1.25000000e+00]\n",
            "   [7.50000000e+00 7.50000000e+00 6.50000000e+00]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 1.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 1.00000000e+00 2.00000000e+00]\n",
            "   ...\n",
            "   [1.00000000e+00 1.00000000e+00 3.00000000e+00]\n",
            "   [0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 1.00000000e+00 3.00000000e+00]]\n",
            "\n",
            "  [[0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   ...\n",
            "   [1.00000000e+00 1.00000000e+00 3.00000000e+00]\n",
            "   [6.40625000e-01 6.40625000e-01 2.64062500e+00]\n",
            "   [0.00000000e+00 1.00000000e+00 3.00000000e+00]]\n",
            "\n",
            "  [[0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   [0.00000000e+00 0.00000000e+00 2.00000000e+00]\n",
            "   ...\n",
            "   [0.00000000e+00 1.00000000e+00 3.00000000e+00]\n",
            "   [5.00000000e-01 1.50000000e+00 3.50000000e+00]\n",
            "   [0.00000000e+00 2.00000000e+00 3.00000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.88718750e+02 1.82656250e+02 1.76656250e+02]\n",
            "   [1.16406250e+01 1.06406250e+01 6.14062500e+00]\n",
            "   [6.00000000e+00 6.00000000e+00 6.00000000e+00]\n",
            "   ...\n",
            "   [1.15000000e+01 1.15000000e+01 1.35000000e+01]\n",
            "   [6.50000000e+00 7.50000000e+00 9.50000000e+00]\n",
            "   [4.00000000e+00 8.00000000e+00 8.00000000e+00]]\n",
            "\n",
            "  [[6.06093750e+01 5.63281250e+01 5.19687500e+01]\n",
            "   [5.56250000e+00 7.28125000e+00 2.28125000e+00]\n",
            "   [5.35937500e+00 7.35937500e+00 4.35937500e+00]\n",
            "   ...\n",
            "   [1.13593750e+01 1.13593750e+01 1.33593750e+01]\n",
            "   [7.50000000e+00 7.50000000e+00 9.50000000e+00]\n",
            "   [3.50000000e+00 7.50000000e+00 8.50000000e+00]]\n",
            "\n",
            "  [[6.21875000e+00 9.71875000e+00 3.71875000e+00]\n",
            "   [4.68750000e-02 6.95312500e+00 0.00000000e+00]\n",
            "   [1.50000000e+00 6.50000000e+00 5.00000000e-01]\n",
            "   ...\n",
            "   [1.10000000e+01 1.10000000e+01 1.30000000e+01]\n",
            "   [8.00000000e+00 9.00000000e+00 1.10000000e+01]\n",
            "   [2.50000000e+00 8.50000000e+00 8.50000000e+00]]]\n",
            "\n",
            "\n",
            " [[[1.09492188e+02 7.04921875e+01 3.75234375e+01]\n",
            "   [5.05468750e+01 3.15156250e+01 8.00781250e+00]\n",
            "   [8.09765625e+01 5.59843750e+01 3.60234375e+01]\n",
            "   ...\n",
            "   [5.69921875e+01 1.89921875e+01 5.99218750e+00]\n",
            "   [5.14921875e+01 1.34921875e+01 4.49218750e+00]\n",
            "   [4.70156250e+01 8.01562500e+00 2.02343750e+00]]\n",
            "\n",
            "  [[1.08210938e+02 6.90703125e+01 3.00234375e+01]\n",
            "   [7.31406250e+01 4.56406250e+01 2.01406250e+01]\n",
            "   [4.84765625e+01 2.74765625e+01 2.25234375e+01]\n",
            "   ...\n",
            "   [6.02656250e+01 2.02656250e+01 8.26562500e+00]\n",
            "   [5.34296875e+01 1.54296875e+01 4.38281250e+00]\n",
            "   [5.23828125e+01 1.73828125e+01 5.38281250e+00]]\n",
            "\n",
            "  [[1.17914062e+02 7.99531250e+01 4.40703125e+01]\n",
            "   [6.91093750e+01 4.07265625e+01 2.11875000e+01]\n",
            "   [7.46406250e+01 5.76406250e+01 3.58671875e+01]\n",
            "   ...\n",
            "   [6.33515625e+01 1.98906250e+01 5.23437500e+00]\n",
            "   [5.80781250e+01 1.95781250e+01 1.15781250e+01]\n",
            "   [4.91562500e+01 1.36562500e+01 6.15625000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[5.44296875e+01 4.05859375e+01 2.95078125e+01]\n",
            "   [9.57421875e+01 7.91250000e+01 6.76640625e+01]\n",
            "   [1.71242188e+02 1.61859375e+02 1.48476562e+02]\n",
            "   ...\n",
            "   [2.18156250e+02 2.05390625e+02 1.97312500e+02]\n",
            "   [1.77820312e+02 1.48593750e+02 1.45093750e+02]\n",
            "   [8.10546875e+01 3.62890625e+01 4.10546875e+01]]\n",
            "\n",
            "  [[4.32109375e+01 2.52109375e+01 2.12109375e+01]\n",
            "   [5.46875000e+01 4.27578125e+01 3.06875000e+01]\n",
            "   [7.88984375e+01 6.58984375e+01 4.93750000e+01]\n",
            "   ...\n",
            "   [1.60492188e+02 1.30562500e+02 1.25132812e+02]\n",
            "   [8.20859375e+01 4.65859375e+01 3.75859375e+01]\n",
            "   [4.24296875e+01 7.92968750e+00 1.04296875e+01]]\n",
            "\n",
            "  [[4.09765625e+01 1.59765625e+01 1.19765625e+01]\n",
            "   [4.19687500e+01 2.34765625e+01 1.94375000e+01]\n",
            "   [4.79296875e+01 3.49765625e+01 2.64609375e+01]\n",
            "   ...\n",
            "   [5.31875000e+01 2.01875000e+01 1.31718750e+01]\n",
            "   [3.75390625e+01 7.03125000e+00 5.01562500e+00]\n",
            "   [3.84921875e+01 4.98437500e+00 1.50781250e+00]]]\n",
            "\n",
            "\n",
            " [[[1.18000000e+02 1.35500000e+02 1.19000000e+02]\n",
            "   [1.56000000e+02 1.96000000e+02 1.89250000e+02]\n",
            "   [7.40000000e+01 7.70000000e+01 5.67500000e+01]\n",
            "   ...\n",
            "   [2.29000000e+02 1.82250000e+02 1.05750000e+02]\n",
            "   [1.75250000e+02 1.31250000e+02 6.32500000e+01]\n",
            "   [1.61250000e+02 1.24250000e+02 5.62500000e+01]]\n",
            "\n",
            "  [[1.06750000e+02 1.09250000e+02 7.57500000e+01]\n",
            "   [1.46500000e+02 1.86250000e+02 1.79000000e+02]\n",
            "   [8.15000000e+01 9.60000000e+01 8.60000000e+01]\n",
            "   ...\n",
            "   [2.25750000e+02 1.66750000e+02 8.67500000e+01]\n",
            "   [2.27250000e+02 1.69250000e+02 9.12500000e+01]\n",
            "   [2.43000000e+02 1.95500000e+02 1.14500000e+02]]\n",
            "\n",
            "  [[1.03500000e+02 1.00000000e+02 6.45000000e+01]\n",
            "   [1.58000000e+02 1.95500000e+02 1.94000000e+02]\n",
            "   [7.67500000e+01 9.62500000e+01 8.45000000e+01]\n",
            "   ...\n",
            "   [2.31250000e+02 1.76750000e+02 1.00250000e+02]\n",
            "   [2.28500000e+02 1.65000000e+02 8.55000000e+01]\n",
            "   [2.30750000e+02 1.66750000e+02 7.97500000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.44250000e+02 9.72500000e+01 4.80000000e+01]\n",
            "   [1.85000000e+01 1.22500000e+01 1.60000000e+01]\n",
            "   [8.15000000e+01 7.05000000e+01 4.50000000e+01]\n",
            "   ...\n",
            "   [4.55000000e+01 3.17500000e+01 7.25000000e+00]\n",
            "   [4.75000000e+01 2.70000000e+01 4.00000000e+00]\n",
            "   [2.87500000e+01 2.77500000e+01 2.60000000e+01]]\n",
            "\n",
            "  [[1.44750000e+02 9.27500000e+01 4.92500000e+01]\n",
            "   [1.35000000e+02 8.85000000e+01 3.32500000e+01]\n",
            "   [1.80000000e+01 1.35000000e+01 1.00000000e+01]\n",
            "   ...\n",
            "   [7.07500000e+01 4.82500000e+01 3.72500000e+01]\n",
            "   [7.95000000e+01 8.15000000e+01 5.92500000e+01]\n",
            "   [1.12000000e+02 9.12500000e+01 5.22500000e+01]]\n",
            "\n",
            "  [[1.40250000e+02 8.97500000e+01 4.17500000e+01]\n",
            "   [1.39250000e+02 8.80000000e+01 4.52500000e+01]\n",
            "   [1.47750000e+02 9.57500000e+01 5.12500000e+01]\n",
            "   ...\n",
            "   [4.45000000e+01 3.12500000e+01 2.05000000e+01]\n",
            "   [2.24750000e+02 1.75250000e+02 1.16000000e+02]\n",
            "   [2.46250000e+02 2.21750000e+02 1.45250000e+02]]]]\n",
            "Output batch:\n",
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UaPCEthRJ-"
      },
      "source": [
        "We can see that indeed the generator produces a tensor of the appropriate dimensions with the inputs for the neural network, and that the outputs have also been properly codified in one-hot form. However, there is still an issue with the data: the pixel values are in the range [0, 255], which might produce training problems. We will solve this later in the network definition. For now, let's move on and prepare a funcion that builds the training, validation and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQyBbf-hRJ-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Create a function <b>create_datasets</b> that receives the following parameters:\n",
        "    <ul>\n",
        "      <li><b>traindir</b>: the directory where training images are located.</li>\n",
        "      <li><b>valdir</b>: the directory where validation images are located.</li>\n",
        "      <li><b>testdir</b>: the directory where test images are located.</li>\n",
        "      <li><b>image_size</b>: the size that will be used to resize all the images to a common resolution.</li>\n",
        "      <li><b>batch_size</b>: the size of the batches of images to be generated.</li>\n",
        "    </ul>\n",
        "    The function must create datasets for the training, validation and test directories, and return the three datasets created.\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-wm19eJhRJ-"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "def create_datasets(traindir=TRAINDIR, valdir=VALDIR, testdir=TESTDIR, image_size=32, batch_size=64):\n",
        "  label_mode = 'categorical'\n",
        "  \n",
        "  train_dataset = image_dataset_from_directory(\n",
        "    traindir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  val_dataset = image_dataset_from_directory(\n",
        "    valdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  test_dataset = image_dataset_from_directory(\n",
        "    testdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = label_mode\n",
        "    )\n",
        "  \n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wGaULQsjwJz"
      },
      "source": [
        "Let's test if the function you just implemented works correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZlNbHhiOIR",
        "outputId": "834742a5-973f-4ee6-e30c-b1eae0a6aad2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=512)\n",
        "train_dataset_64, val_dataset_64, test_dataset_64 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=64, batch_size=256)\n",
        "train_dataset_128, val_dataset_128, test_dataset_128 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=128)\n",
        "train_dataset_256, val_dataset_256, test_dataset_256 = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=256, batch_size=64)\n",
        "\n",
        "\n",
        "# Test whether all returned objects are valid Tensorflow datasets\n",
        "assert isinstance(train_dataset, tf.data.Dataset)\n",
        "assert isinstance(val_dataset, tf.data.Dataset)\n",
        "assert isinstance(test_dataset, tf.data.Dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIo6LBfhRJ-"
      },
      "source": [
        "Now that we have our datasets we can train a deep learning model using them! For illustration purposes, let's build an extremely simple convolutional network. Note how we have added a special pre-processing layer `Rescaling` that takes care of normalizing the data to the range [0, 1].\n",
        "\n",
        "Be careful! This network will work, but has some flaws in its design you might want to fix in the network you will desing later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cumg6qzsZHCl"
      },
      "source": [
        "from numpy.random import seed\n",
        "import random\n",
        "\n",
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbwmYJiLhRJ_"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model.add(Convolution2D(4, 3, activation='linear'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9OxA-AhRJ_"
      },
      "source": [
        "The `fit` method of a Keras model can receive a `Dataset` with training data, instead of a pair of tensors with (inputs, outputs). Since when building the `Dataset` we already specified the batch size, we don't need to do it now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZXjs6CghRKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0fff44-f041-40d1-a35e-386ca9d6a8d2"
      },
      "source": [
        "model.fit(train_dataset, epochs=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 44s 127ms/step - loss: 1.7360 - accuracy: 0.2501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5efef98150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ux64pchRKA"
      },
      "source": [
        "Similarly, we can evaluate the performance of our model over our test `Dataset` as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_kQhkY-hRKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb22fed-6d20-4698-f3d6-d5736fb67af8"
      },
      "source": [
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 122ms/step - loss: 2.2224 - accuracy: 0.1986\n",
            "Loss 2.22, accuracy 19.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXf9OBehRKB"
      },
      "source": [
        "The accuracy might seem poor, but take into account we have used a very simple model and this problem has 6 classes. Will you be able to do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZnaNQIhRKB"
      },
      "source": [
        "## Building your network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNd8dC2whRKB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Design a neural network that maximizes the accuracy over the test data. You can use the training and validation datasets for anything you like, but you can <b>only</b> use the test data to evaluate the model performance. You should obtain a network able to attain at least 40% accuracy over the test set.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C9GNYihRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Some tips and strategies that can help you optimize your network design:\n",
        "\n",
        "    \n",
        "- Make use of all the tricks you learned from previous notebooks: convolutional + pooling layers, ReLU activations, dropout... also make sure to use a good optimizer with an adequate loss function, as well as the correct activation for the output layer.\n",
        "- Configuring the datasets to load the images with a larger size can significantly improve your performance. But be careful, you can also run out of memory (CUDA memory error) if they become too large! Note that for this problem a size larger than 256 might be too large.\n",
        "- Start with networks with a small number of parameters, so you are able to check fast how well they work. Then you can make your network larger in three directions: larger input images, more layers and more kernels per convolutional layer or units per dense layer. If you use larger images make sure to add more Convolution+Pooling layers, so that only very small images (less than 10x10 pixels) arrive at the Flatten layer.\n",
        "- If you see large differences in loss between your training data and your validation or test data, try increasing the Dropout probabilities, especially for the Dense layers.\n",
        "- Make use of that validation data! For instance, use an <a href=\"https://keras.io/api/callbacks/early_stopping/\">**EarlyStopping strategy**</a> to monitor the loss of these validation data, and stop when training when after a number of iterations such loss has not decreased. Configuring the EarlyStopping to restore the best weights found in the optimization is also useful.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDs0SGghRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "Other advanced strategies you can try are:\n",
        "\n",
        "- Use **image augmentation techniques** to artifically create new training images. To do so, explore the rest of layers available in the <a href=\"https://keras.io/api/layers/preprocessing_layers/image_preprocessing/\">Keras Image Preprocessing module</a>.\n",
        "- Use <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> layers to improve the optimization procedure.\n",
        "    \n",
        "It you use all the tricks, it is possible to obtain more than 60% accuracy in the test set.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FL-KdMKGsuS"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomCrop,RandomTranslation, RandomRotation, RandomZoom\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvLgIBwvPPo"
      },
      "source": [
        "### Model 1: Changing optimizer\n",
        "\n",
        "The first change in the original model is to replace the *sgd* optimizer by the *Adam*. As we can see, the model performance on test has improved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kzlLHkFSIxd"
      },
      "source": [
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgz7w3VqhRKC"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "pool_size = 2\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_1.add(Convolution2D(4, 3, activation='linear'))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVO-hqeLyNBw",
        "outputId": "231dc42a-07fb-4459-b6db-8f88335d42d3"
      },
      "source": [
        "model_1.fit(train_dataset, epochs=1, validation_data = val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 19s 182ms/step - loss: 1.8442 - accuracy: 0.2558 - val_loss: 1.6544 - val_accuracy: 0.2941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90b7621050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLFP2VEFySIb",
        "outputId": "22503fac-188c-4467-bf02-f76056ee18ef"
      },
      "source": [
        "loss, acc = model_1.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 114ms/step - loss: 1.6408 - accuracy: 0.3159\n",
            "Loss 1.64, accuracy 31.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmpC3tQgqQys"
      },
      "source": [
        "### Model 2: adding a max pooling layer\n",
        "\n",
        "A MaxPooling layer has been introduced after the convolution. The number of epochs during training has been increased.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5NcS9d1qT_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea436698-153d-4391-9c85-cbaf34c9720a"
      },
      "source": [
        "pool_size = 2\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_2.add(Convolution2D(4, 3, activation='linear', strides=2))\n",
        "model_2.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_4 (Rescaling)      (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 15, 4)         112       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 7, 7, 4)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 196)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 1182      \n",
            "=================================================================\n",
            "Total params: 1,294\n",
            "Trainable params: 1,294\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s03O1V5yrrlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7514a1a3-e5ab-4caf-b4cb-a2ea7f81144b"
      },
      "source": [
        "model_2.fit(train_dataset, epochs=10, validation_data=val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 20s 190ms/step - loss: 1.9338 - accuracy: 0.2055 - val_loss: 1.6844 - val_accuracy: 0.2737\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.6721 - accuracy: 0.2879 - val_loss: 1.6418 - val_accuracy: 0.3041\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.6236 - accuracy: 0.3209 - val_loss: 1.5910 - val_accuracy: 0.3325\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 19s 184ms/step - loss: 1.5809 - accuracy: 0.3398 - val_loss: 1.5648 - val_accuracy: 0.3558\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.5553 - accuracy: 0.3557 - val_loss: 1.5271 - val_accuracy: 0.3676\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 19s 183ms/step - loss: 1.5156 - accuracy: 0.3777 - val_loss: 1.5026 - val_accuracy: 0.3795\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 19s 183ms/step - loss: 1.4949 - accuracy: 0.3796 - val_loss: 1.4910 - val_accuracy: 0.3752\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 19s 183ms/step - loss: 1.4974 - accuracy: 0.3849 - val_loss: 1.4901 - val_accuracy: 0.3833\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 19s 183ms/step - loss: 1.4754 - accuracy: 0.3977 - val_loss: 1.4728 - val_accuracy: 0.3866\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 19s 184ms/step - loss: 1.4687 - accuracy: 0.3943 - val_loss: 1.4731 - val_accuracy: 0.3871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90b73c2750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEN_VuEktyfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcb3e44-7042-4429-896b-5d536cbf88e4"
      },
      "source": [
        "loss, acc = model_2.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 115ms/step - loss: 1.4921 - accuracy: 0.3952\n",
            "Loss 1.49, accuracy 39.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSVMTscA0BwV"
      },
      "source": [
        "### Model 3: Changing activation functions\n",
        "\n",
        "Th activation function in the last dense layer has been change from *sigmoid* to *softmax*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Unyq630Kfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38e24f4-45bb-4fdd-8b69-8f721a8a2bb8"
      },
      "source": [
        "pool_size = 2\n",
        "\n",
        "model_3 = Sequential()\n",
        "model_3.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_3.add(Convolution2D(8, 3, activation='relu', strides=1))\n",
        "model_3.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_3.add(Convolution2D(16, 3, activation='relu', strides=1))\n",
        "model_3.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_3.add(Flatten())\n",
        "model_3.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_7 (Rescaling)      (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 30, 30, 8)         224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 6)                 3462      \n",
            "=================================================================\n",
            "Total params: 4,854\n",
            "Trainable params: 4,854\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aWow7-s0NNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb1c14b-0cb9-4a22-8f99-90fda88d6b2b"
      },
      "source": [
        "model_3.fit(train_dataset, epochs=10, validation_data=val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 16s 156ms/step - loss: 1.6873 - accuracy: 0.2868 - val_loss: 1.5024 - val_accuracy: 0.3686\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 16s 160ms/step - loss: 1.5047 - accuracy: 0.3786 - val_loss: 1.4747 - val_accuracy: 0.3819\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 16s 158ms/step - loss: 1.4824 - accuracy: 0.3828 - val_loss: 1.4611 - val_accuracy: 0.3971\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 16s 159ms/step - loss: 1.4557 - accuracy: 0.4116 - val_loss: 1.4627 - val_accuracy: 0.4018\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 16s 158ms/step - loss: 1.4542 - accuracy: 0.4170 - val_loss: 1.4450 - val_accuracy: 0.4046\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 16s 158ms/step - loss: 1.4190 - accuracy: 0.4227 - val_loss: 1.4141 - val_accuracy: 0.4122\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 16s 159ms/step - loss: 1.4027 - accuracy: 0.4416 - val_loss: 1.4104 - val_accuracy: 0.4132\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 16s 159ms/step - loss: 1.3994 - accuracy: 0.4355 - val_loss: 1.4505 - val_accuracy: 0.3852\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 16s 159ms/step - loss: 1.3829 - accuracy: 0.4444 - val_loss: 1.3998 - val_accuracy: 0.4194\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 16s 159ms/step - loss: 1.3841 - accuracy: 0.4452 - val_loss: 1.3937 - val_accuracy: 0.4146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90b70c8890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZqvxtLf06O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e717aae-c81c-48ad-b5ec-94dbbdc40e25"
      },
      "source": [
        "loss, acc = model_3.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 109ms/step - loss: 1.3956 - accuracy: 0.4338\n",
            "Loss 1.4, accuracy 43.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdtuqZOHtgS"
      },
      "source": [
        "### Model 4 \n",
        "We have added another convolution but with relu activation, a rectified linear activation and dropout with probability 0.5\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXsLxGEeIScc"
      },
      "source": [
        "Now, we try to improve the network adding another convolution with 32 channels to generate and an relu activation, a dense layer with 50 units and rectified linear activation and a dropout with 50% of probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxr4uPIIOzC"
      },
      "source": [
        "kernel_size = 3\n",
        "model_4 = Sequential()\n",
        "model_4.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_4.add(Convolution2D(8, (kernel_size, kernel_size), input_shape=(image_size, image_size, 3), activation='relu'))\n",
        "model_4.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "model_4.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_4.add(Flatten())\n",
        "model_4.add(Dense(50, activation=\"relu\"))\n",
        "model_4.add(Dropout(0.5))\n",
        "model_4.add(Dense(6, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoNzcrB0JKcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8c1999-8f4f-42d3-a63f-8dfc704f3acd"
      },
      "source": [
        "model_4.fit(train_dataset, epochs=10, validation_data=val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 14s 136ms/step - loss: 1.8523 - accuracy: 0.2556\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 14s 134ms/step - loss: 1.5526 - accuracy: 0.3514\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.5708 - accuracy: 0.3502\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 1.5821 - accuracy: 0.3539\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 13s 125ms/step - loss: 1.5140 - accuracy: 0.3748\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4610 - accuracy: 0.4019\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4637 - accuracy: 0.4084\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4913 - accuracy: 0.3979\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 1.4761 - accuracy: 0.3991\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.4976 - accuracy: 0.3978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fef101b2dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvw5uKOB8XN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cb18d5-8a4c-428d-a7e2-f75e4efbcf6f"
      },
      "source": [
        "loss, acc = model_4.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 89ms/step - loss: 1.5514 - accuracy: 0.3478\n",
            "Loss 1.55, accuracy 34.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO95UHFHAC2Q"
      },
      "source": [
        "As we can see, although the accuracy in train has decreased slightly, we have achieved a better one in test. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6s5XhID-Wk"
      },
      "source": [
        "### Model 5: increasing image size 64x64 images\n",
        "We decided to try with larger input images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbzAqV1IEKe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96d333e-afe0-4727-c2ea-366099a2aba7"
      },
      "source": [
        "image_size = 64\n",
        "model_5 = Sequential()\n",
        "model_5.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_5.add(Convolution2D(8, 3, input_shape=(image_size, image_size, 3), activation='linear'))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_5.add(Convolution2D(32, 3, activation=\"relu\"))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_5.add(Convolution2D(64, 3, activation=\"relu\"))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=2))\n",
        "model_5.add(Convolution2D(128, 3, activation=\"relu\"))\n",
        "model_5.add(MaxPooling2D(pool_size=pool_size, strides=1))\n",
        "model_5.add(Dropout(0.5))\n",
        "model_5.add(Flatten())\n",
        "model_5.add(Dense(200, activation=\"relu\"))\n",
        "model_5.add(Dropout(0.5))\n",
        "model_5.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_12 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 62, 62, 8)         224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 31, 31, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 29, 29, 32)        2336      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 200)               230600    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 326,718\n",
            "Trainable params: 326,718\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpDw4mI5Egle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bdb53a-c7c5-4383-a32e-dd1347751937"
      },
      "source": [
        "model_5.fit(train_dataset_64, epochs=10, validation_data=val_dataset_64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "24/24 [==============================] - 16s 496ms/step - loss: 1.7302 - accuracy: 0.2469 - val_loss: 1.6033 - val_accuracy: 0.2922\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 15s 488ms/step - loss: 1.5980 - accuracy: 0.3171 - val_loss: 1.4501 - val_accuracy: 0.3918\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 15s 494ms/step - loss: 1.4800 - accuracy: 0.3854 - val_loss: 1.4189 - val_accuracy: 0.4241\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 15s 493ms/step - loss: 1.4342 - accuracy: 0.4121 - val_loss: 1.3753 - val_accuracy: 0.4417\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 15s 497ms/step - loss: 1.4099 - accuracy: 0.4278 - val_loss: 1.3583 - val_accuracy: 0.4602\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 15s 500ms/step - loss: 1.3813 - accuracy: 0.4354 - val_loss: 1.3408 - val_accuracy: 0.4758\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 15s 502ms/step - loss: 1.3404 - accuracy: 0.4690 - val_loss: 1.3311 - val_accuracy: 0.4611\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 15s 497ms/step - loss: 1.3298 - accuracy: 0.4673 - val_loss: 1.3079 - val_accuracy: 0.4824\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 15s 503ms/step - loss: 1.2848 - accuracy: 0.4825 - val_loss: 1.3116 - val_accuracy: 0.4791\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 15s 501ms/step - loss: 1.2365 - accuracy: 0.5184 - val_loss: 1.2512 - val_accuracy: 0.5166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90b6335750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVe7QIVwEsQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8ab5b8-3081-471f-91db-4546d34c4a09"
      },
      "source": [
        "loss, acc = model_5.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 4s 12ms/step - loss: 1.2325 - accuracy: 0.5309\n",
            "Loss 1.23, accuracy 53.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXsfvkKZSZTV"
      },
      "source": [
        "### Model 6:\n",
        "\n",
        "This we are going to ...\n",
        "\n",
        "This model reaches a high performance over training set, but not on validation wich means is overfitted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pii1GrxBv7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bafdd7b-0799-4d50-c193-4aab9c8bdf8b"
      },
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_6.add(Convolution2D(8, 2, input_shape=(64, 64), activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=pool_size, strides=1, padding='valid'))\n",
        "model_6.add(Convolution2D(32, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n",
        "model_6.add(Convolution2D(64, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n",
        "model_6.add(Convolution2D(128, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n",
        "model_6.add(Convolution2D(256, 2, activation='relu'))\n",
        "model_6.add(MaxPooling2D(pool_size=2, strides=1, padding='valid'))\n",
        "model_6.add(Dropout(0.5))\n",
        "model_6.add(Flatten())\n",
        "model_6.add(Dense(1024, activation='relu'))\n",
        "model_6.add(Dropout(0.5))\n",
        "model_6.add(Dense(200, activation='relu'))\n",
        "model_6.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_6.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_13 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 63, 63, 8)         104       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 62, 62, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 61, 61, 32)        1056      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 29, 29, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 13, 13, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 5, 5, 256)         131328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 200)               205000    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 4,575,174\n",
            "Trainable params: 4,575,174\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed7L7AKmCQDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5798f2f0-7685-4e04-9115-1fc14adf0be0"
      },
      "source": [
        "model_6.fit(train_dataset_64, epochs=20, validation_data=val_dataset_64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 18s 560ms/step - loss: 1.7249 - accuracy: 0.2330 - val_loss: 1.5863 - val_accuracy: 0.3188\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 16s 521ms/step - loss: 1.5658 - accuracy: 0.3486 - val_loss: 1.4941 - val_accuracy: 0.3676\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 16s 516ms/step - loss: 1.5019 - accuracy: 0.3581 - val_loss: 1.5286 - val_accuracy: 0.3757\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 16s 517ms/step - loss: 1.4821 - accuracy: 0.3841 - val_loss: 1.4172 - val_accuracy: 0.4222\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 16s 519ms/step - loss: 1.4132 - accuracy: 0.4198 - val_loss: 1.3857 - val_accuracy: 0.4307\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 16s 519ms/step - loss: 1.3639 - accuracy: 0.4445 - val_loss: 1.3551 - val_accuracy: 0.4635\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 16s 520ms/step - loss: 1.3340 - accuracy: 0.4572 - val_loss: 1.3285 - val_accuracy: 0.4587\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 16s 518ms/step - loss: 1.2881 - accuracy: 0.4800 - val_loss: 1.2731 - val_accuracy: 0.4910\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 16s 520ms/step - loss: 1.2698 - accuracy: 0.4926 - val_loss: 1.2692 - val_accuracy: 0.4934\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 16s 518ms/step - loss: 1.2005 - accuracy: 0.5213 - val_loss: 1.2446 - val_accuracy: 0.5071\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 16s 516ms/step - loss: 1.2004 - accuracy: 0.5260 - val_loss: 1.2210 - val_accuracy: 0.5308\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 16s 521ms/step - loss: 1.1563 - accuracy: 0.5456 - val_loss: 1.2068 - val_accuracy: 0.5266\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 16s 525ms/step - loss: 1.1082 - accuracy: 0.5571 - val_loss: 1.2346 - val_accuracy: 0.5256\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 16s 533ms/step - loss: 1.0806 - accuracy: 0.5840 - val_loss: 1.3055 - val_accuracy: 0.5199\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 16s 518ms/step - loss: 1.0960 - accuracy: 0.5741 - val_loss: 1.2520 - val_accuracy: 0.5218\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 16s 531ms/step - loss: 1.0249 - accuracy: 0.6027 - val_loss: 1.1403 - val_accuracy: 0.5674\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 16s 519ms/step - loss: 0.9547 - accuracy: 0.6410 - val_loss: 1.1426 - val_accuracy: 0.5773\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 16s 520ms/step - loss: 0.9028 - accuracy: 0.6536 - val_loss: 1.1296 - val_accuracy: 0.5797\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 16s 506ms/step - loss: 0.8601 - accuracy: 0.6765 - val_loss: 1.1434 - val_accuracy: 0.5740\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 17s 537ms/step - loss: 0.8499 - accuracy: 0.6919 - val_loss: 1.2158 - val_accuracy: 0.5323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90b60f3dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYsAhviSCU91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99722d42-af66-47ce-9429-941efb0ee21c"
      },
      "source": [
        "loss, acc = model_6.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 5s 29ms/step - loss: 1.2359 - accuracy: 0.5357\n",
            "Loss 1.24, accuracy 53.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUPiuvLDDLt6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc10VqihZnTD"
      },
      "source": [
        "### Model 7 (este es el que pensamos con 64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQgaqg7II4Km"
      },
      "source": [
        "image_size = 64\n",
        "model_7 = Sequential()\n",
        "model_7.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_7.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=2, activation= 'relu'))\n",
        "model_7.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_7.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_7.add(Convolution2D(32, 3,strides=1, activation= 'relu'))\n",
        "model_7.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_7.add(Flatten())\n",
        "model_7.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_7.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_7.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbPnPwkKjsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daefac8e-bfb8-462f-cc6b-20dd5e156473"
      },
      "source": [
        "model_7.fit(train_dataset_64, epochs=20, validation_data=val_dataset_64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.7289 - accuracy: 0.2546\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.5786 - accuracy: 0.3330\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.5501 - accuracy: 0.3519\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.4830 - accuracy: 0.3987\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.4454 - accuracy: 0.4164\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.4167 - accuracy: 0.4370\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 11s 104ms/step - loss: 1.4064 - accuracy: 0.4383\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3930 - accuracy: 0.4433\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3772 - accuracy: 0.4496\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3482 - accuracy: 0.4615\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 11s 106ms/step - loss: 1.3813 - accuracy: 0.4513\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3531 - accuracy: 0.4577\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 11s 105ms/step - loss: 1.3095 - accuracy: 0.4850\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 11s 104ms/step - loss: 1.3265 - accuracy: 0.4812\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3409 - accuracy: 0.4733\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 11s 107ms/step - loss: 1.3099 - accuracy: 0.4651\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 12s 114ms/step - loss: 1.2809 - accuracy: 0.4948\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 11s 110ms/step - loss: 1.2755 - accuracy: 0.5014\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 12s 114ms/step - loss: 1.2690 - accuracy: 0.5075\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 12s 112ms/step - loss: 1.2503 - accuracy: 0.5043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c1a6df0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqgEkVjGLFio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b78ae-0d38-4b30-ccf3-f9bc298295d4"
      },
      "source": [
        "loss, acc = model_8.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 95ms/step - loss: 1.3564 - accuracy: 0.4700\n",
            "Loss 1.36, accuracy 47.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMp8ZdtPLgvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7168390e-9f21-45a7-92a2-05ae134f4116"
      },
      "source": [
        "model_8.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_11 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 31, 31, 4)         112       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 29, 29, 8)         296       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 6)                 3462      \n",
            "=================================================================\n",
            "Total params: 5,038\n",
            "Trainable params: 5,038\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpjC7Zz2i1AU"
      },
      "source": [
        "### Model 8: 50.5 accuracy with 50 epochs + overfitted\n",
        "\n",
        "Another dense layer close to the output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4lM7k12tbpm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YIb8WuYi4rX"
      },
      "source": [
        "image_size = 64\n",
        "model_8 = Sequential()\n",
        "model_8.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_8.add(Convolution2D(4, 3, input_shape=(image_size, image_size, 3), strides=2, activation= 'relu'))\n",
        "model_8.add(Convolution2D(8, 3,strides=1, activation= 'relu'))\n",
        "model_8.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_8.add(Convolution2D(16, 3,strides=1, activation= 'relu'))\n",
        "model_8.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_8.add(Flatten())\n",
        "model_8.add(Dense(128, activation='relu'))\n",
        "model_8.add(Dropout(0.5))\n",
        "model_8.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_8.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYJXy2hMj0BW",
        "outputId": "4a560352-abae-4aef-f1d5-15f60e6dae53"
      },
      "source": [
        "model_8.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_1 (Rescaling)      (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 31, 31, 4)         112       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 29, 29, 8)         296       \n",
            "=================================================================\n",
            "Total params: 408\n",
            "Trainable params: 408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmrrqdggjAhy",
        "outputId": "cc6a7be0-2b33-49a1-8259-e37d626ce5ff"
      },
      "source": [
        "model_8.fit(train_dataset_64, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "96/96 [==============================] - 45s 129ms/step - loss: 1.7353 - accuracy: 0.2451\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 1.5906 - accuracy: 0.3284\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 13s 129ms/step - loss: 1.5534 - accuracy: 0.3384\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.5153 - accuracy: 0.3703\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.4707 - accuracy: 0.4075\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 1.4404 - accuracy: 0.4051\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.4326 - accuracy: 0.4054\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 1.4009 - accuracy: 0.4342\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 14s 130ms/step - loss: 1.3500 - accuracy: 0.4468\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.3645 - accuracy: 0.4547\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 14s 130ms/step - loss: 1.3064 - accuracy: 0.4813\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.3177 - accuracy: 0.4762\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.3081 - accuracy: 0.4822\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.2756 - accuracy: 0.4846\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 14s 130ms/step - loss: 1.2472 - accuracy: 0.5029\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 1.2465 - accuracy: 0.5064\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 1.2152 - accuracy: 0.5236\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.2464 - accuracy: 0.5083\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 1.2479 - accuracy: 0.4928\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 1.1603 - accuracy: 0.5458\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.1742 - accuracy: 0.5359\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.1172 - accuracy: 0.5666\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.1447 - accuracy: 0.5557\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.2201 - accuracy: 0.5236\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.0876 - accuracy: 0.5730\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 13s 129ms/step - loss: 1.0601 - accuracy: 0.5902\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 1.0512 - accuracy: 0.6002\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.0543 - accuracy: 0.5987\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 1.0124 - accuracy: 0.6096\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 1.0038 - accuracy: 0.6076\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 13s 128ms/step - loss: 0.9883 - accuracy: 0.6241\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 0.9469 - accuracy: 0.6331\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 0.9710 - accuracy: 0.6317\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.9207 - accuracy: 0.6376\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8981 - accuracy: 0.6586\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8729 - accuracy: 0.6651\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8696 - accuracy: 0.6690\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8254 - accuracy: 0.6823\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8135 - accuracy: 0.6964\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 13s 127ms/step - loss: 0.8039 - accuracy: 0.6967\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 0.7996 - accuracy: 0.6907\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.8015 - accuracy: 0.6948\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.7811 - accuracy: 0.6963\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.7630 - accuracy: 0.7136\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 0.7105 - accuracy: 0.7283\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.7016 - accuracy: 0.7262\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 13s 130ms/step - loss: 0.6926 - accuracy: 0.7274\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.7105 - accuracy: 0.7270\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 14s 132ms/step - loss: 0.6664 - accuracy: 0.7392\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 14s 131ms/step - loss: 0.7626 - accuracy: 0.7024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8a09fd0950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM89AiMhluS7",
        "outputId": "e96ca749-35cc-429d-dc13-ef0a4d0749b2"
      },
      "source": [
        "loss, acc = model_8.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 4s 89ms/step - loss: 1.6145 - accuracy: 0.4952\n",
            "Loss 1.61, accuracy 49.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RidbDkp5Z9qd"
      },
      "source": [
        "### Model 9: AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cNg64tCMZNB"
      },
      "source": [
        "image_size = 256\n",
        "model_9 = Sequential()\n",
        "model_9.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_9.add(Convolution2D(96, 7, input_shape=(image_size, image_size, 3), strides=1, activation= 'relu'))\n",
        "model_9.add(Convolution2D(128, 5, strides=2, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_9.add(Convolution2D(256, 5, strides=1, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "model_9.add(Convolution2D(256, 3, strides=1, activation= 'relu'))\n",
        "model_9.add(Convolution2D(512, 3, strides=1, activation= 'relu'))\n",
        "model_9.add(Convolution2D(512, 3, strides=1, activation= 'relu'))\n",
        "model_9.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model_9.add(Flatten())\n",
        "model_9.add(Dense(512, activation='relu'))\n",
        "model_9.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_9.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk3EwCrddFo6",
        "outputId": "81b03e43-7045-4fee-f4b4-68350d8acd16"
      },
      "source": [
        "model_9.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_22 (Rescaling)     (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 250, 250, 96)      14208     \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 123, 123, 128)     307328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 61, 61, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 57, 57, 256)       819456    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 26, 26, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 24, 24, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 73728)             0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 512)               37749248  \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 40,663,558\n",
            "Trainable params: 40,663,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqkgj_hycu0W"
      },
      "source": [
        "model_9.fit(train_dataset_256, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJzOLNTpezJL"
      },
      "source": [
        "loss, acc = model_9.evaluate(test_dataset_256)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU2gG22CtdFs"
      },
      "source": [
        "### Model 10: overfitted\n",
        "\n",
        "The size is 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEZoGayAvN-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f7968f-0fbf-4b4a-b9e4-01133db8aea1"
      },
      "source": [
        "image_size = 128\n",
        "model_10 = Sequential()\n",
        "model_10.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_10.add(Convolution2D(32, 3, strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_10.add(Convolution2D(64, 5,strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_10.add(Convolution2D(128, 3,strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_10.add(Convolution2D(256, 3,strides=1, activation= 'relu'))\n",
        "model_10.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_10.add(Flatten())\n",
        "model_10.add(Dense(4096, activation='relu'))\n",
        "model_10.add(Dropout(0.5))\n",
        "model_10.add(Dense(256, activation='relu'))\n",
        "model_10.add(Dropout(0.5))\n",
        "model_10.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_10.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_10.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_34 (Rescaling)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_136 (Conv2D)          (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_130 (MaxPoolin (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_137 (Conv2D)          (None, 59, 59, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_131 (MaxPoolin (None, 29, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_138 (Conv2D)          (None, 27, 27, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_132 (MaxPoolin (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_133 (MaxPoolin (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 4096)              26218496  \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 256)               1048832   \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 27,690,054\n",
            "Trainable params: 27,690,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC2HEiGBxDZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b238129-91c5-4948-8ef9-6b86832ed3fc"
      },
      "source": [
        "model_10.fit(train_dataset_128, epochs=20, validation_data=val_dataset_128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 19s 178ms/step - loss: 1.7070 - accuracy: 0.2477\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.5485 - accuracy: 0.3381\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 18s 183ms/step - loss: 1.4580 - accuracy: 0.3848\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 18s 181ms/step - loss: 1.4394 - accuracy: 0.3955\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 18s 181ms/step - loss: 1.3852 - accuracy: 0.4278\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 18s 182ms/step - loss: 1.3381 - accuracy: 0.4646\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 19s 184ms/step - loss: 1.2766 - accuracy: 0.5011\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 19s 189ms/step - loss: 1.2134 - accuracy: 0.5241\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 19s 188ms/step - loss: 1.2138 - accuracy: 0.5221\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.2140 - accuracy: 0.5355\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 19s 188ms/step - loss: 0.9891 - accuracy: 0.6212\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 0.9781 - accuracy: 0.6284\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 0.9660 - accuracy: 0.6375\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 0.7379 - accuracy: 0.7181\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 0.7838 - accuracy: 0.7142\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 0.5574 - accuracy: 0.7911\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 0.4931 - accuracy: 0.8189\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 0.3559 - accuracy: 0.8776\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 0.2909 - accuracy: 0.9028\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 0.2689 - accuracy: 0.9123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93e3a5fb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it794Ja9xPdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16abada-cc56-415d-e75f-3219452c0f68"
      },
      "source": [
        "loss, acc = model_10.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 141ms/step - loss: 2.5931 - accuracy: 0.5256\n",
            "Loss 2.59, accuracy 52.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H96f6btb9HW7"
      },
      "source": [
        "### Model 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbggQHjrdo2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516a6f0a-7ee7-4b14-8aa5-f516397053d7"
      },
      "source": [
        "image_size = 128\n",
        "model_11 = Sequential()\n",
        "model_11.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_11.add(Convolution2D(32, 3, strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_11.add(Convolution2D(64, 5,strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_11.add(Convolution2D(128, 3,strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_11.add(Convolution2D(256, 3,strides=1, activation= 'relu'))\n",
        "model_11.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_11.add(Flatten())\n",
        "model_11.add(Dense(4096, activation='relu'))\n",
        "model_11.add(Dropout(0.5))\n",
        "model_11.add(Dense(1024, activation='relu'))\n",
        "model_11.add(Dropout(0.5))\n",
        "model_11.add(Dense(256, activation='relu'))\n",
        "model_11.add(Dropout(0.5))\n",
        "model_11.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_11.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_11.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_36 (Rescaling)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_144 (Conv2D)          (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_138 (MaxPoolin (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_145 (Conv2D)          (None, 59, 59, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_139 (MaxPoolin (None, 29, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_146 (Conv2D)          (None, 27, 27, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_140 (MaxPoolin (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_147 (Conv2D)          (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_141 (MaxPoolin (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 4096)              26218496  \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 31,098,950\n",
            "Trainable params: 31,098,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMJ7BHmedpNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490dbb3a-a44c-4866-a5de-aefe62a6ed58"
      },
      "source": [
        "model_11.fit(train_dataset_128, epochs=20, validation_data=val_dataset_128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 19s 181ms/step - loss: 1.7504 - accuracy: 0.2252\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.6319 - accuracy: 0.2878\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.5670 - accuracy: 0.3242\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.5108 - accuracy: 0.3452\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.4491 - accuracy: 0.3735\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.4016 - accuracy: 0.4208\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 19s 189ms/step - loss: 1.4188 - accuracy: 0.4257\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 18s 183ms/step - loss: 1.3143 - accuracy: 0.4645\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 1.2339 - accuracy: 0.5032\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 20s 195ms/step - loss: 1.3008 - accuracy: 0.4812\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 19s 192ms/step - loss: 1.1766 - accuracy: 0.5237\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 19s 189ms/step - loss: 1.0942 - accuracy: 0.5799\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.0188 - accuracy: 0.6092\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 0.8612 - accuracy: 0.6724\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 18s 177ms/step - loss: 0.7197 - accuracy: 0.7284\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 0.6003 - accuracy: 0.7790\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 20s 198ms/step - loss: 0.8882 - accuracy: 0.6910\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 19s 193ms/step - loss: 0.5764 - accuracy: 0.8080\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 19s 189ms/step - loss: 0.3750 - accuracy: 0.8657\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 0.3266 - accuracy: 0.8858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93e3ba90d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kPnsU5Kdpgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0bc67b-f5c8-4089-ffeb-444bd39a9cd1"
      },
      "source": [
        "loss, acc = model_11.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 146ms/step - loss: 2.4466 - accuracy: 0.5005\n",
            "Loss 2.45, accuracy 50.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8VavX9z9Mcd"
      },
      "source": [
        "### Model 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xyDt8aEgiuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f9fe15-3d3f-4018-8aea-41bb90275cca"
      },
      "source": [
        "image_size = 128\n",
        "model_12 = Sequential()\n",
        "model_12.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_12.add(Convolution2D(32, 3, strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_12.add(Convolution2D(64, 3,strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_12.add(Convolution2D(128, 3,strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_12.add(Convolution2D(256, 3,strides=1, activation= 'relu'))\n",
        "model_12.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_12.add(Flatten())\n",
        "model_12.add(Dense(2048, activation='relu')) #con 4096 llegabamos a 0.67 en train y 0.45 en test\n",
        "model_12.add(Dropout(0.6))\n",
        "model_12.add(Dense(1024, activation='relu'))\n",
        "model_12.add(Dropout(0.6))\n",
        "model_12.add(Dense(256, activation='relu'))\n",
        "model_12.add(Dropout(0.5))\n",
        "model_12.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_12.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_12.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_38 (Rescaling)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_152 (Conv2D)          (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_146 (MaxPoolin (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_153 (Conv2D)          (None, 61, 61, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_147 (MaxPoolin (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_154 (Conv2D)          (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_148 (MaxPoolin (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_155 (Conv2D)          (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_149 (MaxPoolin (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_33 (Flatten)         (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 2048)              18876416  \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 21,626,950\n",
            "Trainable params: 21,626,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCbq3Md9gjXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9bba1b-218e-4673-8290-0d876983386b"
      },
      "source": [
        "model_12.fit(train_dataset_128, epochs=15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "96/96 [==============================] - 21s 198ms/step - loss: 1.7536 - accuracy: 0.2326\n",
            "Epoch 2/15\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.6229 - accuracy: 0.2908\n",
            "Epoch 3/15\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.5985 - accuracy: 0.3016\n",
            "Epoch 4/15\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.5370 - accuracy: 0.3371\n",
            "Epoch 5/15\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.5004 - accuracy: 0.3498\n",
            "Epoch 6/15\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 1.4635 - accuracy: 0.3803\n",
            "Epoch 7/15\n",
            "96/96 [==============================] - 18s 181ms/step - loss: 1.3991 - accuracy: 0.4164\n",
            "Epoch 8/15\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.4147 - accuracy: 0.4118\n",
            "Epoch 9/15\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 1.3839 - accuracy: 0.4296\n",
            "Epoch 10/15\n",
            "96/96 [==============================] - 18s 175ms/step - loss: 1.3656 - accuracy: 0.4407\n",
            "Epoch 11/15\n",
            "96/96 [==============================] - 18s 182ms/step - loss: 1.3614 - accuracy: 0.4512\n",
            "Epoch 12/15\n",
            "96/96 [==============================] - 19s 184ms/step - loss: 1.2260 - accuracy: 0.5056\n",
            "Epoch 13/15\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.1272 - accuracy: 0.5589\n",
            "Epoch 14/15\n",
            "96/96 [==============================] - 18s 173ms/step - loss: 1.1052 - accuracy: 0.5611\n",
            "Epoch 15/15\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 0.9784 - accuracy: 0.6241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93b631c2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EQcWKBDgj3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1173c55-381a-4935-e431-0db770107b7d"
      },
      "source": [
        "loss, acc = model_12.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 5s 133ms/step - loss: 1.5043 - accuracy: 0.5043\n",
            "Loss 1.5, accuracy 50.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlSt0kaq5ctf"
      },
      "source": [
        "### Model 13: ImageGenerator and Early Stopping\n",
        "\n",
        "Same model but data augmentation techniques "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WToVR3M3ioY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff0dd29-c3f5-4d9d-976f-d8422c0eb2aa"
      },
      "source": [
        "image_size = 128\n",
        "model_13 = Sequential()\n",
        "model_13.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model_13.add(RandomFlip(mode=\"horizontal\"))\n",
        "model_13.add(RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "model_13.add(RandomRotation(factor=(-0.1, 0.1)))\n",
        "model_13.add(RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "model_13.add(Convolution2D(32, 3, strides=1, activation= 'relu'))\n",
        "model_13.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_13.add(Convolution2D(64, 3,strides=1, activation= 'relu'))\n",
        "model_13.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_13.add(Convolution2D(128, 3,strides=1, activation= 'relu'))\n",
        "model_13.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_13.add(Convolution2D(256, 3,strides=1, activation= 'relu'))\n",
        "model_13.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_13.add(Flatten())\n",
        "model_13.add(Dense(2048, activation='relu'))\n",
        "model_13.add(Dropout(0.6))\n",
        "#model_21.add(Dense(1024, activation='relu'))\n",
        "#model_21.add(Dropout(0.6))\n",
        "model_13.add(Dense(256, activation='relu'))\n",
        "model_13.add(Dropout(0.5))\n",
        "model_13.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "model_13.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_13.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_14 (Rescaling)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_flip (RandomFlip)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_zoom (RandomZoom)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_rotation (RandomRotat (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_translation (RandomTr (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 2048)              18876416  \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 19,790,918\n",
            "Trainable params: 19,790,918\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-2qVR8OER2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8a2e4c-4216-4758-f61f-bb7e1143159a"
      },
      "source": [
        "callback = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "history = model_13.fit(train_dataset_128, epochs=50, validation_data=val_dataset_128, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "48/48 [==============================] - 23s 402ms/step - loss: 1.7587 - accuracy: 0.2224 - val_loss: 1.6300 - val_accuracy: 0.3245\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 19s 357ms/step - loss: 1.6297 - accuracy: 0.2922 - val_loss: 1.6450 - val_accuracy: 0.3283\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 19s 361ms/step - loss: 1.5410 - accuracy: 0.3572 - val_loss: 1.4336 - val_accuracy: 0.4156\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 20s 373ms/step - loss: 1.4985 - accuracy: 0.3847 - val_loss: 1.4465 - val_accuracy: 0.4236\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 20s 370ms/step - loss: 1.4544 - accuracy: 0.4059 - val_loss: 1.4506 - val_accuracy: 0.4250\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 20s 390ms/step - loss: 1.4209 - accuracy: 0.4185 - val_loss: 1.4011 - val_accuracy: 0.4739\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 21s 407ms/step - loss: 1.3920 - accuracy: 0.4356 - val_loss: 1.4659 - val_accuracy: 0.4663\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 21s 411ms/step - loss: 1.3644 - accuracy: 0.4478 - val_loss: 1.3728 - val_accuracy: 0.4801\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 22s 414ms/step - loss: 1.3625 - accuracy: 0.4546 - val_loss: 1.3985 - val_accuracy: 0.4929\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 22s 418ms/step - loss: 1.3263 - accuracy: 0.4745 - val_loss: 1.4223 - val_accuracy: 0.4891\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 22s 422ms/step - loss: 1.2978 - accuracy: 0.4813 - val_loss: 1.2217 - val_accuracy: 0.5389\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 22s 430ms/step - loss: 1.2937 - accuracy: 0.4878 - val_loss: 1.3304 - val_accuracy: 0.5261\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 22s 425ms/step - loss: 1.2630 - accuracy: 0.5042 - val_loss: 1.4626 - val_accuracy: 0.4839\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 22s 417ms/step - loss: 1.2485 - accuracy: 0.5057 - val_loss: 1.3222 - val_accuracy: 0.5294\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 22s 423ms/step - loss: 1.2164 - accuracy: 0.5250 - val_loss: 1.3677 - val_accuracy: 0.5346\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 22s 416ms/step - loss: 1.2344 - accuracy: 0.5308 - val_loss: 1.1992 - val_accuracy: 0.5640\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 21s 408ms/step - loss: 1.2000 - accuracy: 0.5341 - val_loss: 1.1592 - val_accuracy: 0.5512\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 21s 408ms/step - loss: 1.2435 - accuracy: 0.5304 - val_loss: 1.4464 - val_accuracy: 0.5076\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 21s 407ms/step - loss: 1.1767 - accuracy: 0.5438 - val_loss: 1.2341 - val_accuracy: 0.5621\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 21s 405ms/step - loss: 1.1458 - accuracy: 0.5545 - val_loss: 1.1833 - val_accuracy: 0.5811\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 21s 409ms/step - loss: 1.1511 - accuracy: 0.5653 - val_loss: 1.1622 - val_accuracy: 0.5811\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 22s 414ms/step - loss: 1.1116 - accuracy: 0.5786 - val_loss: 1.0790 - val_accuracy: 0.6072\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 22s 418ms/step - loss: 1.1234 - accuracy: 0.5714 - val_loss: 1.2293 - val_accuracy: 0.5565\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 24s 448ms/step - loss: 1.1045 - accuracy: 0.5826 - val_loss: 1.5471 - val_accuracy: 0.4967\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 22s 412ms/step - loss: 1.1077 - accuracy: 0.5770 - val_loss: 1.1538 - val_accuracy: 0.5920\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 21s 402ms/step - loss: 1.0716 - accuracy: 0.5925 - val_loss: 1.2377 - val_accuracy: 0.5598\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 22s 418ms/step - loss: 1.0579 - accuracy: 0.6157 - val_loss: 1.3486 - val_accuracy: 0.5574\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 21s 410ms/step - loss: 1.0383 - accuracy: 0.6164 - val_loss: 1.2648 - val_accuracy: 0.5655\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 23s 442ms/step - loss: 1.0321 - accuracy: 0.6176 - val_loss: 1.3300 - val_accuracy: 0.5583\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 22s 427ms/step - loss: 1.0203 - accuracy: 0.6164 - val_loss: 1.2179 - val_accuracy: 0.5683\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 22s 424ms/step - loss: 1.0322 - accuracy: 0.6154 - val_loss: 1.3101 - val_accuracy: 0.5773\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 22s 418ms/step - loss: 1.0029 - accuracy: 0.6270 - val_loss: 1.0845 - val_accuracy: 0.6181\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 22s 422ms/step - loss: 0.9861 - accuracy: 0.6353 - val_loss: 1.1139 - val_accuracy: 0.6167\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 22s 427ms/step - loss: 0.9869 - accuracy: 0.6332 - val_loss: 1.0200 - val_accuracy: 0.6333\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 22s 425ms/step - loss: 0.9921 - accuracy: 0.6280 - val_loss: 1.2045 - val_accuracy: 0.5920\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 22s 426ms/step - loss: 0.9722 - accuracy: 0.6364 - val_loss: 1.2795 - val_accuracy: 0.5797\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 22s 432ms/step - loss: 0.9396 - accuracy: 0.6454 - val_loss: 1.0537 - val_accuracy: 0.6319\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 22s 428ms/step - loss: 0.9891 - accuracy: 0.6330 - val_loss: 1.3413 - val_accuracy: 0.5769\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 22s 425ms/step - loss: 0.9561 - accuracy: 0.6386 - val_loss: 1.1862 - val_accuracy: 0.5996\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 22s 423ms/step - loss: 0.9276 - accuracy: 0.6652 - val_loss: 1.0044 - val_accuracy: 0.6366\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 22s 431ms/step - loss: 0.9462 - accuracy: 0.6415 - val_loss: 1.1148 - val_accuracy: 0.6157\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 22s 428ms/step - loss: 0.9197 - accuracy: 0.6581 - val_loss: 1.3826 - val_accuracy: 0.5740\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 22s 414ms/step - loss: 0.9277 - accuracy: 0.6493 - val_loss: 1.0760 - val_accuracy: 0.6252\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 21s 405ms/step - loss: 0.9093 - accuracy: 0.6505 - val_loss: 1.3762 - val_accuracy: 0.5764\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 22s 415ms/step - loss: 0.9225 - accuracy: 0.6555 - val_loss: 1.0272 - val_accuracy: 0.6442\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 22s 421ms/step - loss: 0.9162 - accuracy: 0.6609 - val_loss: 1.2788 - val_accuracy: 0.5916\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 21s 391ms/step - loss: 0.8863 - accuracy: 0.6710 - val_loss: 1.2218 - val_accuracy: 0.6139\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 21s 402ms/step - loss: 0.8700 - accuracy: 0.6814 - val_loss: 1.1813 - val_accuracy: 0.6181\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 22s 414ms/step - loss: 0.8534 - accuracy: 0.6848 - val_loss: 1.3372 - val_accuracy: 0.5863\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 22s 419ms/step - loss: 0.8387 - accuracy: 0.6886 - val_loss: 1.0126 - val_accuracy: 0.6433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mHCPLVO0wmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014a4f01-765e-446e-93e7-5b6a90da3735"
      },
      "source": [
        "loss, acc = model_13.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 5s 200ms/step - loss: 0.9942 - accuracy: 0.6483\n",
            "Loss 0.994, accuracy 64.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxSuRemcJLwQ"
      },
      "source": [
        "### Model 14: Huge last model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa8Zs1Mwx-SR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7172b0-bd01-4ffe-f6ee-06a9a5208d7a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "\n",
        "model_14 = Sequential()\n",
        "model_14.add(Rescaling(scale=1./255, input_shape=(128, 128, 3)))\n",
        "model_14.add(RandomFlip(mode=\"horizontal\"))\n",
        "model_14.add(RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "model_14.add(RandomRotation(factor=(-0.1, 0.1)))\n",
        "model_14.add(RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "\n",
        "model_14.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(128, 128, 3)))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Dropout(0.5))\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_14.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "#model.add(BatchNormalization(axis=chanDim))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_14.add(Dropout(0.5))\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_14.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_14.add(Dropout(0.5))\n",
        "\n",
        "model_14.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_14.add(Dropout(0.5))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model_14.add(Flatten())\n",
        "model_14.add(Dense(5000))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(Dense(500))\n",
        "model_14.add(Activation(\"relu\"))\n",
        "model_14.add(BatchNormalization())\n",
        "model_14.add(Dropout(0.5))\n",
        "# use a *softmax* activation for single-label classification\n",
        "# and *sigmoid* activation for multi-label classification\n",
        "model_14.add(Dense(6))\n",
        "model_14.add(Activation(\"softmax\"))\n",
        "\n",
        "model_14.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_14.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 5000)              81925000  \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 5000)              20000     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 500)               2500500   \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 6)                 3006      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 85,026,122\n",
            "Trainable params: 85,013,778\n",
            "Non-trainable params: 12,344\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YprqktE2ynro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1529137-12a9-4aef-f32f-e779218092e1"
      },
      "source": [
        "model_14.fit(train_dataset_128, epochs=50, validation_data=val_dataset_128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "48/48 [==============================] - 32s 542ms/step - loss: 2.5393 - accuracy: 0.2667 - val_loss: 3.2329 - val_accuracy: 0.2244\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 27s 535ms/step - loss: 1.7110 - accuracy: 0.3920 - val_loss: 4.2824 - val_accuracy: 0.1309\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 28s 550ms/step - loss: 1.5540 - accuracy: 0.4424 - val_loss: 4.2788 - val_accuracy: 0.1347\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 28s 548ms/step - loss: 1.4434 - accuracy: 0.4921 - val_loss: 1.6822 - val_accuracy: 0.4279\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 28s 546ms/step - loss: 1.3053 - accuracy: 0.5218 - val_loss: 1.7886 - val_accuracy: 0.4227\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 28s 544ms/step - loss: 1.2111 - accuracy: 0.5495 - val_loss: 1.6337 - val_accuracy: 0.4322\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 28s 547ms/step - loss: 1.1558 - accuracy: 0.5806 - val_loss: 1.5215 - val_accuracy: 0.4720\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 1.0451 - accuracy: 0.6121 - val_loss: 1.5538 - val_accuracy: 0.4806\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 28s 550ms/step - loss: 0.9881 - accuracy: 0.6347 - val_loss: 1.4504 - val_accuracy: 0.5100\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 28s 548ms/step - loss: 0.8582 - accuracy: 0.6785 - val_loss: 1.3855 - val_accuracy: 0.5313\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 28s 556ms/step - loss: 0.7559 - accuracy: 0.7185 - val_loss: 1.4197 - val_accuracy: 0.5313\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 28s 548ms/step - loss: 0.6766 - accuracy: 0.7509 - val_loss: 1.9506 - val_accuracy: 0.4606\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 28s 555ms/step - loss: 0.6021 - accuracy: 0.7801 - val_loss: 1.9975 - val_accuracy: 0.5014\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 28s 551ms/step - loss: 0.4764 - accuracy: 0.8253 - val_loss: 1.8174 - val_accuracy: 0.5204\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 0.4662 - accuracy: 0.8369 - val_loss: 1.7459 - val_accuracy: 0.5332\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 28s 550ms/step - loss: 0.3702 - accuracy: 0.8643 - val_loss: 1.8848 - val_accuracy: 0.5408\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 28s 549ms/step - loss: 0.3159 - accuracy: 0.8892 - val_loss: 2.5615 - val_accuracy: 0.4616\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 28s 544ms/step - loss: 0.2970 - accuracy: 0.8942 - val_loss: 1.9617 - val_accuracy: 0.5470\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 0.2245 - accuracy: 0.9231 - val_loss: 2.2105 - val_accuracy: 0.5346\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 28s 546ms/step - loss: 0.2060 - accuracy: 0.9307 - val_loss: 2.0705 - val_accuracy: 0.5527\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 28s 542ms/step - loss: 0.1780 - accuracy: 0.9376 - val_loss: 2.2740 - val_accuracy: 0.5270\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 28s 550ms/step - loss: 0.1527 - accuracy: 0.9426 - val_loss: 2.1661 - val_accuracy: 0.5332\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 27s 534ms/step - loss: 0.1124 - accuracy: 0.9617 - val_loss: 2.1707 - val_accuracy: 0.5531\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 28s 549ms/step - loss: 0.1270 - accuracy: 0.9583 - val_loss: 2.2503 - val_accuracy: 0.5493\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 28s 546ms/step - loss: 0.1684 - accuracy: 0.9413 - val_loss: 2.4135 - val_accuracy: 0.5361\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 28s 555ms/step - loss: 0.1859 - accuracy: 0.9388 - val_loss: 2.5147 - val_accuracy: 0.5365\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 28s 546ms/step - loss: 0.1084 - accuracy: 0.9619 - val_loss: 2.2681 - val_accuracy: 0.5588\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 28s 557ms/step - loss: 0.0739 - accuracy: 0.9747 - val_loss: 2.3013 - val_accuracy: 0.5598\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 29s 560ms/step - loss: 0.0822 - accuracy: 0.9717 - val_loss: 2.6733 - val_accuracy: 0.5261\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 28s 547ms/step - loss: 0.0867 - accuracy: 0.9697 - val_loss: 2.7408 - val_accuracy: 0.5541\n",
            "Epoch 31/50\n",
            "12/48 [======>.......................] - ETA: 17s - loss: 0.0943 - accuracy: 0.9675"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b333d62db5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset_128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QCX1g-60_lJ"
      },
      "source": [
        "loss, acc = model_14.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLe9mFwhRKC"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5xs6_RhRKC"
      },
      "source": [
        "While designing your own network might produce some nice results, it is generally better to transfer the knowledge available in a pre-trained network. This not only can produce better results, but also saves a lot of design time. The [Keras Applications](https://keras.io/api/applications/) module contains several network designs ready to use. For instance, to exploit the famous VGG16 network we do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTLIqMTAhRKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae398563-5d15-4527-d68b-3f6f809f7aa2"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUvx4zdhRKD"
      },
      "source": [
        "By default all Keras Applications networks are preloaded with the weights that were obtained from training the network over the [ImageNet dataset](http://www.image-net.org/). To adapt the network to our problem we need to specify the shape of our input images, and also remove the output layers (top) of the original network, since we have a different number of classes.\n",
        "\n",
        "Now, how do we do transfer learning over this network? We will show here how to implement the bottleneck features strategy. First, we will mark the VGG16 model as **non-trainable**, so that the weights remain frozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWsuQg0phRKD"
      },
      "source": [
        "vgg16_model.trainable = False"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljtS1PhRhRKE"
      },
      "source": [
        "Now we will build a neural network that includes the VGG16 model as one of its \"layers\". Since the VGG16 was trained with an specific way of normalizing the images, we will need to normalize our images in the same way. Conveniently, Keras also provides a function for doing VGG16-style normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjFpoNRIhRKE"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBk5uRChRKF"
      },
      "source": [
        "We can try this with some image ir our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ukW26shRKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145c96eb-d40d-4f28-df85-d9fb7aa22be5"
      },
      "source": [
        "for X_batch, _ in train_dataset:\n",
        "    break\n",
        "    \n",
        "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
        "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before normalizing: [[[ 62.5   38.75  25.25]\n",
            "  [ 97.5   88.    87.5 ]\n",
            "  [119.   109.5  109.5 ]]\n",
            "\n",
            " [[ 48.25  26.    21.  ]\n",
            "  [ 39.25  18.25  15.25]\n",
            "  [ 45.75  25.75  16.25]]\n",
            "\n",
            " [[116.    97.5   79.  ]\n",
            "  [115.5  100.25  78.25]\n",
            "  [ 60.    55.25  53.5 ]]]\n",
            "After normalizing: [[[-78.689     -78.029     -61.18     ]\n",
            "  [-16.439003  -28.779     -26.18     ]\n",
            "  [  5.560997   -7.2789993  -4.6800003]]\n",
            "\n",
            " [[-82.939     -90.779     -75.43     ]\n",
            "  [-88.689     -98.529     -84.43     ]\n",
            "  [-87.689     -91.029     -77.93     ]]\n",
            "\n",
            " [[-24.939003  -19.279      -7.6800003]\n",
            "  [-25.689003  -16.529      -8.18     ]\n",
            "  [-50.439003  -61.529     -63.68     ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx3EQeEThRKF"
      },
      "source": [
        "The normalization required by VGG16 involves swapping the order of color channels (RGB -> BGR) and substracting the mean values over the ImageNet dataset for each color channel separately. Fortunately the `preprocess_input` function does all the work for us. Furthermore, we can plug this function as the first layer of our network, similarly to the `Rescaling` we used before. We can do this with a `Lambda` layer, which allows building a layer out of any (differentiable!) function. Let's start our model with this layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnSoUUrQhRKG"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83-_MmdhRKG"
      },
      "source": [
        "After this, we can add the whole VGG16 network as a layer, and our custom trainable layers after it. Note this is an overly simple model with some mistakes introduced; a real transfer learning network would have a better design."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f0RdEumhRKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebcf566-743e-437c-a4b1-3da9d33d8b43"
      },
      "source": [
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 14,717,766\n",
            "Trainable params: 3,078\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NueAGF7DhRKG"
      },
      "source": [
        "Notice how in the model summary we can see the whole network has millions of parameters, but since we have frozen the VGG16 part, only a few thousand parameters will be trained: those in the Dense layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7iMKlQhRKH"
      },
      "source": [
        "We can now compile and train this model in the usual way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBG6zKwhRKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ecf7d6-cb82-49a8-e56e-5e21a7aed53c"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
        "model.fit(train_dataset, epochs=1)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 15s 127ms/step - loss: 77.1788 - accuracy: 0.3071\n",
            "33/33 [==============================] - 6s 136ms/step - loss: 157.6190 - accuracy: 0.2367\n",
            "Loss 1.58e+02, accuracy 23.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sb7SVSkhRKH"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Using the bottleneck strategy, implement a network doing transfer learning from VGG16. If you do it properly, you should be able to obtain better results than with your previously designed network, at least a 80% of accuracy over the test set.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2LQa-mhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Some tips and strategies that can help you optimize your network design:\n",
        "\n",
        "    \n",
        "- Include one or more Dense layers with the appropriate activation functions before the output layer.\n",
        "- Try using a [GlobalAveragePooling layer](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) instead of a Flatten layer. This layer computes an average of all pixel values for each channel, and sometimes performs better than a regular Flatten.\n",
        "- And remember all the tricks from the previous exercise!\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6bbHEWhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "You can further improve the network accuracy with the following ideas\n",
        "\n",
        "- Use the PRO strategies from the previous exercise.\n",
        "- Try other pre-trained networks from <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, such as ResNet, Xception or EfficientNet.\n",
        "- Use more advanced transfer learning strategies, like fine-tuning or a combination of bottleneck features and fine-tuning.\n",
        "   \n",
        "If you use all the tricks, it is possible to obtain more than 90% accuracy in the test set.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Cq1drT9qLt"
      },
      "source": [
        "### Model: vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAqZScMGEo7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60383f49-a784-432f-aded-48b9e7308669"
      },
      "source": [
        "model_vgg16 = Sequential()\n",
        "model_vgg16.add(Lambda(preprocess_input, input_shape=(32, 32, 3)))\n",
        "model_vgg16.add(vgg16_model)\n",
        "model_vgg16.add(Flatten())\n",
        "model_vgg16.add(Dense(128, activation='relu'))\n",
        "model_vgg16.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_vgg16.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_2 (Lambda)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 14,781,126\n",
            "Trainable params: 66,438\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXVzMX7WO_Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebfe7f0c-ca43-4abd-d861-1e1648b104e6"
      },
      "source": [
        "model_vgg16.fit(train_dataset, epochs=10, validation_data= val_dataset)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "96/96 [==============================] - 18s 173ms/step - loss: 12.4963 - accuracy: 0.3155 - val_loss: 4.8476 - val_accuracy: 0.4018\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 3.6560 - accuracy: 0.4664 - val_loss: 3.8530 - val_accuracy: 0.4189\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 2.1970 - accuracy: 0.5391 - val_loss: 3.4546 - val_accuracy: 0.4160\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 1.4519 - accuracy: 0.6252 - val_loss: 3.2545 - val_accuracy: 0.4293\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 18s 175ms/step - loss: 1.0506 - accuracy: 0.6995 - val_loss: 3.1346 - val_accuracy: 0.4279\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 17s 171ms/step - loss: 0.7899 - accuracy: 0.7542 - val_loss: 3.1389 - val_accuracy: 0.4274\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 0.6152 - accuracy: 0.7964 - val_loss: 3.0815 - val_accuracy: 0.4431\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 0.4528 - accuracy: 0.8554 - val_loss: 3.0366 - val_accuracy: 0.4407\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 17s 173ms/step - loss: 0.3740 - accuracy: 0.8809 - val_loss: 3.1374 - val_accuracy: 0.4303\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 0.3007 - accuracy: 0.9099 - val_loss: 3.1958 - val_accuracy: 0.4355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5eac2ee4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vpTUElt-mr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d87531-676e-42e5-ab86-5287d1b0fdb0"
      },
      "source": [
        "loss, acc = model_vgg16.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 6s 132ms/step - loss: 3.1444 - accuracy: 0.4657\n",
            "Loss 3.14, accuracy 46.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh6reqXYtM6d"
      },
      "source": [
        "En esta nueva red reemplazamos la cpa Flatten por GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRIh-y08H3qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf9e2bd-8de4-4b90-f29a-54dff2562a22"
      },
      "source": [
        "vgg16_model = VGG16(include_top=False, input_shape=(128, 128, 3))\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "model_vgg16 = Sequential()\n",
        "model_vgg16.add(Lambda(preprocess_input, input_shape=(128, 128, 3)))\n",
        "model_vgg16.add(vgg16_model)\n",
        "model_vgg16.add(GlobalAveragePooling2D())\n",
        "model_vgg16.add(Dense(128, activation='relu'))\n",
        "model_vgg16.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_vgg16.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_3 (Lambda)            (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 14,781,126\n",
            "Trainable params: 66,438\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUNNO51ZISWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8db427-00fb-47c7-d6ae-8b35c34a23e4"
      },
      "source": [
        "model_vgg16.fit(train_dataset_128, epochs=10, validation_data= val_dataset_128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "48/48 [==============================] - 43s 714ms/step - loss: 5.7730 - accuracy: 0.3728 - val_loss: 1.6179 - val_accuracy: 0.6063\n",
            "Epoch 2/10\n",
            "48/48 [==============================] - 25s 480ms/step - loss: 1.2322 - accuracy: 0.6510 - val_loss: 1.2329 - val_accuracy: 0.6613\n",
            "Epoch 3/10\n",
            "48/48 [==============================] - 25s 485ms/step - loss: 0.8343 - accuracy: 0.7275 - val_loss: 1.1457 - val_accuracy: 0.6779\n",
            "Epoch 4/10\n",
            "48/48 [==============================] - 26s 502ms/step - loss: 0.6075 - accuracy: 0.7935 - val_loss: 1.1088 - val_accuracy: 0.6940\n",
            "Epoch 5/10\n",
            "48/48 [==============================] - 25s 495ms/step - loss: 0.5099 - accuracy: 0.8290 - val_loss: 1.0490 - val_accuracy: 0.7040\n",
            "Epoch 6/10\n",
            "48/48 [==============================] - 26s 510ms/step - loss: 0.3887 - accuracy: 0.8726 - val_loss: 1.0478 - val_accuracy: 0.7026\n",
            "Epoch 7/10\n",
            "48/48 [==============================] - 26s 511ms/step - loss: 0.3156 - accuracy: 0.9039 - val_loss: 1.0491 - val_accuracy: 0.7135\n",
            "Epoch 8/10\n",
            "48/48 [==============================] - 25s 491ms/step - loss: 0.2374 - accuracy: 0.9337 - val_loss: 1.0608 - val_accuracy: 0.7139\n",
            "Epoch 9/10\n",
            "48/48 [==============================] - 26s 506ms/step - loss: 0.1994 - accuracy: 0.9491 - val_loss: 1.0474 - val_accuracy: 0.7135\n",
            "Epoch 10/10\n",
            "48/48 [==============================] - 25s 486ms/step - loss: 0.1709 - accuracy: 0.9631 - val_loss: 1.0826 - val_accuracy: 0.7168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63cc3d87d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IREC0PhGJUrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2beceb-4726-4efe-8fce-e2c1882dfd45"
      },
      "source": [
        "loss, acc = model_vgg16.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 9s 417ms/step - loss: 1.1150 - accuracy: 0.7130\n",
            "Loss 1.12, accuracy 71.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do2csVSftW2b"
      },
      "source": [
        "hemos mejorado el accuracy en el testset pero es evidente el overfitting dado que ealcanzamos un accuracy en training del 90% que se reduce en un 20 en el test set. Por esta razon agregaremos capas Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDvSt5_AKQSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e54e230-f840-4a0a-82ef-a3f4f5082b59"
      },
      "source": [
        "vgg16_model = VGG16(include_top=False, input_shape=(128, 128, 3))\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "model_vgg16 = Sequential()\n",
        "model_vgg16.add(Lambda(preprocess_input, input_shape=(128, 128, 3)))\n",
        "model_vgg16.add(vgg16_model)\n",
        "model_vgg16.add(GlobalAveragePooling2D())\n",
        "model_vgg16.add(Dense(256, activation='relu'))\n",
        "model_vgg16.add(Dropout(0.6))\n",
        "model_vgg16.add(Dense(128, activation='relu'))\n",
        "model_vgg16.add(Dropout(0.6))\n",
        "model_vgg16.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_vgg16.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_6 (Lambda)            (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 14,879,686\n",
            "Trainable params: 164,998\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glycjf_XKfax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d8b4e7-439d-49fa-b7c4-c8b7c86c6909"
      },
      "source": [
        "model_vgg16.fit(train_dataset_128, epochs=40, validation_data= val_dataset_128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "48/48 [==============================] - 30s 587ms/step - loss: 8.7273 - accuracy: 0.2573 - val_loss: 1.3162 - val_accuracy: 0.5085\n",
            "Epoch 2/40\n",
            "48/48 [==============================] - 28s 550ms/step - loss: 1.9594 - accuracy: 0.3453 - val_loss: 1.3658 - val_accuracy: 0.5052\n",
            "Epoch 3/40\n",
            "48/48 [==============================] - 26s 513ms/step - loss: 1.6269 - accuracy: 0.3856 - val_loss: 1.2916 - val_accuracy: 0.5527\n",
            "Epoch 4/40\n",
            "48/48 [==============================] - 27s 530ms/step - loss: 1.5059 - accuracy: 0.4249 - val_loss: 1.2242 - val_accuracy: 0.5754\n",
            "Epoch 5/40\n",
            "48/48 [==============================] - 27s 535ms/step - loss: 1.4208 - accuracy: 0.4719 - val_loss: 1.1478 - val_accuracy: 0.6167\n",
            "Epoch 6/40\n",
            "48/48 [==============================] - 27s 518ms/step - loss: 1.3548 - accuracy: 0.5058 - val_loss: 1.0871 - val_accuracy: 0.6162\n",
            "Epoch 7/40\n",
            "48/48 [==============================] - 28s 544ms/step - loss: 1.2716 - accuracy: 0.5373 - val_loss: 1.0375 - val_accuracy: 0.6456\n",
            "Epoch 8/40\n",
            "48/48 [==============================] - 26s 509ms/step - loss: 1.1805 - accuracy: 0.5706 - val_loss: 1.0383 - val_accuracy: 0.6475\n",
            "Epoch 9/40\n",
            "48/48 [==============================] - 27s 522ms/step - loss: 1.1459 - accuracy: 0.5715 - val_loss: 0.9856 - val_accuracy: 0.6713\n",
            "Epoch 10/40\n",
            "48/48 [==============================] - 28s 557ms/step - loss: 1.0812 - accuracy: 0.6119 - val_loss: 0.9346 - val_accuracy: 0.6940\n",
            "Epoch 11/40\n",
            "48/48 [==============================] - 26s 512ms/step - loss: 1.0601 - accuracy: 0.6258 - val_loss: 0.9208 - val_accuracy: 0.6902\n",
            "Epoch 12/40\n",
            "48/48 [==============================] - 27s 524ms/step - loss: 0.9938 - accuracy: 0.6394 - val_loss: 0.8957 - val_accuracy: 0.6964\n",
            "Epoch 13/40\n",
            "48/48 [==============================] - 27s 521ms/step - loss: 0.9721 - accuracy: 0.6581 - val_loss: 0.9150 - val_accuracy: 0.6940\n",
            "Epoch 14/40\n",
            "48/48 [==============================] - 29s 569ms/step - loss: 0.9056 - accuracy: 0.6745 - val_loss: 0.8596 - val_accuracy: 0.7192\n",
            "Epoch 15/40\n",
            "48/48 [==============================] - 26s 515ms/step - loss: 0.8686 - accuracy: 0.6871 - val_loss: 0.8456 - val_accuracy: 0.7244\n",
            "Epoch 16/40\n",
            "48/48 [==============================] - 27s 533ms/step - loss: 0.8261 - accuracy: 0.7058 - val_loss: 0.8404 - val_accuracy: 0.7215\n",
            "Epoch 17/40\n",
            "48/48 [==============================] - 29s 573ms/step - loss: 0.8105 - accuracy: 0.7008 - val_loss: 0.8265 - val_accuracy: 0.7306\n",
            "Epoch 18/40\n",
            "48/48 [==============================] - 27s 519ms/step - loss: 0.7927 - accuracy: 0.7152 - val_loss: 0.8349 - val_accuracy: 0.7201\n",
            "Epoch 19/40\n",
            "48/48 [==============================] - 29s 562ms/step - loss: 0.7796 - accuracy: 0.7225 - val_loss: 0.8063 - val_accuracy: 0.7315\n",
            "Epoch 20/40\n",
            "48/48 [==============================] - 28s 545ms/step - loss: 0.7377 - accuracy: 0.7359 - val_loss: 0.7912 - val_accuracy: 0.7372\n",
            "Epoch 21/40\n",
            "48/48 [==============================] - 27s 535ms/step - loss: 0.7056 - accuracy: 0.7535 - val_loss: 0.7878 - val_accuracy: 0.7424\n",
            "Epoch 22/40\n",
            "48/48 [==============================] - 26s 509ms/step - loss: 0.7427 - accuracy: 0.7437 - val_loss: 0.7929 - val_accuracy: 0.7367\n",
            "Epoch 23/40\n",
            "48/48 [==============================] - 27s 526ms/step - loss: 0.6710 - accuracy: 0.7539 - val_loss: 0.7912 - val_accuracy: 0.7481\n",
            "Epoch 24/40\n",
            "48/48 [==============================] - 27s 518ms/step - loss: 0.6682 - accuracy: 0.7618 - val_loss: 0.7879 - val_accuracy: 0.7509\n",
            "Epoch 25/40\n",
            "48/48 [==============================] - 28s 553ms/step - loss: 0.6402 - accuracy: 0.7737 - val_loss: 0.7871 - val_accuracy: 0.7509\n",
            "Epoch 26/40\n",
            "48/48 [==============================] - 26s 508ms/step - loss: 0.6424 - accuracy: 0.7746 - val_loss: 0.7961 - val_accuracy: 0.7424\n",
            "Epoch 27/40\n",
            "48/48 [==============================] - 27s 521ms/step - loss: 0.6133 - accuracy: 0.7844 - val_loss: 0.7896 - val_accuracy: 0.7495\n",
            "Epoch 28/40\n",
            "48/48 [==============================] - 26s 508ms/step - loss: 0.5959 - accuracy: 0.7838 - val_loss: 0.7714 - val_accuracy: 0.7519\n",
            "Epoch 29/40\n",
            "48/48 [==============================] - 26s 516ms/step - loss: 0.5761 - accuracy: 0.7902 - val_loss: 0.7739 - val_accuracy: 0.7419\n",
            "Epoch 30/40\n",
            "48/48 [==============================] - 25s 489ms/step - loss: 0.5672 - accuracy: 0.7887 - val_loss: 0.7919 - val_accuracy: 0.7472\n",
            "Epoch 31/40\n",
            "48/48 [==============================] - 27s 536ms/step - loss: 0.5518 - accuracy: 0.8049 - val_loss: 0.7895 - val_accuracy: 0.7476\n",
            "Epoch 32/40\n",
            "48/48 [==============================] - 26s 517ms/step - loss: 0.5437 - accuracy: 0.7996 - val_loss: 0.7917 - val_accuracy: 0.7538\n",
            "Epoch 33/40\n",
            "48/48 [==============================] - 27s 521ms/step - loss: 0.5366 - accuracy: 0.8032 - val_loss: 0.7946 - val_accuracy: 0.7543\n",
            "Epoch 34/40\n",
            "48/48 [==============================] - 29s 570ms/step - loss: 0.5313 - accuracy: 0.8043 - val_loss: 0.7933 - val_accuracy: 0.7453\n",
            "Epoch 35/40\n",
            "48/48 [==============================] - 27s 521ms/step - loss: 0.5261 - accuracy: 0.8098 - val_loss: 0.7700 - val_accuracy: 0.7547\n",
            "Epoch 36/40\n",
            "48/48 [==============================] - 27s 524ms/step - loss: 0.5017 - accuracy: 0.8179 - val_loss: 0.7736 - val_accuracy: 0.7623\n",
            "Epoch 37/40\n",
            "48/48 [==============================] - 27s 525ms/step - loss: 0.5027 - accuracy: 0.8131 - val_loss: 0.7805 - val_accuracy: 0.7528\n",
            "Epoch 38/40\n",
            "48/48 [==============================] - 27s 532ms/step - loss: 0.4909 - accuracy: 0.8162 - val_loss: 0.8246 - val_accuracy: 0.7448\n",
            "Epoch 39/40\n",
            "48/48 [==============================] - 28s 540ms/step - loss: 0.4815 - accuracy: 0.8345 - val_loss: 0.8131 - val_accuracy: 0.7543\n",
            "Epoch 40/40\n",
            "48/48 [==============================] - 27s 526ms/step - loss: 0.4735 - accuracy: 0.8310 - val_loss: 0.7968 - val_accuracy: 0.7495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f65df328f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfp_aLnKeYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27410ff1-7cac-408c-96b5-b3d8387a7570"
      },
      "source": [
        "loss, acc = model_vgg16.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 7s 298ms/step - loss: 0.7450 - accuracy: 0.7594\n",
            "Loss 0.745, accuracy 75.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeYYNv0WtnFq"
      },
      "source": [
        "Se ha mitigado el overfitting, sin embargo, el accuracy ha suido pero aun hay sobreentrenamineto por lo cual acudimos a tcnicas de Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-kF4mYXY7mT",
        "outputId": "b28add7a-d80e-4fa6-d2d1-939b6c1302b6"
      },
      "source": [
        "vgg16_model = VGG16(include_top=False, input_shape=(128, 128, 3))\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "model_vgg16 = Sequential()\n",
        "model_vgg16.add(Rescaling(scale=1./255, input_shape=(128, 128, 3)))\n",
        "model_vgg16.add(RandomFlip(mode=\"horizontal\"))\n",
        "model_vgg16.add(RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "# model_vgg16.add(RandomRotation(factor=(-0.1, 0.1)))\n",
        "model_vgg16.add(RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "model_vgg16.add(Lambda(preprocess_input, input_shape=(128, 128, 3)))\n",
        "\n",
        "model_vgg16.add(vgg16_model)\n",
        "\n",
        "model_vgg16.add(GlobalAveragePooling2D())\n",
        "model_vgg16.add(Dense(256, activation='relu'))\n",
        "model_vgg16.add(Dropout(0.6))\n",
        "model_vgg16.add(Dense(128, activation='relu'))\n",
        "model_vgg16.add(Dropout(0.5))\n",
        "model_vgg16.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_vgg16.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_9 (Lambda)            (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling_9 (Rescaling)      (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_flip_16 (RandomFlip)  (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_flip_17 (RandomFlip)  (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_zoom_8 (RandomZoom)   (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_rotation_8 (RandomRot (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_translation_8 (Random (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_10  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 14,879,686\n",
            "Trainable params: 164,998\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGQtJtrLfo9-",
        "outputId": "67d71ca4-cef9-49b9-8069-42c3d4e41d57"
      },
      "source": [
        "model_vgg16.fit(train_dataset_128, epochs=50, validation_data= val_dataset_128)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "48/48 [==============================] - 27s 499ms/step - loss: 1.8590 - accuracy: 0.2359 - val_loss: 1.6033 - val_accuracy: 0.3776\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 26s 507ms/step - loss: 1.6256 - accuracy: 0.3251 - val_loss: 1.4423 - val_accuracy: 0.4445\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 26s 506ms/step - loss: 1.5354 - accuracy: 0.3818 - val_loss: 1.3289 - val_accuracy: 0.4981\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 26s 510ms/step - loss: 1.4652 - accuracy: 0.4127 - val_loss: 1.2555 - val_accuracy: 0.5266\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 26s 508ms/step - loss: 1.4124 - accuracy: 0.4484 - val_loss: 1.1828 - val_accuracy: 0.5602\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 29s 566ms/step - loss: 1.3537 - accuracy: 0.4744 - val_loss: 1.1462 - val_accuracy: 0.5750\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 28s 551ms/step - loss: 1.3439 - accuracy: 0.4817 - val_loss: 1.1504 - val_accuracy: 0.5787\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 31s 614ms/step - loss: 1.3220 - accuracy: 0.4903 - val_loss: 1.1339 - val_accuracy: 0.5825\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 28s 542ms/step - loss: 1.2742 - accuracy: 0.5206 - val_loss: 1.1263 - val_accuracy: 0.5754\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 28s 541ms/step - loss: 1.3196 - accuracy: 0.4895 - val_loss: 1.0864 - val_accuracy: 0.6025\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 27s 521ms/step - loss: 1.2865 - accuracy: 0.5066 - val_loss: 1.1003 - val_accuracy: 0.6025\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 27s 539ms/step - loss: 1.2532 - accuracy: 0.5278 - val_loss: 1.0763 - val_accuracy: 0.6063\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 29s 580ms/step - loss: 1.2629 - accuracy: 0.5260 - val_loss: 1.0610 - val_accuracy: 0.6110\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 29s 575ms/step - loss: 1.2396 - accuracy: 0.5289 - val_loss: 1.0730 - val_accuracy: 0.6139\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 30s 600ms/step - loss: 1.2255 - accuracy: 0.5391 - val_loss: 1.0617 - val_accuracy: 0.6200\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 29s 575ms/step - loss: 1.2205 - accuracy: 0.5413 - val_loss: 1.0540 - val_accuracy: 0.6210\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 30s 592ms/step - loss: 1.2349 - accuracy: 0.5357 - val_loss: 1.0457 - val_accuracy: 0.6115\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 27s 538ms/step - loss: 1.1902 - accuracy: 0.5430 - val_loss: 1.0487 - val_accuracy: 0.6172\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 28s 555ms/step - loss: 1.2047 - accuracy: 0.5567 - val_loss: 1.0597 - val_accuracy: 0.6139\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 27s 533ms/step - loss: 1.2049 - accuracy: 0.5500 - val_loss: 1.0687 - val_accuracy: 0.6077\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 27s 531ms/step - loss: 1.1958 - accuracy: 0.5418 - val_loss: 1.0139 - val_accuracy: 0.6276\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 27s 528ms/step - loss: 1.1907 - accuracy: 0.5399 - val_loss: 1.0407 - val_accuracy: 0.6243\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 31s 620ms/step - loss: 1.1795 - accuracy: 0.5565 - val_loss: 1.0444 - val_accuracy: 0.6243\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 1.1752 - accuracy: 0.5612 - val_loss: 1.0232 - val_accuracy: 0.6271\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 27s 532ms/step - loss: 1.1823 - accuracy: 0.5546 - val_loss: 1.0113 - val_accuracy: 0.6290\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 30s 583ms/step - loss: 1.1596 - accuracy: 0.5674 - val_loss: 1.0275 - val_accuracy: 0.6366\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 27s 535ms/step - loss: 1.1635 - accuracy: 0.5579 - val_loss: 1.0283 - val_accuracy: 0.6328\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 30s 604ms/step - loss: 1.1712 - accuracy: 0.5598 - val_loss: 1.0436 - val_accuracy: 0.6129\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 31s 605ms/step - loss: 1.1779 - accuracy: 0.5622 - val_loss: 1.0195 - val_accuracy: 0.6319\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 29s 579ms/step - loss: 1.1640 - accuracy: 0.5620 - val_loss: 1.0103 - val_accuracy: 0.6380\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 29s 563ms/step - loss: 1.1493 - accuracy: 0.5766 - val_loss: 0.9966 - val_accuracy: 0.6404\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 28s 540ms/step - loss: 1.1278 - accuracy: 0.5714 - val_loss: 1.0051 - val_accuracy: 0.6380\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 29s 569ms/step - loss: 1.1285 - accuracy: 0.5806 - val_loss: 1.0065 - val_accuracy: 0.6338\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 31s 612ms/step - loss: 1.1457 - accuracy: 0.5742 - val_loss: 1.0278 - val_accuracy: 0.6243\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 27s 529ms/step - loss: 1.1557 - accuracy: 0.5572 - val_loss: 1.0006 - val_accuracy: 0.6423\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 30s 593ms/step - loss: 1.1339 - accuracy: 0.5781 - val_loss: 0.9982 - val_accuracy: 0.6433\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 30s 599ms/step - loss: 1.1412 - accuracy: 0.5722 - val_loss: 1.0060 - val_accuracy: 0.6385\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 28s 546ms/step - loss: 1.1044 - accuracy: 0.5939 - val_loss: 0.9923 - val_accuracy: 0.6404\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 27s 535ms/step - loss: 1.1343 - accuracy: 0.5710 - val_loss: 0.9957 - val_accuracy: 0.6380\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 1.1292 - accuracy: 0.5820 - val_loss: 1.0129 - val_accuracy: 0.6433\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 28s 552ms/step - loss: 1.1188 - accuracy: 0.5874 - val_loss: 0.9971 - val_accuracy: 0.6442\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 30s 592ms/step - loss: 1.1210 - accuracy: 0.5843 - val_loss: 0.9950 - val_accuracy: 0.6352\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 30s 584ms/step - loss: 1.1260 - accuracy: 0.5846 - val_loss: 0.9889 - val_accuracy: 0.6461\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 28s 548ms/step - loss: 1.1239 - accuracy: 0.5775 - val_loss: 1.0027 - val_accuracy: 0.6385\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 27s 539ms/step - loss: 1.1045 - accuracy: 0.5880 - val_loss: 0.9874 - val_accuracy: 0.6428\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 29s 561ms/step - loss: 1.0895 - accuracy: 0.5838 - val_loss: 1.0030 - val_accuracy: 0.6418\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 29s 569ms/step - loss: 1.1238 - accuracy: 0.5834 - val_loss: 0.9767 - val_accuracy: 0.6480\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 30s 601ms/step - loss: 1.1118 - accuracy: 0.5801 - val_loss: 0.9820 - val_accuracy: 0.6409\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 27s 523ms/step - loss: 1.1219 - accuracy: 0.5869 - val_loss: 0.9888 - val_accuracy: 0.6471\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 31s 610ms/step - loss: 1.1155 - accuracy: 0.5799 - val_loss: 0.9852 - val_accuracy: 0.6452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c02f9fad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M8M86Tok-gf",
        "outputId": "98f296d5-27cc-4699-a6e8-07568258c8fa"
      },
      "source": [
        "loss, acc = model_vgg16.evaluate(test_dataset_128)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 9s 448ms/step - loss: 0.9818 - accuracy: 0.6469\n",
            "Loss 0.982, accuracy 64.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVLvijQJ998Y"
      },
      "source": [
        "### Model ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y50WL1gjO7R3"
      },
      "source": [
        "from keras.applications import ResNet\n",
        "\n",
        "resnet152 = ResNet152(include_top=False, input_shape=(64, 64, 3))\n",
        "resnet152.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCiJvf6dTJg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cf8397-555b-4e21-ec7a-0265a31de2bc"
      },
      "source": [
        "model_resnet = Sequential()\n",
        "model_resnet.add(Lambda(preprocess_input, input_shape=(64, 64, 3)))\n",
        "model_resnet.add(resnet152)\n",
        "model_resnet.add(Flatten())\n",
        "model_resnet.add(Dense(4096, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(512, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(128, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_resnet.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_5 (Lambda)            (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "resnet152v2 (Functional)     (None, 2, 2, 2048)        58331648  \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4096)              33558528  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 94,054,278\n",
            "Trainable params: 35,722,630\n",
            "Non-trainable params: 58,331,648\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7NoVSH_TbY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3aa03a-7428-4576-c2e6-ab881b022ac4"
      },
      "source": [
        "model_resnet.fit(train_dataset_64, epochs=20, validation_data=val_dataset_64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 32s 819ms/step - loss: 124892.1936 - accuracy: 0.1977 - val_loss: 1.7527 - val_accuracy: 0.2372\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 20s 698ms/step - loss: 21.8636 - accuracy: 0.2538 - val_loss: 1.7402 - val_accuracy: 0.2372\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 21s 741ms/step - loss: 4.0723 - accuracy: 0.2455 - val_loss: 1.7370 - val_accuracy: 0.2372\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 21s 746ms/step - loss: 2.2222 - accuracy: 0.2511 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 21s 742ms/step - loss: 13.8523 - accuracy: 0.2473 - val_loss: 1.7360 - val_accuracy: 0.2372\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 21s 744ms/step - loss: 3.8347 - accuracy: 0.2459 - val_loss: 1.7359 - val_accuracy: 0.2372\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 21s 747ms/step - loss: 1.8053 - accuracy: 0.2540 - val_loss: 1.7359 - val_accuracy: 0.2372\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 21s 743ms/step - loss: 6.8825 - accuracy: 0.2490 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 21s 744ms/step - loss: 2.2814 - accuracy: 0.2442 - val_loss: 1.7357 - val_accuracy: 0.2372\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 21s 745ms/step - loss: 2.6084 - accuracy: 0.2510 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 21s 749ms/step - loss: 8.4088 - accuracy: 0.2432 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 21s 743ms/step - loss: 1.7257 - accuracy: 0.2526 - val_loss: 1.7361 - val_accuracy: 0.2372\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 21s 744ms/step - loss: 1.8356 - accuracy: 0.2530 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 21s 748ms/step - loss: 4.9870 - accuracy: 0.2476 - val_loss: 1.7355 - val_accuracy: 0.2372\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 21s 748ms/step - loss: 1.8631 - accuracy: 0.2510 - val_loss: 1.7361 - val_accuracy: 0.2372\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 21s 745ms/step - loss: 1.7586 - accuracy: 0.2444 - val_loss: 1.7357 - val_accuracy: 0.2372\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 21s 745ms/step - loss: 1.8604 - accuracy: 0.2539 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 21s 738ms/step - loss: 1.9077 - accuracy: 0.2480 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 21s 747ms/step - loss: 1.7307 - accuracy: 0.2481 - val_loss: 1.7358 - val_accuracy: 0.2372\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 21s 744ms/step - loss: 1.7320 - accuracy: 0.2470 - val_loss: 1.7359 - val_accuracy: 0.2372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f65d7410690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZxpOgmU-dAx"
      },
      "source": [
        "loss, acc = model_resnet.evaluate(test_dataset_64)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e07tijY-wq8"
      },
      "source": [
        "### Model: ResNet 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCo3toS8-Vt7"
      },
      "source": [
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "model_resnet = Sequential()\n",
        "\n",
        "\n",
        "# model_resnet.add(Lambda(preprocess_input, input_shape=(64, 64, 3)))\n",
        "model_resnet.add(resnet152)\n",
        "model_resnet.add(GlobalAveragePooling2D())\n",
        "model_resnet.add(Dense(128, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(6, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQBMsq1Dok3"
      },
      "source": [
        "from keras.applications import InceptionResNetV2\n",
        "\n",
        "inception = InceptionResNetV2(include_top=False, input_shape=(256, 256, 3))\n",
        "inception.trainable = False"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b7OPlwiEC7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be7a98a-58cf-4636-eef0-20d33a00a97d"
      },
      "source": [
        "model_inception = Sequential()\n",
        "model_inception.add(Rescaling(scale=1./255, input_shape=(256, 256, 3)))\n",
        "model_inception.add(RandomFlip(mode=\"horizontal\"))\n",
        "# model_inception.add(RandomFlip(mode=\"vertical\"))\n",
        "model_inception.add(RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "model_inception.add(RandomRotation(factor=(-0.1, 0.1)))\n",
        "model_inception.add(RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)))\n",
        "\n",
        "model_inception.add(inception)\n",
        "model_inception.add(GlobalAveragePooling2D())\n",
        "model_inception.add(Dense(512, activation='relu'))\n",
        "model_inception.add(Dropout(0.6))\n",
        "model_inception.add(Dense(128, activation='relu'))\n",
        "model_inception.add(Dropout(0.5))\n",
        "model_inception.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model_inception.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_inception.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_14 (Rescaling)     (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_flip_23 (RandomFlip)  (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_zoom_13 (RandomZoom)  (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_rotation_13 (RandomRo (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "random_translation_13 (Rando (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v2 (Functio (None, 6, 6, 1536)        54336736  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_15  (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 512)               786944    \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 55,190,118\n",
            "Trainable params: 853,382\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIBcCfbpGAeA"
      },
      "source": [
        "earlystopping = EarlyStopping(\n",
        "    monitor = 'val_loss', \n",
        "    verbose = 1, \n",
        "    patience = 30,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXxNpkI8FR1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1801f00f-5f7b-4e2f-8a5a-441183177b0a"
      },
      "source": [
        "model_inception.fit(train_dataset_256, epochs=50, validation_data=val_dataset_256, callbacks=[earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "96/96 [==============================] - 87s 797ms/step - loss: 1.7899 - accuracy: 0.3532 - val_loss: 0.8982 - val_accuracy: 0.6926\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 75s 767ms/step - loss: 1.1620 - accuracy: 0.5747 - val_loss: 0.7754 - val_accuracy: 0.7372\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 75s 766ms/step - loss: 1.0792 - accuracy: 0.6176 - val_loss: 0.7029 - val_accuracy: 0.7652\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 75s 765ms/step - loss: 1.0126 - accuracy: 0.6445 - val_loss: 0.6666 - val_accuracy: 0.7742\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 75s 765ms/step - loss: 0.9681 - accuracy: 0.6677 - val_loss: 0.6415 - val_accuracy: 0.7680\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 75s 767ms/step - loss: 0.9368 - accuracy: 0.6767 - val_loss: 0.6296 - val_accuracy: 0.7813\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 75s 767ms/step - loss: 0.9356 - accuracy: 0.6768 - val_loss: 0.6268 - val_accuracy: 0.7842\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 75s 768ms/step - loss: 0.8678 - accuracy: 0.6875 - val_loss: 0.6031 - val_accuracy: 0.7979\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 75s 767ms/step - loss: 0.8480 - accuracy: 0.7000 - val_loss: 0.5836 - val_accuracy: 0.7894\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 75s 766ms/step - loss: 0.8592 - accuracy: 0.7005 - val_loss: 0.6065 - val_accuracy: 0.7837\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 75s 767ms/step - loss: 0.8640 - accuracy: 0.6934 - val_loss: 0.6044 - val_accuracy: 0.7865\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 75s 765ms/step - loss: 0.8860 - accuracy: 0.6868 - val_loss: 0.5616 - val_accuracy: 0.8036\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 75s 765ms/step - loss: 0.8436 - accuracy: 0.7200 - val_loss: 0.5792 - val_accuracy: 0.8107\n",
            "Epoch 14/50\n",
            "61/96 [==================>...........] - ETA: 20s - loss: 0.8213 - accuracy: 0.7149"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFuE0CNXFYWo"
      },
      "source": [
        "loss, acc = model_inception.evaluate(test_dataset_256)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc7DMJhRKK"
      },
      "source": [
        "## Summary of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2c2JCKhRKK"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Write in the following cell a short report explaining what network designs you tried, and what test accuracies you obtained. What worked and what didn't? What have you learned from this exercise?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofHCraehRKL"
      },
      "source": [
        "Example results table\n",
        "\n",
        "|Model|Image processing|Neural network model|Training strategy|Test accuracy|\n",
        "|--------------------|----------------|--------------------|-----------------|-------------|\n",
        "|1|Size 32x32, batch size 256|Conv(4, linear) + Flatten + Dense(6, sigmoid)|Train from scratch|xx%|\n",
        "|2|Size 32x32, batch size 256|Conv(4, linear) + MaxPooling(2) + Flatten + Dense(6, sigmoid)|Train from scratch|xx%|\n",
        "|3|Size 32x32, batch size 256|Conv(8, relu) + MaxPooling(2) + Conv(16, relu) + MaxPooling(2) + Flatten + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|4|Size 32x32, batch size 256|Conv(8, relu) + Conv(32, relu) + MaxPooling(2) + Flatten + Dense(50, relu) + Dropout(0,5) + Dense(6, sigmoid)|Train from scratch|xx%|\n",
        "|5|Size 64x64, batch size 128|Conv(8, linear) + MaxPooling(2) + Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Flatten + Dense(200, relu) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|6|Size 64x64, batch size 128|Conv(8, relu) + MaxPooling(2) + Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Conv(128, relu) + MaxPooling(2) + Conv(256, relu) + MaxPooling(2) + Dropout(0,5) + Flatten + Dense(1024, relu) + Dropout(0,5) + Dense(200, relu) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|7|Size 64x64, batch size 128|Conv(4, relu) + Conv(8, relu) + MaxPooling(3) + Conv(16, relu) + MaxPooling(2) + Flatten + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|8|Size 64x64, batch size 128|Conv(4, relu) + Conv(8, relu) + MaxPooling(3) + Conv(16, relu) + MaxPooling(2) + Flatten + Dense(128, softmax) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|9|Size 256, batch size 64|Conv(96, relu) + Conv(128, relu) + MaxPooling(3) + Conv(256, relu) + MaxPooling(3) + Conv(256, relu) + Conv(512, relu) + Conv(512, relu) + MaxPooling(2) + Flatten + Dense(512, relu) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|10|Size 128, batch size 128|Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Conv(128, relu) + MaxPooling(2) + Conv(256, relu) + MaxPooling(2) + Flatten + Dense(4096, softmax) + Dropout(0,5) + Dense(256, softmax) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|11|Size 128, batch size 128|Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Conv(128, relu) + MaxPooling(2) + Conv(256, relu) + MaxPooling(2) + Flatten + Dense(4096, softmax) + Dropout(0,5) + Dense(1024, softmax) + Dropout(0,5) + Dense(256, softmax) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|12|Size 128, batch size 128|Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Conv(128, relu) + MaxPooling(2) + Conv(256, relu) + MaxPooling(2) + Flatten + Dense(2048, softmax) + Dropout(0,6) + Dense(1024, softmax) + Dropout(0,6) + Dense(256, softmax) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "|13|Size 128, batch size 128|Conv(32, relu) + MaxPooling(2) + Conv(64, relu) + MaxPooling(2) + Conv(128, relu) + MaxPooling(2) + Conv(256, relu) + MaxPooling(2) + Flatten + Dense(2048, softmax) + Dropout(0,6) + Dense(1024, softmax) + Dropout(0,6) + Dense(256, softmax) + Dropout(0,5) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "\n",
        "\n",
        "Conv(128, relu) + MaxPooling(3) + Conv(256, relu) + MaxPooling(3) + Conv(256, relu) + Conv(512, relu) + Conv(512, relu) + MaxPooling(2) + Flatten + Dense(512, relu) + Dense(6, softmax)|Train from scratch|xx%|\n",
        "\n",
        "|Size 64x64, batch size 32|VGG16 + Flatten + Dense(32)|Bottleneck features|yy%|\n",
        "|...|...|...|...|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8mAq5ohrWyt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}