{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "deeplearning_mnist_cnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiaqw/deep-learning/blob/main/deeplearning_mnist_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "rdhGBr4OMicv"
      },
      "source": [
        "# Lab assignment: classifying digits with Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "VXD5qKFWMicy"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/lenet.png\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "wK8PAeYHMicz"
      },
      "source": [
        "In this assignment we come back to the the problem of recognizing handwritten digits, this time using Convolutional Neural Networks. We will see how this architecture allows us to attain higher accuracy rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "KgLfi-2yMicz"
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "zJbNhkifMicz"
      },
      "source": [
        "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ii46IzuMic0"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZAgX1UDMic0"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u76joUhNMic1"
      },
      "source": [
        "To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Deep Learning environment files](https://github.com/albarji/teaching-environments-deeplearning), or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLWVFaLMic1"
      },
      "source": [
        "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Shift+Tab to produce a pop-out with related documentation. This will only work inside code cells. \n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxokwzOMic1"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koZvA1a5Mic2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Load and prepared the data as you did in the previous notebook. Make sure to normalize the pixel values, and encode the outputs a one-hot vectors. You <b>don't need to reshape the data</b> to 1-dimensional vectors, the Convolutional Network will take care of that.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wnaX1Z86Mic2"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train_norm = X_train.astype('float32') / 255\n",
        "X_test_norm = X_test.astype('float32') / 255\n",
        "Y_train = np_utils.to_categorical(y_train, 10) # We have 10 classes to codify\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMTUHJnMic3"
      },
      "source": [
        "In what follows this notebook assumes you have loaded your training images as **X_train_norm**, training labels as **Y_train**, test images as **X_test_norm** and test labels as **Y_test**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpKRIstdMic3"
      },
      "source": [
        "## Keras imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjg4IriOMic3"
      },
      "source": [
        "We will need the following keras classes, which you already used in the previous notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwoHQD3ZMic3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh8H6g9yMic4"
      },
      "source": [
        "## Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2kAnUBTMic4"
      },
      "source": [
        "To further improve on this image recognition problem we need network layers that do consider the data as images, and take into account closeness of pixels to make decisions instead of just throwing all pixel data into a fully connected network and expect intelligence to emerge from chaos. **Convolutional** and **Pooling** layers are the best way to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo565bKgMic4"
      },
      "source": [
        "### Formatting the data as tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2zUs-0yMic4"
      },
      "source": [
        "While for the perceptrons in the previous notebook we vectorized the data to fit into the perceptron framework, for convolutional networks we will need to shape the data in the form of a **4-dimensional tensor**. The dimensions of such tensor represent the following:\n",
        "* Image index (e.g. 3th image in the dataset)\n",
        "* Row index\n",
        "* Column index\n",
        "* Channel index (e.g. colour channel in colored images)\n",
        "Our data currently has the following shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK_-vOYbMic5",
        "outputId": "06f1079d-cdcc-45c6-e47e-90ec6b9ca7d7"
      },
      "source": [
        "X_train_norm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWVh6KDqMic5"
      },
      "source": [
        "So, once again we will need to make use of the reshape function to transformation the data to appropriate shape. We have 60000 images in our training set, and those images have 28 rows x 28 columns. Since these images are grayscale, the channel dimension only contains one channel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QybuTlCMic5",
        "outputId": "71e79683-c3c8-47da-e73e-e511076b8357"
      },
      "source": [
        "traintensor = X_train_norm.reshape(60000, 28, 28, 1)\n",
        "traintensor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znfiyXScMic6"
      },
      "source": [
        "Now the data is correctly shaped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vCyQ-2Mic6"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Repeat the transformation for the test data. Save the resulting tensor in a variable named <b>testtensor</b>.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIuvtXcfMic7"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "testtensor = X_test_norm.reshape(10000, 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZsE-dd7Mic8"
      },
      "source": [
        "### Convolution and pooling layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SJsbFTMic9"
      },
      "source": [
        "When defining a convolutional network, Convolution and Pooling layers work together. The most popular way of using these layers is in the following pattern:\n",
        "* A Convolution layer with rectified linear activations\n",
        "* A Pooling layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Qf9xq3Mic9"
      },
      "source": [
        "We can thus define a minimal convolutional network as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9U2MPW1Mic9"
      },
      "source": [
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "kernel_size = 3 # Size of the kernel for the convolution layers\n",
        "pool_size = 2 # Size of the pooling region for the pooling layers\n",
        "\n",
        "convnet = Sequential()\n",
        "\n",
        "convnet.add(Convolution2D(\n",
        "    32, # Number convolution channels to generate\n",
        "    (kernel_size, kernel_size), # Size of convolution kernels\n",
        "    padding='valid', # Strategy to deal with borders\n",
        "    input_shape=(img_rows, img_cols, 1), # Size = image rows x image columns x channels\n",
        "    activation=\"relu\"  # Activation function after the convolution\n",
        ")) \n",
        "convnet.add(MaxPooling2D(pool_size=(pool_size, pool_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwIOJFOgMic9"
      },
      "source": [
        "There is an issue, though: at some point we need to transform the tensor data into a vector, as the output of the network should be a vector of 10 values, representing class probabilities. We can do this by using a **Flatten** layer. Then we can add a standard Dense layer to produce the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMRL9uxOMic-"
      },
      "source": [
        "from keras.layers.core import Flatten\n",
        "convnet.add(Flatten())\n",
        "convnet.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHJ5fPzMic-"
      },
      "source": [
        "Let's take a look at the network we just defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QJ5INtNMic-",
        "outputId": "5e80f1d9-8659-4257-a989-6a7bae654429"
      },
      "source": [
        "convnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                54090     \n",
            "=================================================================\n",
            "Total params: 54,410\n",
            "Trainable params: 54,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCt-VRGAMic-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Compile the defined network, choosing \"adam\" as the optimization algorithm, and train it with the data. Use the reshaped tensor data you prepared above, not the original data. Also, use a batch size of 128 and 20 training epochs. Then measure the accuracy over the test data. Have the Convolution and MaxPooling helped?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "G_bauhJ0Mic_",
        "outputId": "4411a7de-174e-4d0b-e92d-8160d0db13a8"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "convnet.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "convnet.fit(\n",
        "    traintensor, # Training data\n",
        "    Y_train, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=2 # Level of verbosity of the log messages\n",
        ")\n",
        "score = convnet.evaluate(testtensor, Y_test)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 - 2s - loss: 0.3522 - accuracy: 0.9041\n",
            "Epoch 2/20\n",
            "469/469 - 2s - loss: 0.1252 - accuracy: 0.9653\n",
            "Epoch 3/20\n",
            "469/469 - 2s - loss: 0.0854 - accuracy: 0.9763\n",
            "Epoch 4/20\n",
            "469/469 - 2s - loss: 0.0698 - accuracy: 0.9798\n",
            "Epoch 5/20\n",
            "469/469 - 2s - loss: 0.0599 - accuracy: 0.9833\n",
            "Epoch 6/20\n",
            "469/469 - 2s - loss: 0.0522 - accuracy: 0.9854\n",
            "Epoch 7/20\n",
            "469/469 - 2s - loss: 0.0475 - accuracy: 0.9867\n",
            "Epoch 8/20\n",
            "469/469 - 2s - loss: 0.0439 - accuracy: 0.9869\n",
            "Epoch 9/20\n",
            "469/469 - 3s - loss: 0.0398 - accuracy: 0.9883\n",
            "Epoch 10/20\n",
            "469/469 - 3s - loss: 0.0367 - accuracy: 0.9892\n",
            "Epoch 11/20\n",
            "469/469 - 3s - loss: 0.0332 - accuracy: 0.9902\n",
            "Epoch 12/20\n",
            "469/469 - 2s - loss: 0.0307 - accuracy: 0.9911\n",
            "Epoch 13/20\n",
            "469/469 - 2s - loss: 0.0288 - accuracy: 0.9916\n",
            "Epoch 14/20\n",
            "469/469 - 2s - loss: 0.0262 - accuracy: 0.9925\n",
            "Epoch 15/20\n",
            "469/469 - 2s - loss: 0.0236 - accuracy: 0.9932\n",
            "Epoch 16/20\n",
            "469/469 - 2s - loss: 0.0224 - accuracy: 0.9935\n",
            "Epoch 17/20\n",
            "469/469 - 2s - loss: 0.0203 - accuracy: 0.9944\n",
            "Epoch 18/20\n",
            "469/469 - 2s - loss: 0.0185 - accuracy: 0.9951\n",
            "Epoch 19/20\n",
            "469/469 - 2s - loss: 0.0169 - accuracy: 0.9954\n",
            "Epoch 20/20\n",
            "469/469 - 2s - loss: 0.0163 - accuracy: 0.9957\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0535 - accuracy: 0.9837\n",
            "Test loss 0.05353615805506706\n",
            "Test accuracy 0.9836999773979187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkgWydjOMic_"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Build and train a convolutional network with the following layers:\n",
        "<ul>\n",
        "     <li>A Convolution layer of 32 channels, kernel size 3 and rectified linear activation</li>\n",
        "     <li>Another Convolution layer of 32 channels, kernel size 3 and rectified linear activation</li>\n",
        "     <li>A MaxPooling layer of size 2</li>\n",
        "     <li>A 25% Dropout</li>\n",
        "     <li>A Flatten layer</li>\n",
        "     <li>A Dense layer with 128 units and rectified linear activation</li>\n",
        "     <li>A 50% Dropout</li>\n",
        "     <li>An output Dense layer with softmax activation</li>\n",
        "</ul>\n",
        "Has the added complexity improved the accuracy results?    \n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_uE12GvoMic_",
        "outputId": "e7cf535e-ba91-475a-dcb4-003d12cd41be"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "kernel_size = 3 # Size of the kernel for the convolution layers\n",
        "pool_size = 2 # Size of the pooling region for the pooling layers\n",
        "\n",
        "large_convnet = Sequential()\n",
        "\n",
        "large_convnet.add(Convolution2D(32, # Number convolution channels to generate\n",
        "                        (kernel_size, kernel_size),\n",
        "                        padding='valid',\n",
        "                        input_shape=(img_rows, img_cols, 1),\n",
        "                        activation=\"relu\"))\n",
        "large_convnet.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "large_convnet.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
        "large_convnet.add(Flatten())\n",
        "large_convnet.add(Dense(128, activation=\"relu\"))\n",
        "large_convnet.add(Dropout(0.5))\n",
        "large_convnet.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "large_convnet.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "large_convnet.fit(\n",
        "    traintensor, # Training data\n",
        "    Y_train, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")\n",
        "score = large_convnet.evaluate(testtensor, Y_test)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2547 - accuracy: 0.9223\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0810 - accuracy: 0.9758\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0609 - accuracy: 0.9811\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0487 - accuracy: 0.9847\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0431 - accuracy: 0.9867\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0349 - accuracy: 0.9887\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0266 - accuracy: 0.9916\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0274 - accuracy: 0.9913\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0224 - accuracy: 0.9929\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0199 - accuracy: 0.9936\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0193 - accuracy: 0.9934\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0178 - accuracy: 0.9938\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0145 - accuracy: 0.9956\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0139 - accuracy: 0.9952\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0123 - accuracy: 0.9957\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0111 - accuracy: 0.9964\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9913\n",
            "Test loss 0.038704849779605865\n",
            "Test accuracy 0.9912999868392944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05weyr1bMidA"
      },
      "source": [
        "## LeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTb7bz0MidB"
      },
      "source": [
        "<a href=http://yann.lecun.com/exdb/lenet/>LeNet</a> is a particular convolutional neural network definition that has proven to be quite effective for this problem. As a final exercise we will build a network similar to LeNet and try it on our digits problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA0lnqIoMidB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Build and train the following network:\n",
        "<ul>\n",
        "     <li>A Convolution layer of 32 channels, kernel size 5 and rectified linear activation</li>\n",
        "     <li>A MaxPooling layer of size 2</li>\n",
        "     <li>A Convolution layer of 50 channels, kernel size 5 and rectified linear activation</li>\n",
        "     <li>A MaxPooling layer of size 2</li>\n",
        "     <li>A Flatten layer</li>\n",
        "     <li>A Dense layer with 256 units and rectified linear activation</li>\n",
        "     <li>A 50% Dropout</li>\n",
        "     <li>An output Dense layer with softmax activation</li>\n",
        "</ul>\n",
        "Is this the best network so far for the problem?   \n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "kKF7xrGhMidB",
        "outputId": "99e3ff47-0baf-499b-d9ca-beb646557c9e"
      },
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "\n",
        "lenet = Sequential()\n",
        "\n",
        "lenet.add(Convolution2D(\n",
        "    32,\n",
        "    (5, 5),\n",
        "    padding='valid',\n",
        "    input_shape=(img_rows, img_cols, 1),\n",
        "    activation=\"relu\"\n",
        "))\n",
        "lenet.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "lenet.add(Convolution2D(50, (5, 5), activation=\"relu\"))\n",
        "lenet.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "lenet.add(Flatten())\n",
        "lenet.add(Dense(256, activation=\"relu\"))\n",
        "lenet.add(Dropout(0.5))\n",
        "lenet.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "lenet.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "lenet.fit(\n",
        "    traintensor, # Training data\n",
        "    Y_train, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")\n",
        "score = lenet.evaluate(testtensor, Y_test)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.2479 - accuracy: 0.9240\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0725 - accuracy: 0.9775\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0512 - accuracy: 0.9841\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0383 - accuracy: 0.9883\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0336 - accuracy: 0.9896\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0264 - accuracy: 0.9917\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0240 - accuracy: 0.9923\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0213 - accuracy: 0.9929\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0187 - accuracy: 0.9938\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0159 - accuracy: 0.9949\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0155 - accuracy: 0.9948\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0121 - accuracy: 0.9958\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0121 - accuracy: 0.9959\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0109 - accuracy: 0.9964\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0075 - accuracy: 0.9974\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0092 - accuracy: 0.9969\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0076 - accuracy: 0.9973\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9972\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0257 - accuracy: 0.9932\n",
            "Test loss 0.02574528194963932\n",
            "Test accuracy 0.9932000041007996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4UXDzOhMidC"
      },
      "source": [
        "## Bonus rounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjx5N9T8MidD"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#259b4c>\n",
        "Rebuild the network above with a larger number of training epochs. What is the best test error you can achieve? \n",
        "</font>\n",
        "\n",
        "***"
      ]
    }
  ]
}